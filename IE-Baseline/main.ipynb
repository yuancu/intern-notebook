{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3660d3e",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "Adapted from the original `main.py`. Intergrated with AWS SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a0f9e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from tqdm import tqdm\n",
    "import model\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "#import data_prepare\n",
    "import os\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89ae5536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for macOS compatibility\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "CHAR_SIZE = 128\n",
    "SENT_LENGTH = 4\n",
    "HIDDEN_SIZE = 64\n",
    "EPOCH_NUM = 100\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aea8feb",
   "metadata": {},
   "source": [
    "Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79ec01dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_now_time():\n",
    "    a = time.time()\n",
    "    return time.ctime(a)\n",
    "\n",
    "\n",
    "def seq_padding(X):\n",
    "    L = [len(x) for x in X]\n",
    "    ML = max(L)\n",
    "    # print(\"ML\",ML)\n",
    "    return [x + [0] * (ML - len(x)) for x in X]\n",
    "\n",
    "\n",
    "def seq_padding_vec(X):\n",
    "    L = [len(x) for x in X]\n",
    "    ML = max(L)\n",
    "    # print(\"ML\",ML)\n",
    "    return [x + [[1, 0]] * (ML - len(x)) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d92193de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(self, data, batch_size=64):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = len(self.data) // self.batch_size\n",
    "        if len(self.data) % self.batch_size != 0:\n",
    "            self.steps += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "\n",
    "    def pro_res(self):\n",
    "        idxs = list(range(len(self.data)))\n",
    "        # print(idxs)\n",
    "        np.random.shuffle(idxs)\n",
    "        T, S1, S2, K1, K2, O1, O2, = [], [], [], [], [], [], []\n",
    "        for i in idxs:\n",
    "            d = self.data[i]\n",
    "            text = d['text']\n",
    "            items = {}\n",
    "            items = defaultdict(list)\n",
    "            for sp in d['spo_list']:\n",
    "                subjectid = text.find(sp[0])\n",
    "                objectid = text.find(sp[2])\n",
    "                if subjectid != -1 and objectid != -1:\n",
    "                    key = (subjectid, subjectid+len(sp[0])) # key is the span(start, end) of the subject\n",
    "                    # items is {(S_start, S_end): list of (O_start_pos, O_end_pos, predicate_id)}\n",
    "                    items[key].append(\n",
    "                        (objectid, objectid+len(sp[2]), predicate2id[sp[1]]))\n",
    "            if items:\n",
    "                # T is list of text tokens(ids)\n",
    "                T.append([char2id.get(c, 1) for c in text])  # 1是unk，0是padding\n",
    "         \n",
    "                # s1: one-hot vector where start of subject is 1\n",
    "                # s2: one-hot vector where end of subject is 1\n",
    "                s1, s2 = [0] * len(text), [0] * len(text)\n",
    "                for j in items:\n",
    "                    s1[j[0]] = 1\n",
    "                    s2[j[1]-1] = 1\n",
    "                # print(items.keys())\n",
    "                # k1, k2: randomly sampled (S_start, S_end) pair?\n",
    "                k1, k2 = choice(list(items.keys()))\n",
    "                # o1: zero vector, the start of each O is marked with its predicate ID\n",
    "                # o2: zero vector, the end of each O is marked with its predicate ID\n",
    "                o1, o2 = [0] * len(text), [0] * len(text)  # 0是unk类（共49+1个类）\n",
    "                for j in items[(k1, k2)]:\n",
    "                    o1[j[0]] = j[2]\n",
    "                    o2[j[1]-1] = j[2]\n",
    "                S1.append(s1)\n",
    "                S2.append(s2)\n",
    "                K1.append([k1])\n",
    "                K2.append([k2-1])\n",
    "                O1.append(o1)\n",
    "                O2.append(o2)\n",
    "\n",
    "        T = np.array(seq_padding(T))\n",
    "        S1 = np.array(seq_padding(S1))\n",
    "        S2 = np.array(seq_padding(S2))\n",
    "        O1 = np.array(seq_padding(O1))\n",
    "        O2 = np.array(seq_padding(O2))\n",
    "        K1, K2 = np.array(K1), np.array(K2)\n",
    "        return [T, S1, S2, K1, K2, O1, O2]\n",
    "\n",
    "\n",
    "class MyDataset(Data.Dataset):\n",
    "    \"\"\"\n",
    "        下载数据、初始化数据，都可以在这里完成\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, _T, _S1, _S2, _K1, _K2, _O1, _O2):\n",
    "        # xy = np.loadtxt('../dataSet/diabetes.csv.gz', delimiter=',', dtype=np.float32) # 使用numpy读取数据\n",
    "        self.x_data = _T\n",
    "        self.y1_data = _S1\n",
    "        self.y2_data = _S2\n",
    "        self.k1_data = _K1\n",
    "        self.k2_data = _K2\n",
    "        self.o1_data = _O1\n",
    "        self.o2_data = _O2\n",
    "        self.len = len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y1_data[index], self.y2_data[index], self.k1_data[index], self.k2_data[index], self.o1_data[index], self.o2_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    t = np.array([item[0] for item in data], np.int32)\n",
    "    s1 = np.array([item[1] for item in data], np.int32)\n",
    "    s2 = np.array([item[2] for item in data], np.int32)\n",
    "    k1 = np.array([item[3] for item in data], np.int32)\n",
    "\n",
    "    k2 = np.array([item[4] for item in data], np.int32)\n",
    "    o1 = np.array([item[5] for item in data], np.int32)\n",
    "    o2 = np.array([item[6] for item in data], np.int32)\n",
    "    return {\n",
    "        'T': torch.LongTensor(t),  # targets_i\n",
    "        'S1': torch.FloatTensor(s1),\n",
    "        'S2': torch.FloatTensor(s2),\n",
    "        'K1': torch.LongTensor(k1),\n",
    "        'K2': torch.LongTensor(k2),\n",
    "        'O1': torch.LongTensor(o1),\n",
    "        'O2': torch.LongTensor(o2),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee6b065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_items(text_in):\n",
    "    R = []\n",
    "    _s = [char2id.get(c, 1) for c in text_in]\n",
    "    _s = np.array([_s])\n",
    "    _k1, _k2, t, t_max, mask = s_m(torch.LongTensor(_s).to(device))\n",
    "    _k1, _k2 = _k1[0, :, 0], _k2[0, :, 0]\n",
    "    _kk1s = []\n",
    "    for i, _kk1 in enumerate(_k1):\n",
    "        if _kk1 > 0.5:\n",
    "            _subject = ''\n",
    "            for j, _kk2 in enumerate(_k2[i:]):\n",
    "                if _kk2 > 0.5:\n",
    "                    _subject = text_in[i: i+j+1]\n",
    "                    break\n",
    "            if _subject:\n",
    "                _k1, _k2 = torch.LongTensor([[i]]), torch.LongTensor(\n",
    "                    [[i+j]])  # np.array([i]), np.array([i+j])\n",
    "                _o1, _o2 = po_m(t.to(device), t_max.to(\n",
    "                    device), _k1.to(device), _k2.to(device))\n",
    "                _o1, _o2 = _o1.cpu().data.numpy(), _o2.cpu().data.numpy()\n",
    "\n",
    "                _o1, _o2 = np.argmax(_o1[0], 1), np.argmax(_o2[0], 1)\n",
    "\n",
    "                for i, _oo1 in enumerate(_o1):\n",
    "                    if _oo1 > 0:\n",
    "                        for j, _oo2 in enumerate(_o2[i:]):\n",
    "                            if _oo2 == _oo1:\n",
    "                                _object = text_in[i: i+j+1]\n",
    "                                _predicate = id2predicate[_oo1]\n",
    "                                # print((_subject, _predicate, _object))\n",
    "                                R.append((_subject, _predicate, _object))\n",
    "                                break\n",
    "        _kk1s.append(_kk1.data.cpu().numpy())\n",
    "    _kk1s = np.array(_kk1s)\n",
    "    return list(set(R))\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    A, B, C = 1e-10, 1e-10, 1e-10\n",
    "    cnt = 0\n",
    "    for d in tqdm(iter(dev_data)):\n",
    "        R = set(extract_items(d['text']))\n",
    "        T = set([tuple(i) for i in d['spo_list']])\n",
    "        A += len(R & T)\n",
    "        B += len(R)\n",
    "        C += len(T)\n",
    "        # if cnt % 1000 == 0:\n",
    "        #     print('iter: %d f1: %.4f, precision: %.4f, recall: %.4f\\n' % (cnt, 2 * A / (B + C), A / B, A / C))\n",
    "        cnt += 1\n",
    "    return 2 * A / (B + C), A / B, A / C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37f30f6",
   "metadata": {},
   "source": [
    "Load train and dev data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d821be41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'generated/train_data_me.json'\n",
    "dev_path = 'generated/dev_data_me.json'\n",
    "generated_schema_path =  'generated/schemas_me.json'\n",
    "generated_char_path = 'generated/all_chars_me.json'\n",
    "train_data = json.load(open(train_path))\n",
    "dev_data = json.load(open(dev_path))\n",
    "id2predicate, predicate2id = json.load(open(generated_schema_path))\n",
    "id2predicate = {int(i): j for i, j in id2predicate.items()}\n",
    "id2char, char2id = json.load(open(generated_char_path))\n",
    "num_classes = len(id2predicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37429abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = DataGenerator(train_data)\n",
    "T, S1, S2, K1, K2, O1, O2 = dg.pro_res()\n",
    "# print(\"len\",len(T))\n",
    "\n",
    "torch_dataset = MyDataset(T, S1, S2, K1, K2, O1, O2)\n",
    "loader = Data.DataLoader(\n",
    "    dataset=torch_dataset,      # torch TensorDataset format\n",
    "    batch_size=BATCH_SIZE,      # mini batch size\n",
    "    shuffle=True,               # random shuffle for training\n",
    "    num_workers=8,\n",
    "    collate_fn=collate_fn,      # subprocesses for loading data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9f127f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_m = model.s_model(len(char2id)+2, CHAR_SIZE, HIDDEN_SIZE).to(device)v吃   \n",
    "po_m = model.po_model(len(char2id)+2, CHAR_SIZE,\n",
    "                      HIDDEN_SIZE, 49).to(device)\n",
    "\n",
    "params = list(s_m.parameters())\n",
    "params += list(po_m.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss().to(device)\n",
    "b_loss = torch.nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c0f72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1 = 0\n",
    "best_epoch = 0\n",
    "\n",
    "for i in range(EPOCH_NUM):\n",
    "    for step, loader_res in tqdm(iter(enumerate(loader))):\n",
    "        # print(get_now_time())\n",
    "        t_s = loader_res[\"T\"].to(device)\n",
    "        k1 = loader_res[\"K1\"].to(device)\n",
    "        k2 = loader_res[\"K2\"].to(device)\n",
    "        s1 = loader_res[\"S1\"].to(device)\n",
    "        s2 = loader_res[\"S2\"].to(device)\n",
    "        o1 = loader_res[\"O1\"].to(device)\n",
    "        o2 = loader_res[\"O2\"].to(device)\n",
    "\n",
    "        ps_1, ps_2, t, t_max, mask = s_m(t_s)\n",
    "\n",
    "        t, t_max, k1, k2 = t.to(device), t_max.to(\n",
    "            device), k1.to(device), k2.to(device)\n",
    "        po_1, po_2 = po_m(t, t_max, k1, k2)\n",
    "\n",
    "        ps_1 = ps_1.to(device)\n",
    "        ps_2 = ps_2.to(device)\n",
    "        po_1 = po_1.to(device)\n",
    "        po_2 = po_2.to(device)\n",
    "\n",
    "        s1 = torch.unsqueeze(s1, 2)\n",
    "        s2 = torch.unsqueeze(s2, 2)\n",
    "\n",
    "        s1_loss = b_loss(ps_1, s1)\n",
    "        s1_loss = torch.sum(s1_loss.mul(mask))/torch.sum(mask)\n",
    "        s2_loss = b_loss(ps_2, s2)\n",
    "        s2_loss = torch.sum(s2_loss.mul(mask))/torch.sum(mask)\n",
    "\n",
    "        po_1 = po_1.permute(0, 2, 1)\n",
    "        po_2 = po_2.permute(0, 2, 1)\n",
    "\n",
    "        o1_loss = loss(po_1, o1)\n",
    "        o1_loss = torch.sum(o1_loss.mul(mask[:, :, 0])) / torch.sum(mask)\n",
    "        o2_loss = loss(po_2, o2)\n",
    "        o2_loss = torch.sum(o2_loss.mul(mask[:, :, 0])) / torch.sum(mask)\n",
    "\n",
    "        loss_sum = 2.5 * (s1_loss + s2_loss) + (o1_loss + o2_loss)\n",
    "\n",
    "        # if step % 500 == 0:\n",
    "        # \ttorch.save(s_m, 'models_real/s_'+str(step)+\"epoch_\"+str(i)+'.pkl')\n",
    "        # \ttorch.save(po_m, 'models_real/po_'+str(step)+\"epoch_\"+str(i)+'.pkl')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_sum.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    torch.save(s_m, 'models_real/s_'+str(i)+'.pkl')\n",
    "    torch.save(po_m, 'models_real/po_'+str(i)+'.pkl')\n",
    "    f1, precision, recall = evaluate()\n",
    "\n",
    "    print(\"epoch:\", i, \"loss:\", loss_sum.data)\n",
    "\n",
    "    if f1 >= best_f1:\n",
    "        best_f1 = f1\n",
    "        best_epoch = i\n",
    "\n",
    "    print('f1: %.4f, precision: %.4f, recall: %.4f, bestf1: %.4f, bestepoch: %d \\n ' % (\n",
    "        f1, precision, recall, best_f1, best_epoch))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
