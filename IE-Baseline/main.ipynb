{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "518c3442",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "Adapted from the original `main.py`. Intergrated with AWS SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962fc300",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "(actually only tqdm, since other packages are pre-installed in aws pytorch environments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "defbb0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.61.0-py2.py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 537 kB/s  eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.61.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8c20e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from tqdm import tqdm\n",
    "import model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "#import data_prepare\n",
    "import os\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c84b9956",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3,4,5,6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7432d5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for macOS compatibility\n",
    "#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "CHAR_SIZE = 128\n",
    "SENT_LENGTH = 4\n",
    "HIDDEN_SIZE = 64\n",
    "EPOCH_NUM = 200\n",
    "BATCH_SIZE = 5096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f06f8aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a42be6",
   "metadata": {},
   "source": [
    "Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "144d317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_now_time():\n",
    "    a = time.time()\n",
    "    return time.ctime(a)\n",
    "\n",
    "\n",
    "def seq_padding(X):\n",
    "    L = [len(x) for x in X]\n",
    "    ML = max(L)\n",
    "    # print(\"ML\",ML)\n",
    "    return [x + [0] * (ML - len(x)) for x in X]\n",
    "\n",
    "\n",
    "def seq_padding_vec(X):\n",
    "    L = [len(x) for x in X]\n",
    "    ML = max(L)\n",
    "    # print(\"ML\",ML)\n",
    "    return [x + [[1, 0]] * (ML - len(x)) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83dd461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(self, data, batch_size=64):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = len(self.data) // self.batch_size\n",
    "        if len(self.data) % self.batch_size != 0:\n",
    "            self.steps += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "\n",
    "    def pro_res(self):\n",
    "        idxs = list(range(len(self.data)))\n",
    "        # print(idxs)\n",
    "        np.random.shuffle(idxs)\n",
    "        T, S1, S2, K1, K2, O1, O2, = [], [], [], [], [], [], []\n",
    "        for i in idxs:\n",
    "            d = self.data[i]\n",
    "            text = d['text']\n",
    "            items = {}\n",
    "            items = defaultdict(list)\n",
    "            for sp in d['spo_list']:\n",
    "                subjectid = text.find(sp[0])\n",
    "                objectid = text.find(sp[2])\n",
    "                if subjectid != -1 and objectid != -1:\n",
    "                    key = (subjectid, subjectid+len(sp[0])) # key is the span(start, end) of the subject\n",
    "                    # items is {(S_start, S_end): list of (O_start_pos, O_end_pos, predicate_id)}\n",
    "                    items[key].append(\n",
    "                        (objectid, objectid+len(sp[2]), predicate2id[sp[1]]))\n",
    "            if items:\n",
    "                # T is list of text tokens(ids)\n",
    "                T.append([char2id.get(c, 1) for c in text])  # 1是unk，0是padding\n",
    "         \n",
    "                # s1: one-hot vector where start of subject is 1\n",
    "                # s2: one-hot vector where end of subject is 1\n",
    "                s1, s2 = [0] * len(text), [0] * len(text)\n",
    "                for j in items:\n",
    "                    s1[j[0]] = 1\n",
    "                    s2[j[1]-1] = 1\n",
    "                # print(items.keys())\n",
    "                # k1, k2: randomly sampled (S_start, S_end) pair?\n",
    "                k1, k2 = choice(list(items.keys()))\n",
    "                # o1: zero vector, the start of each O is marked with its predicate ID\n",
    "                # o2: zero vector, the end of each O is marked with its predicate ID\n",
    "                o1, o2 = [0] * len(text), [0] * len(text)  # 0是unk类（共49+1个类）\n",
    "                for j in items[(k1, k2)]:\n",
    "                    o1[j[0]] = j[2]\n",
    "                    o2[j[1]-1] = j[2]\n",
    "                S1.append(s1)\n",
    "                S2.append(s2)\n",
    "                K1.append([k1])\n",
    "                K2.append([k2-1])\n",
    "                O1.append(o1)\n",
    "                O2.append(o2)\n",
    "\n",
    "        T = np.array(seq_padding(T))\n",
    "        S1 = np.array(seq_padding(S1))\n",
    "        S2 = np.array(seq_padding(S2))\n",
    "        O1 = np.array(seq_padding(O1))\n",
    "        O2 = np.array(seq_padding(O2))\n",
    "        K1, K2 = np.array(K1), np.array(K2)\n",
    "        return [T, S1, S2, K1, K2, O1, O2]\n",
    "\n",
    "\n",
    "class MyDataset(Data.Dataset):\n",
    "    \"\"\"\n",
    "        下载数据、初始化数据，都可以在这里完成\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, _T, _S1, _S2, _K1, _K2, _O1, _O2):\n",
    "        # xy = np.loadtxt('../dataSet/diabetes.csv.gz', delimiter=',', dtype=np.float32) # 使用numpy读取数据\n",
    "        self.x_data = _T\n",
    "        self.y1_data = _S1\n",
    "        self.y2_data = _S2\n",
    "        self.k1_data = _K1\n",
    "        self.k2_data = _K2\n",
    "        self.o1_data = _O1\n",
    "        self.o2_data = _O2\n",
    "        self.len = len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y1_data[index], self.y2_data[index], self.k1_data[index], self.k2_data[index], self.o1_data[index], self.o2_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    t = np.array([item[0] for item in data], np.int32)\n",
    "    s1 = np.array([item[1] for item in data], np.int32)\n",
    "    s2 = np.array([item[2] for item in data], np.int32)\n",
    "    k1 = np.array([item[3] for item in data], np.int32)\n",
    "\n",
    "    k2 = np.array([item[4] for item in data], np.int32)\n",
    "    o1 = np.array([item[5] for item in data], np.int32)\n",
    "    o2 = np.array([item[6] for item in data], np.int32)\n",
    "    return {\n",
    "        'T': torch.LongTensor(t),  # targets_i\n",
    "        'S1': torch.FloatTensor(s1),\n",
    "        'S2': torch.FloatTensor(s2),\n",
    "        'K1': torch.LongTensor(k1),\n",
    "        'K2': torch.LongTensor(k2),\n",
    "        'O1': torch.LongTensor(o1),\n",
    "        'O2': torch.LongTensor(o2),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2064a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_items(text_in):\n",
    "    R = []\n",
    "    _s = [char2id.get(c, 1) for c in text_in]\n",
    "    _s = np.array([_s])\n",
    "    _k1, _k2, t, t_max, mask = s_m(torch.LongTensor(_s).to(device))\n",
    "    _k1, _k2 = _k1[0, :, 0], _k2[0, :, 0]\n",
    "    _kk1s = []\n",
    "    for i, _kk1 in enumerate(_k1):\n",
    "        if _kk1 > 0.5:\n",
    "            _subject = ''\n",
    "            for j, _kk2 in enumerate(_k2[i:]):\n",
    "                if _kk2 > 0.5:\n",
    "                    _subject = text_in[i: i+j+1]\n",
    "                    break\n",
    "            if _subject:\n",
    "                _k1, _k2 = torch.LongTensor([[i]]), torch.LongTensor(\n",
    "                    [[i+j]])  # np.array([i]), np.array([i+j])\n",
    "                _o1, _o2 = po_m(t.to(device), t_max.to(\n",
    "                    device), _k1.to(device), _k2.to(device))\n",
    "                _o1, _o2 = _o1.cpu().data.numpy(), _o2.cpu().data.numpy()\n",
    "\n",
    "                _o1, _o2 = np.argmax(_o1[0], 1), np.argmax(_o2[0], 1)\n",
    "\n",
    "                for i, _oo1 in enumerate(_o1):\n",
    "                    if _oo1 > 0:\n",
    "                        for j, _oo2 in enumerate(_o2[i:]):\n",
    "                            if _oo2 == _oo1:\n",
    "                                _object = text_in[i: i+j+1]\n",
    "                                _predicate = id2predicate[_oo1]\n",
    "                                # print((_subject, _predicate, _object))\n",
    "                                R.append((_subject, _predicate, _object))\n",
    "                                break\n",
    "        _kk1s.append(_kk1.data.cpu().numpy())\n",
    "    _kk1s = np.array(_kk1s)\n",
    "    return list(set(R))\n",
    "\n",
    "def para_extract_items(dataloader, s_m, po_m):\n",
    "    with torch.no_grad():\n",
    "        for step, loader_res in tqdm(iter(enumerate(loader))):\n",
    "            t_s = loader_res[\"T\"].to(device)\n",
    "            k1 = loader_res[\"K1\"].to(device)\n",
    "            k2 = loader_res[\"K2\"].to(device)\n",
    "            s1 = loader_res[\"S1\"].to(device)\n",
    "            s2 = loader_res[\"S2\"].to(device)\n",
    "            o1 = loader_res[\"O1\"].to(device)\n",
    "            o2 = loader_res[\"O2\"].to(device)\n",
    "\n",
    "            ps_1, ps_2, t, t_max, mask = s_m(t_s)\n",
    "\n",
    "            t, t_max, k1, k2 = t.to(device), t_max.to(\n",
    "                device), k1.to(device), k2.to(device)\n",
    "            po_1, po_2 = po_m(t, t_max, k1, k2)\n",
    "\n",
    "            ps_1 = ps_1.to(device)\n",
    "            ps_2 = ps_2.to(device)\n",
    "            po_1 = po_1.to(device)\n",
    "            po_2 = po_2.to(device)\n",
    "\n",
    "            s1 = torch.unsqueeze(s1, 2)\n",
    "            s2 = torch.unsqueeze(s2, 2)\n",
    "    \n",
    "\n",
    "def evaluate():\n",
    "    A, B, C = 1e-10, 1e-10, 1e-10\n",
    "    cnt = 0\n",
    "    s_m.eval()\n",
    "    po_m.eval()\n",
    "    for d in tqdm(iter(dev_data)):\n",
    "        if cnt > 1000:\n",
    "            break\n",
    "        R = set(extract_items(d['text']))\n",
    "        T = set([tuple(i) for i in d['spo_list']])\n",
    "        A += len(R & T)\n",
    "        B += len(R)\n",
    "        C += len(T)\n",
    "        # if cnt % 1000 == 0:\n",
    "        #     print('iter: %d f1: %.4f, precision: %.4f, recall: %.4f\\n' % (cnt, 2 * A / (B + C), A / B, A / C))\n",
    "        cnt += 1\n",
    "    return 2 * A / (B + C), A / B, A / C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252e294e",
   "metadata": {},
   "source": [
    "# Download training data\n",
    "Skip the downloading step if you have alreay done it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efbc5171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-10 10:19:53--  https://dataset-bj.cdn.bcebos.com/qianyan/DuIE_2_0.zip\n",
      "Resolving dataset-bj.cdn.bcebos.com (dataset-bj.cdn.bcebos.com)... 221.5.75.35\n",
      "Connecting to dataset-bj.cdn.bcebos.com (dataset-bj.cdn.bcebos.com)|221.5.75.35|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 37097755 (35M) [application/zip]\n",
      "Saving to: ‘DuIE_2_0.zip’\n",
      "\n",
      "DuIE_2_0.zip        100%[===================>]  35.38M  6.14MB/s    in 6.9s    \n",
      "\n",
      "2021-06-10 10:20:02 (5.15 MB/s) - ‘DuIE_2_0.zip’ saved [37097755/37097755]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!wget https://dataset-bj.cdn.bcebos.com/qianyan/DuIE_2_0.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73215f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  DuIE_2_0.zip\n",
      "  inflating: data/.DS_Store          \n",
      "  inflating: data/._.DS_Store        \n",
      "  inflating: data/test.json          \n",
      "  inflating: data/._test.json        \n",
      "  inflating: data/dev.json           \n",
      "  inflating: data/._dev.json         \n",
      "  inflating: data/License.pdf        \n",
      "  inflating: data/._License.pdf      \n",
      "  inflating: data/train.json         \n",
      "  inflating: data/._train.json       \n",
      "  inflating: data/schema.json        \n",
      "  inflating: data/._schema.json      \n",
      "  inflating: data/._DuIE_2_0         \n"
     ]
    }
   ],
   "source": [
    "#!unzip -j DuIE_2_0.zip -d data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc16b0a",
   "metadata": {},
   "source": [
    "Transofm raw data to easier usable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf180549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48it [00:00, 128070.35it/s]\n",
      "/home/ec2-user/SageMaker/nlp-notebooks/IE-Baseline/data/train.json\n",
      "171293it [00:06, 25673.67it/s]\n",
      "20674it [00:00, 23275.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# !mkdir generated\n",
    "# !python trans.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68a1052",
   "metadata": {},
   "source": [
    "## Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0918b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'generated/train_data_me.json'\n",
    "dev_path = 'generated/dev_data_me.json'\n",
    "generated_schema_path =  'generated/schemas_me.json'\n",
    "generated_char_path = 'generated/all_chars_me.json'\n",
    "train_data = json.load(open(train_path))\n",
    "dev_data = json.load(open(dev_path))\n",
    "id2predicate, predicate2id = json.load(open(generated_schema_path))\n",
    "id2predicate = {int(i): j for i, j in id2predicate.items()}\n",
    "id2char, char2id = json.load(open(generated_char_path))\n",
    "num_classes = len(id2predicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0040369",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = DataGenerator(train_data)\n",
    "T, S1, S2, K1, K2, O1, O2 = dg.pro_res()\n",
    "# print(\"len\",len(T))\n",
    "\n",
    "torch_dataset = MyDataset(T, S1, S2, K1, K2, O1, O2)\n",
    "loader = Data.DataLoader(\n",
    "    dataset=torch_dataset,      # torch TensorDataset format\n",
    "    batch_size=BATCH_SIZE,      # mini batch size\n",
    "    shuffle=True,               # random shuffle for training\n",
    "    num_workers=64,\n",
    "    collate_fn=collate_fn,      # subprocesses for loading data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f371c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dg = DataGenerator(dev_data)\n",
    "T_dev, S1_dev, S2_dev, K1_dev, K2_dev, O1_dev, O2_dev = dev_dg.pro_res()\n",
    "dev_dataset = MyDataset(T_dev, S1_dev, S2_dev, K1_dev, K2_dev, O1_dev, O2_dev)\n",
    "dev_loader = Data.DataLoader(\n",
    "    dataset=dev_dataset,      # torch TensorDataset format\n",
    "    batch_size=BATCH_SIZE,      # mini batch size\n",
    "    shuffle=True,               # random shuffle for training\n",
    "    num_workers=64,\n",
    "    collate_fn=collate_fn,      # subprocesses for loading data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ff1d39",
   "metadata": {},
   "source": [
    "### Define model and loss\n",
    "Data are parallimised  to multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6e8fd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 GPUs!\n"
     ]
    }
   ],
   "source": [
    "s_m = model.s_model(len(char2id)+2, CHAR_SIZE, HIDDEN_SIZE)\n",
    "po_m = model.po_model(len(char2id)+2, CHAR_SIZE, HIDDEN_SIZE, 49)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print('Using', torch.cuda.device_count(), \"GPUs!\")\n",
    "    s_m = nn.DataParallel(s_m)\n",
    "    po_m = nn.DataParallel(po_m)\n",
    "\n",
    "s_m = s_m.to(device)\n",
    "po_m = po_m.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b23514d",
   "metadata": {},
   "source": [
    "### Load model if needed\n",
    "Uncomment lines below to load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8a93633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# breakpoint_epoch = 50\n",
    "# model_dir = 'models_real'\n",
    "# s_m = torch.load(os.path.join(model_dir, \"s_{}.pkl\".format(breakpoint_epoch)))\n",
    "# po_m = torch.load(os.path.join(model_dir, \"po_{}.pkl\".format(breakpoint_epoch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9922178",
   "metadata": {},
   "source": [
    "### Define loss metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3514ab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(s_m.parameters())\n",
    "params += list(po_m.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss().to(device)\n",
    "b_loss = torch.nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ced6d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continue training from epoch 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [01:15,  2.21s/it]\n",
      "1001it [00:26, 37.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50 loss: tensor(0.0467, device='cuda:0')\n",
      "epoch 50 used 108.13381290435791 seconds (with bsz=5096)\n",
      "f1: 0.4493, precision: 0.7778, recall: 0.3158, bestf1: 0.4493, bestepoch: 50 \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "34it [01:13,  2.16s/it]\n",
      "1001it [00:16, 61.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 51 loss: tensor(0.0486, device='cuda:0')\n",
      "epoch 51 used 95.8566267490387 seconds (with bsz=5096)\n",
      "f1: 0.4625, precision: 0.7891, recall: 0.3271, bestf1: 0.4625, bestepoch: 51 \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "14it [00:30,  2.15s/it]"
     ]
    }
   ],
   "source": [
    "best_f1 = 0\n",
    "best_epoch = 0\n",
    "\n",
    "starting_epoch = 0\n",
    "try:\n",
    "    breakpoint_epoch\n",
    "except NameError:\n",
    "    print(\"breakpoint epoch not defined, start training from epoch 0\")\n",
    "else:\n",
    "    print(\"continue training from epoch\", breakpoint_epoch)\n",
    "    starting_epoch = breakpoint_epoch\n",
    "\n",
    "for i in range(starting_epoch, EPOCH_NUM):\n",
    "    epoch_start_time = time.time()\n",
    "    s_m.train()\n",
    "    po_m.train()\n",
    "    for step, loader_res in tqdm(iter(enumerate(loader))):\n",
    "        # print(get_now_time())\n",
    "        t_s = loader_res[\"T\"].to(device)\n",
    "        k1 = loader_res[\"K1\"].to(device)\n",
    "        k2 = loader_res[\"K2\"].to(device)\n",
    "        s1 = loader_res[\"S1\"].to(device)\n",
    "        s2 = loader_res[\"S2\"].to(device)\n",
    "        o1 = loader_res[\"O1\"].to(device)\n",
    "        o2 = loader_res[\"O2\"].to(device)\n",
    "\n",
    "        ps_1, ps_2, t, t_max, mask = s_m(t_s)\n",
    "\n",
    "        t, t_max, k1, k2 = t.to(device), t_max.to(\n",
    "            device), k1.to(device), k2.to(device)\n",
    "        po_1, po_2 = po_m(t, t_max, k1, k2)\n",
    "\n",
    "        ps_1 = ps_1.to(device)\n",
    "        ps_2 = ps_2.to(device)\n",
    "        po_1 = po_1.to(device)\n",
    "        po_2 = po_2.to(device)\n",
    "\n",
    "        s1 = torch.unsqueeze(s1, 2)\n",
    "        s2 = torch.unsqueeze(s2, 2)\n",
    "\n",
    "        s1_loss = b_loss(ps_1, s1)\n",
    "        s1_loss = torch.sum(s1_loss.mul(mask))/torch.sum(mask)\n",
    "        s2_loss = b_loss(ps_2, s2)\n",
    "        s2_loss = torch.sum(s2_loss.mul(mask))/torch.sum(mask)\n",
    "\n",
    "        po_1 = po_1.permute(0, 2, 1)\n",
    "        po_2 = po_2.permute(0, 2, 1)\n",
    "\n",
    "        o1_loss = loss(po_1, o1)\n",
    "        o1_loss = torch.sum(o1_loss.mul(mask[:, :, 0])) / torch.sum(mask)\n",
    "        o2_loss = loss(po_2, o2)\n",
    "        o2_loss = torch.sum(o2_loss.mul(mask[:, :, 0])) / torch.sum(mask)\n",
    "\n",
    "        loss_sum = 2.5 * (s1_loss + s2_loss) + (o1_loss + o2_loss)\n",
    "\n",
    "        # if step % 500 == 0:\n",
    "        # \ttorch.save(s_m, 'models_real/s_'+str(step)+\"epoch_\"+str(i)+'.pkl')\n",
    "        # \ttorch.save(po_m, 'models_real/po_'+str(step)+\"epoch_\"+str(i)+'.pkl')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_sum.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        torch.save(s_m, 'models_real/s_'+str(i)+'.pkl')\n",
    "        torch.save(po_m, 'models_real/po_'+str(i)+'.pkl')\n",
    "    f1, precision, recall = evaluate()\n",
    "\n",
    "    print(\"epoch:\", i, \"loss:\", loss_sum.data)\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_time_elapsed = epoch_end_time - epoch_start_time\n",
    "    print(\"epoch {} used {} seconds (with bsz={})\".format(i, epoch_time_elapsed, BATCH_SIZE))\n",
    "\n",
    "    if f1 >= best_f1:\n",
    "        best_f1 = f1\n",
    "        best_epoch = i\n",
    "\n",
    "    print('f1: %.4f, precision: %.4f, recall: %.4f, bestf1: %.4f, bestepoch: %d \\n ' % (\n",
    "        f1, precision, recall, best_f1, best_epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b553bca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
