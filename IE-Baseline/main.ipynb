{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbd9043b",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "Adapted from the original `main.py`. Intergrated with AWS SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7036660",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "(actually only tqdm, since other packages are pre-installed in aws pytorch environments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b98029c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.61.1-py2.py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 5.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard\n",
      "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 27.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 60.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 2)) (1.0.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 2)) (0.36.2)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 2)) (2.25.1)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.31.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 67.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 57.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 2)) (3.15.8)\n",
      "Collecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.38.0-cp36-cp36m-manylinux2014_x86_64.whl (4.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.2 MB 66.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.4\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 74.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 11.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 2)) (1.19.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 2)) (49.6.0.post20210108)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from absl-py>=0.4->tensorboard->-r requirements.txt (line 2)) (1.15.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 2)) (4.7.2)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 73.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 2)) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 2)) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 2)) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 2)) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 2)) (3.0.4)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 72.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->-r requirements.txt (line 2)) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->-r requirements.txt (line 2)) (3.7.4.3)\n",
      "Installing collected packages: pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, absl-py, tqdm, tensorboard\n",
      "Successfully installed absl-py-0.13.0 cachetools-4.2.2 google-auth-1.31.0 google-auth-oauthlib-0.4.4 grpcio-1.38.0 markdown-3.3.4 oauthlib-3.1.1 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tqdm-4.61.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "585afd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from tqdm import tqdm\n",
    "import model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "#import data_prepare\n",
    "import os\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f40f5bb",
   "metadata": {},
   "source": [
    "Define a tensorboard logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "598c6cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir='./logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15a1d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3,4,5,6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b0ca4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for macOS compatibility\n",
    "#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "CHAR_SIZE = 128\n",
    "SENT_LENGTH = 4\n",
    "HIDDEN_SIZE = 64\n",
    "EPOCH_NUM = 210\n",
    "BATCH_SIZE = 5096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a2c850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910c7926",
   "metadata": {},
   "source": [
    "Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13816e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_now_time():\n",
    "    a = time.time()\n",
    "    return time.ctime(a)\n",
    "\n",
    "\n",
    "def seq_padding(X):\n",
    "    L = [len(x) for x in X]\n",
    "    ML = max(L)\n",
    "    # print(\"ML\",ML)\n",
    "    return [x + [0] * (ML - len(x)) for x in X]\n",
    "\n",
    "\n",
    "def seq_padding_vec(X):\n",
    "    L = [len(x) for x in X]\n",
    "    ML = max(L)\n",
    "    # print(\"ML\",ML)\n",
    "    return [x + [[1, 0]] * (ML - len(x)) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccf1ca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(self, data, batch_size=64):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = len(self.data) // self.batch_size\n",
    "        if len(self.data) % self.batch_size != 0:\n",
    "            self.steps += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "\n",
    "    def pro_res(self):\n",
    "        idxs = list(range(len(self.data)))\n",
    "        # print(idxs)\n",
    "        np.random.shuffle(idxs)\n",
    "        T, S1, S2, K1, K2, O1, O2, = [], [], [], [], [], [], []\n",
    "        for i in idxs:\n",
    "            d = self.data[i]\n",
    "            text = d['text']\n",
    "            items = {}\n",
    "            items = defaultdict(list)\n",
    "            for sp in d['spo_list']:\n",
    "                subjectid = text.find(sp[0])\n",
    "                objectid = text.find(sp[2])\n",
    "                if subjectid != -1 and objectid != -1:\n",
    "                    key = (subjectid, subjectid+len(sp[0])) # key is the span(start, end) of the subject\n",
    "                    # items is {(S_start, S_end): list of (O_start_pos, O_end_pos, predicate_id)}\n",
    "                    items[key].append(\n",
    "                        (objectid, objectid+len(sp[2]), predicate2id[sp[1]]))\n",
    "            if items:\n",
    "                # T is list of text tokens(ids)\n",
    "                T.append([char2id.get(c, 1) for c in text])  # 1是unk，0是padding\n",
    "         \n",
    "                # s1: one-hot vector where start of subject is 1\n",
    "                # s2: one-hot vector where end of subject is 1\n",
    "                s1, s2 = [0] * len(text), [0] * len(text)\n",
    "                for j in items:\n",
    "                    s1[j[0]] = 1\n",
    "                    s2[j[1]-1] = 1\n",
    "                # print(items.keys())\n",
    "                # k1, k2: randomly sampled (S_start, S_end) pair?\n",
    "                k1, k2 = choice(list(items.keys()))\n",
    "                # o1: zero vector, the start of each O is marked with its predicate ID\n",
    "                # o2: zero vector, the end of each O is marked with its predicate ID\n",
    "                o1, o2 = [0] * len(text), [0] * len(text)  # 0是unk类（共49+1个类）\n",
    "                for j in items[(k1, k2)]:\n",
    "                    o1[j[0]] = j[2]\n",
    "                    o2[j[1]-1] = j[2]\n",
    "                S1.append(s1)\n",
    "                S2.append(s2)\n",
    "                K1.append([k1])\n",
    "                K2.append([k2-1])\n",
    "                O1.append(o1)\n",
    "                O2.append(o2)\n",
    "\n",
    "        T = np.array(seq_padding(T))\n",
    "        S1 = np.array(seq_padding(S1))\n",
    "        S2 = np.array(seq_padding(S2))\n",
    "        O1 = np.array(seq_padding(O1))\n",
    "        O2 = np.array(seq_padding(O2))\n",
    "        K1, K2 = np.array(K1), np.array(K2)\n",
    "        return [T, S1, S2, K1, K2, O1, O2]\n",
    "\n",
    "\n",
    "class MyDataset(Data.Dataset):\n",
    "    \"\"\"\n",
    "        下载数据、初始化数据，都可以在这里完成\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, _T, _S1, _S2, _K1, _K2, _O1, _O2):\n",
    "        # xy = np.loadtxt('../dataSet/diabetes.csv.gz', delimiter=',', dtype=np.float32) # 使用numpy读取数据\n",
    "        self.x_data = _T\n",
    "        self.y1_data = _S1\n",
    "        self.y2_data = _S2\n",
    "        self.k1_data = _K1\n",
    "        self.k2_data = _K2\n",
    "        self.o1_data = _O1\n",
    "        self.o2_data = _O2\n",
    "        self.len = len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y1_data[index], self.y2_data[index], self.k1_data[index], self.k2_data[index], self.o1_data[index], self.o2_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    t = np.array([item[0] for item in data], np.int32)\n",
    "    s1 = np.array([item[1] for item in data], np.int32)\n",
    "    s2 = np.array([item[2] for item in data], np.int32)\n",
    "    k1 = np.array([item[3] for item in data], np.int32)\n",
    "\n",
    "    k2 = np.array([item[4] for item in data], np.int32)\n",
    "    o1 = np.array([item[5] for item in data], np.int32)\n",
    "    o2 = np.array([item[6] for item in data], np.int32)\n",
    "    return {\n",
    "        'T': torch.LongTensor(t),  # targets_i\n",
    "        'S1': torch.FloatTensor(s1),\n",
    "        'S2': torch.FloatTensor(s2),\n",
    "        'K1': torch.LongTensor(k1),\n",
    "        'K2': torch.LongTensor(k2),\n",
    "        'O1': torch.LongTensor(o1),\n",
    "        'O2': torch.LongTensor(o2),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "913436d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_items(text_in):\n",
    "    R = []\n",
    "    _s = [char2id.get(c, 1) for c in text_in]\n",
    "    _s = np.array([_s])\n",
    "    _k1, _k2, t, t_max, mask = s_m(torch.LongTensor(_s).to(device))\n",
    "    _k1, _k2 = _k1[0, :, 0], _k2[0, :, 0]\n",
    "    _kk1s = []\n",
    "    for i, _kk1 in enumerate(_k1):\n",
    "        if _kk1 > 0.5:\n",
    "            _subject = ''\n",
    "            for j, _kk2 in enumerate(_k2[i:]):\n",
    "                if _kk2 > 0.5:\n",
    "                    _subject = text_in[i: i+j+1]\n",
    "                    break\n",
    "            if _subject:\n",
    "                _k1, _k2 = torch.LongTensor([[i]]), torch.LongTensor(\n",
    "                    [[i+j]])  # np.array([i]), np.array([i+j])\n",
    "                _o1, _o2 = po_m(t.to(device), t_max.to(\n",
    "                    device), _k1.to(device), _k2.to(device))\n",
    "                _o1, _o2 = _o1.cpu().data.numpy(), _o2.cpu().data.numpy()\n",
    "\n",
    "                _o1, _o2 = np.argmax(_o1[0], 1), np.argmax(_o2[0], 1)\n",
    "\n",
    "                for i, _oo1 in enumerate(_o1):\n",
    "                    if _oo1 > 0:\n",
    "                        for j, _oo2 in enumerate(_o2[i:]):\n",
    "                            if _oo2 == _oo1:\n",
    "                                _object = text_in[i: i+j+1]\n",
    "                                _predicate = id2predicate[_oo1]\n",
    "                                # print((_subject, _predicate, _object))\n",
    "                                R.append((_subject, _predicate, _object))\n",
    "                                break\n",
    "        _kk1s.append(_kk1.data.cpu().numpy())\n",
    "    _kk1s = np.array(_kk1s)\n",
    "    return list(set(R))\n",
    "\n",
    "def para_extract_items(loader_res):\n",
    "    t_s = loader_res[\"T\"].to(device)\n",
    "    k1 = loader_res[\"K1\"].to(device)\n",
    "    k2 = loader_res[\"K2\"].to(device)\n",
    "    s1 = loader_res[\"S1\"].to(device)\n",
    "    s2 = loader_res[\"S2\"].to(device)\n",
    "    o1 = loader_res[\"O1\"].to(device)\n",
    "    o2 = loader_res[\"O2\"].to(device)\n",
    "\n",
    "    ps_1, ps_2, t, t_max, mask = s_m(t_s)\n",
    "\n",
    "    t, t_max, k1, k2 = t.to(device), t_max.to(\n",
    "        device), k1.to(device), k2.to(device)\n",
    "    po_1, po_2 = po_m(t, t_max, k1, k2)\n",
    "\n",
    "    ps_1 = ps_1.to(device)\n",
    "    ps_2 = ps_2.to(device)\n",
    "    po_1 = po_1.to(device)\n",
    "    po_2 = po_2.to(device)\n",
    "\n",
    "    s1 = torch.unsqueeze(s1, 2)\n",
    "    s2 = torch.unsqueeze(s2, 2)\n",
    "    \n",
    "    \n",
    "    \n",
    "            \n",
    "def para_evaluate():\n",
    "    A, B, C = 1e-10, 1e-10, 1e-10\n",
    "    cnt = 0\n",
    "    s_m.eval()\n",
    "    po_m.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, loader_res in tqdm(iter(enumerate(loader))):\n",
    "            R = set(para_extract_items(loader_res))\n",
    "            T = None\n",
    "            A += len(R & T)\n",
    "            B += len(R)\n",
    "            C += len(T)\n",
    "            cnt += 1\n",
    "    return 2 * A / (B + C), A / B, A / C\n",
    "    \n",
    "\n",
    "def evaluate():\n",
    "    A, B, C = 1e-10, 1e-10, 1e-10\n",
    "    cnt = 0\n",
    "    s_m.eval()\n",
    "    po_m.eval()\n",
    "    for d in tqdm(iter(dev_data)):\n",
    "        if cnt > 5000:\n",
    "            break\n",
    "        R = set(extract_items(d['text']))\n",
    "        T = set([tuple(i) for i in d['spo_list']])\n",
    "        A += len(R & T)\n",
    "        B += len(R)\n",
    "        C += len(T)\n",
    "        # if cnt % 1000 == 0:\n",
    "        #     print('iter: %d f1: %.4f, precision: %.4f, recall: %.4f\\n' % (cnt, 2 * A / (B + C), A / B, A / C))\n",
    "        cnt += 1\n",
    "    return 2 * A / (B + C), A / B, A / C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134c1c41",
   "metadata": {},
   "source": [
    "# Download training data\n",
    "Skip the downloading step if you have alreay done it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daf5cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://dataset-bj.cdn.bcebos.com/qianyan/DuIE_2_0.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "badf0e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip -j DuIE_2_0.zip -d data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142acd4e",
   "metadata": {},
   "source": [
    "Transofm raw data to easier usable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df321b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir generated\n",
    "# !python trans.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea743ef0",
   "metadata": {},
   "source": [
    "## Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "126451cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'generated/train_data_me.json'\n",
    "dev_path = 'generated/dev_data_me.json'\n",
    "generated_schema_path =  'generated/schemas_me.json'\n",
    "generated_char_path = 'generated/all_chars_me.json'\n",
    "train_data = json.load(open(train_path))\n",
    "dev_data = json.load(open(dev_path))\n",
    "id2predicate, predicate2id = json.load(open(generated_schema_path))\n",
    "id2predicate = {int(i): j for i, j in id2predicate.items()}\n",
    "id2char, char2id = json.load(open(generated_char_path))\n",
    "num_classes = len(id2predicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b162025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = DataGenerator(train_data)\n",
    "T, S1, S2, K1, K2, O1, O2 = dg.pro_res()\n",
    "# print(\"len\",len(T))\n",
    "\n",
    "torch_dataset = MyDataset(T, S1, S2, K1, K2, O1, O2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d910d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Data.DataLoader(\n",
    "    dataset=torch_dataset,      # torch TensorDataset format\n",
    "    batch_size=BATCH_SIZE,      # mini batch size\n",
    "    shuffle=True,               # random shuffle for training\n",
    "    num_workers=64,\n",
    "    collate_fn=collate_fn,      # subprocesses for loading data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7da46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dg = DataGenerator(dev_data)\n",
    "T_dev, S1_dev, S2_dev, K1_dev, K2_dev, O1_dev, O2_dev = dev_dg.pro_res()\n",
    "dev_dataset = MyDataset(T_dev, S1_dev, S2_dev, K1_dev, K2_dev, O1_dev, O2_dev)\n",
    "dev_loader = Data.DataLoader(\n",
    "    dataset=dev_dataset,      # torch TensorDataset format\n",
    "    batch_size=BATCH_SIZE,      # mini batch size\n",
    "    shuffle=True,               # random shuffle for training\n",
    "    num_workers=64,\n",
    "    collate_fn=collate_fn,      # subprocesses for loading data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064d9ba5",
   "metadata": {},
   "source": [
    "### Define model and loss\n",
    "Data are parallimised  to multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d178d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_m = model.s_model(len(char2id)+2, CHAR_SIZE, HIDDEN_SIZE)\n",
    "po_m = model.po_model(len(char2id)+2, CHAR_SIZE, HIDDEN_SIZE, 49)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print('Using', torch.cuda.device_count(), \"GPUs!\")\n",
    "    s_m = nn.DataParallel(s_m)\n",
    "    po_m = nn.DataParallel(po_m)\n",
    "\n",
    "s_m = s_m.to(device)\n",
    "po_m = po_m.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f66d42",
   "metadata": {},
   "source": [
    "### Load model if needed\n",
    "Uncomment lines below to load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19805a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpoint_epoch = 197\n",
    "model_dir = 'models_real'\n",
    "s_m = torch.load(os.path.join(model_dir, \"s_{}.pkl\".format(breakpoint_epoch)), map_location=device)\n",
    "po_m = torch.load(os.path.join(model_dir, \"po_{}.pkl\".format(breakpoint_epoch)), map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4af3c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_m = nn.DataParallel(s_m.module)\n",
    "po_m = nn.DataParallel(po_m.module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7263960",
   "metadata": {},
   "source": [
    "### Define loss metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9f0578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(s_m.parameters())\n",
    "params += list(po_m.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss().to(device)\n",
    "b_loss = torch.nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2928ae4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab044aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continue training from epoch 197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/modules/rnn.py:582: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370116979/work/aten/src/ATen/native/cudnn/RNN.cpp:775.)\n",
      "  self.dropout, self.training, self.bidirectional, self.batch_first)\n",
      "34it [02:36,  4.59s/it]\n",
      "5001it [01:37, 51.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 197 loss: tensor(0.0262, device='cuda:0')\n",
      "epoch 197 used 255.26342701911926 seconds (with bsz=5096)\n",
      "f1: 0.5739, precision: 0.7953, recall: 0.4489, bestf1: 0.5739, bestepoch: 197 \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [02:15,  3.99s/it]\n",
      "5001it [01:23, 59.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 198 loss: tensor(0.0256, device='cuda:0')\n",
      "epoch 198 used 222.451429605484 seconds (with bsz=5096)\n",
      "f1: 0.5829, precision: 0.7959, recall: 0.4598, bestf1: 0.5829, bestepoch: 198 \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "34it [02:15,  3.99s/it]\n",
      "5001it [01:25, 58.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 199 loss: tensor(0.0276, device='cuda:0')\n",
      "epoch 199 used 224.8002152442932 seconds (with bsz=5096)\n",
      "f1: 0.5754, precision: 0.7856, recall: 0.4539, bestf1: 0.5829, bestepoch: 198 \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "34it [02:15,  4.00s/it]\n",
      "5001it [01:23, 60.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 200 loss: tensor(0.0270, device='cuda:0')\n",
      "epoch 200 used 222.2125391960144 seconds (with bsz=5096)\n",
      "f1: 0.5821, precision: 0.7888, recall: 0.4613, bestf1: 0.5829, bestepoch: 198 \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "34it [02:15,  3.98s/it]\n",
      "2863it [00:47, 64.94it/s]"
     ]
    }
   ],
   "source": [
    "best_f1 = 0\n",
    "best_epoch = 0\n",
    "\n",
    "starting_epoch = 197\n",
    "\n",
    "try:\n",
    "    breakpoint_epoch\n",
    "except NameError:\n",
    "    print(\"breakpoint epoch not defined, start training from epoch 0\")\n",
    "else:\n",
    "    print(\"continue training from epoch\", breakpoint_epoch)\n",
    "    starting_epoch = breakpoint_epoch\n",
    "\n",
    "for i in range(starting_epoch, EPOCH_NUM):\n",
    "    epoch_start_time = time.time()\n",
    "    s_m.train()\n",
    "    po_m.train()\n",
    "    for step, loader_res in tqdm(iter(enumerate(loader))):\n",
    "        # print(get_now_time())\n",
    "        t_s = loader_res[\"T\"].to(device)\n",
    "        k1 = loader_res[\"K1\"].to(device)\n",
    "        k2 = loader_res[\"K2\"].to(device)\n",
    "        s1 = loader_res[\"S1\"].to(device)\n",
    "        s2 = loader_res[\"S2\"].to(device)\n",
    "        o1 = loader_res[\"O1\"].to(device)\n",
    "        o2 = loader_res[\"O2\"].to(device)\n",
    "\n",
    "        ps_1, ps_2, t, t_max, mask = s_m(t_s)\n",
    "\n",
    "        t, t_max, k1, k2 = t.to(device), t_max.to(\n",
    "            device), k1.to(device), k2.to(device)\n",
    "        po_1, po_2 = po_m(t, t_max, k1, k2)\n",
    "\n",
    "        ps_1 = ps_1.to(device)\n",
    "        ps_2 = ps_2.to(device)\n",
    "        po_1 = po_1.to(device)\n",
    "        po_2 = po_2.to(device)\n",
    "\n",
    "        s1 = torch.unsqueeze(s1, 2)\n",
    "        s2 = torch.unsqueeze(s2, 2)\n",
    "\n",
    "        s1_loss = b_loss(ps_1, s1)\n",
    "        s1_loss = torch.sum(s1_loss.mul(mask))/torch.sum(mask)\n",
    "        s2_loss = b_loss(ps_2, s2)\n",
    "        s2_loss = torch.sum(s2_loss.mul(mask))/torch.sum(mask)\n",
    "\n",
    "        po_1 = po_1.permute(0, 2, 1)\n",
    "        po_2 = po_2.permute(0, 2, 1)\n",
    "\n",
    "        o1_loss = loss(po_1, o1)\n",
    "        o1_loss = torch.sum(o1_loss.mul(mask[:, :, 0])) / torch.sum(mask)\n",
    "        o2_loss = loss(po_2, o2)\n",
    "        o2_loss = torch.sum(o2_loss.mul(mask[:, :, 0])) / torch.sum(mask)\n",
    "\n",
    "        loss_sum = 2.5 * (s1_loss + s2_loss) + (o1_loss + o2_loss)\n",
    "\n",
    "        # if step % 500 == 0:\n",
    "        # \ttorch.save(s_m, 'models_real/s_'+str(step)+\"epoch_\"+str(i)+'.pkl')\n",
    "        # \ttorch.save(po_m, 'models_real/po_'+str(step)+\"epoch_\"+str(i)+'.pkl')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_sum.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    torch.save(s_m, 'models_real/s_'+str(i)+'.pkl')\n",
    "    torch.save(po_m, 'models_real/po_'+str(i)+'.pkl')\n",
    "    f1, precision, recall = evaluate()\n",
    "\n",
    "    print(\"epoch:\", i, \"loss:\", loss_sum.data)\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_time_elapsed = epoch_end_time - epoch_start_time\n",
    "    print(\"epoch {} used {} seconds (with bsz={})\".format(i, epoch_time_elapsed, BATCH_SIZE))\n",
    "    writer.add_scalar('Loss/train', loss_sum.data, i)\n",
    "    writer.add_scalar('f1', f1, i)\n",
    "    writer.add_scalar('precision', precision, i)\n",
    "    writer.add_scalar('recall', recall, i)\n",
    "\n",
    "    if f1 >= best_f1:\n",
    "        best_f1 = f1\n",
    "        best_epoch = i\n",
    "\n",
    "    print('f1: %.4f, precision: %.4f, recall: %.4f, bestf1: %.4f, bestepoch: %d \\n ' % (\n",
    "        f1, precision, recall, best_f1, best_epoch))\n",
    "\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5921c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4070b564",
   "metadata": {},
   "source": [
    "## Test the trained model on some texts\n",
    "Extract plain model from Dataparalell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ddc583",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_m = s_m.module\n",
    "po_m = po_m.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5177379b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  《步步惊心》改编自著名作家桐华的同名清穿小说《甄嬛传》改编自流潋紫所著的同名小说电视剧《何以笙箫默》改编自顾漫同名小说《花千骨》改编自fresh果果同名小说《裸婚时代》是月影兰析创作的一部情感小说《琅琊榜》是根据海宴同名网络小说改编电视剧《宫锁心玉》，又名《宫》《雪豹》，该剧改编自网络小说《特战先驱》《我是特种兵》由红遍网络的小说《最后一颗子弹留给我》改编电视剧《来不及说我爱你》改编自匪我思存同名小说《来不及说我爱你》\n",
      "Predicted SPOs:  [('步步惊心', '改编自', '最后一颗子弹留给我'), ('步步惊心', '作者', '顾漫'), ('步步惊心', '作者', '桐华'), ('步步惊心', '改编自', '特战先驱》《我是特种兵》由红遍网络的小说《最后一颗子弹留给我'), ('步步惊心', '改编自', '甄嬛传》改编自流潋紫所著的同名小说电视剧《何以笙箫默》改编自顾漫同名小说《花千骨'), ('步步惊心', '改编自', '花千骨'), ('步步惊心', '改编自', '裸婚时代》是月影兰析创作的一部情感小说《琅琊榜》是根据海宴同名网络小说改编电视剧《宫锁心玉》，又名《宫》《雪豹》，该剧改编自网络小说《特战先驱》《我是特种兵》由红遍网络的小说《最后一颗子弹留给我'), ('步步惊心', '改编自', '何以笙箫默》改编自顾漫同名小说《花千骨')]\n",
      "Ground Truth SPOs:  [['何以笙箫默', '作者', '顾漫'], ['我是特种兵', '改编自', '最后一颗子弹留给我'], ['步步惊心', '作者', '桐华'], ['甄嬛传', '作者', '流潋紫'], ['花千骨', '作者', 'fresh果果'], ['裸婚时代', '作者', '月影兰析'], ['琅琊榜', '作者', '海宴'], ['雪豹', '改编自', '特战先驱'], ['来不及说我爱你', '改编自', '来不及说我爱你'], ['来不及说我爱你', '作者', '匪我思存']]\n",
      "Text:  摩尔多瓦共和国（摩尔多瓦语：Republica Moldova，英语：Republic of Moldova），简称摩尔多瓦，是位于东南欧的内陆国，与罗马尼亚和乌克兰接壤，首都基希讷乌\n",
      "Predicted SPOs:  []\n",
      "Ground Truth SPOs:  [['摩尔多瓦', '首都', '基希讷乌']]\n",
      "Text:  2月19日，96岁的资深演员侯焕玲离世，候婆婆一生未嫁，但一直热爱电影，她曾在《回魂夜》和《喜剧之王》等电影饰演婆婆一角，而临终前候婆婆一直说，自己好喜欢电影，好喜欢周星驰\n",
      "Predicted SPOs:  [('侯焕玲离世，候婆婆一生未嫁，但一直热爱电影，她曾在《回魂夜》和《喜剧之王', '主演', '侯焕玲')]\n",
      "Ground Truth SPOs:  [['喜剧之王', '编剧', '周星驰']]\n",
      "Text:  这件婚事原本与陈国峻无关，但陈国峻却“欲求配而无由，夜间乃潜入天城公主所居通之\n",
      "Predicted SPOs:  []\n",
      "Ground Truth SPOs:  [['天城公主', '丈夫', '国峻'], ['国峻', '妻子', '天城公主']]\n",
      "Text:  情人节大盘约在4亿特工票房1.32拉拉蓝1.05其他没所谓了@0216幻影9527 @朦胧之于暖春 @Jqpiero @娶个明星这么难 @星爷最低调\n",
      "Predicted SPOs:  []\n",
      "Ground Truth SPOs:  [['情人节', '票房', '4亿']]\n",
      "Text:  《父老乡亲》是由是由由中国人民解放军海政文工团创作的军旅歌曲，石顺义作词，王锡仁作曲，范琳琳演唱\n",
      "Predicted SPOs:  [('父老乡亲', '歌手', '范琳琳'), ('父老乡亲', '作曲', '王锡仁'), ('父老乡亲', '作词', '石顺义')]\n",
      "Ground Truth SPOs:  [['父老乡亲', '歌手', '范琳琳'], ['石顺义', '国籍', '中国'], ['父老乡亲', '作词', '石顺义'], ['父老乡亲', '作曲', '王锡仁']]\n",
      "Text:  2019年2月25日和26日，温氏股份实控人之一、前任董事长温鹏程之妻伍翠珍分别减持公司股票608万股和256万股，成交均价分别为30.78元/股和30.02元/股，共计套现约2.64亿元\n",
      "Predicted SPOs:  [('温氏股份实控人之一、前任董事长温鹏程', '妻子', '伍翠珍')]\n",
      "Ground Truth SPOs:  [['温氏股份', '董事长', '温鹏程'], ['伍翠珍', '丈夫', '温鹏程'], ['温鹏程', '妻子', '伍翠珍']]\n",
      "Text:  宋竹范，口腔医生，女，宋竹范主任医师毕业于佳木斯医学院，在国内三甲医院从事口腔科临床工作三十余年，有丰富的口腔科临床工作经验，熟练掌握口腔内科、口腔外科、儿童口腔科各种常见病及多发病的诊治，以及多项口腔矫形技术\n",
      "Predicted SPOs:  [('宋竹范', '毕业院校', '佳木斯医学院')]\n",
      "Ground Truth SPOs:  [['宋竹范', '毕业院校', '佳木斯医学院']]\n",
      "Text:  由江苏艺星影视文化传播有限公司投资，演员赵荀、傅程鹏、程愫、侯梦莎、任柯诺、安雅萍、杨舒、张进、杨山等主演的大型谍战题材电视剧《与狼共舞2》正在江苏卫视\n",
      "Predicted SPOs:  [('与狼共舞2', '主演', '侯梦莎'), ('与狼共舞2', '主演', '任柯诺'), ('与狼共舞2', '主演', '安雅萍'), ('与狼共舞2', '主演', '傅程鹏'), ('与狼共舞2', '主演', '张进'), ('与狼共舞2', '主演', '杨舒'), ('与狼共舞2', '出品公司', '江苏艺星影视文化传播有限公司'), ('与狼共舞2', '主演', '杨山'), ('与狼共舞2', '主演', '程愫'), ('与狼共舞2', '主演', '赵荀')]\n",
      "Ground Truth SPOs:  [['与狼共舞2', '主演', '赵荀'], ['与狼共舞2', '主演', '侯梦莎'], ['与狼共舞2', '主演', '任柯诺'], ['与狼共舞2', '主演', '程愫'], ['与狼共舞2', '主演', '傅程鹏'], ['与狼共舞2', '主演', '安雅萍'], ['与狼共舞2', '主演', '杨舒'], ['与狼共舞2', '主演', '张进']]\n",
      "Text:  科库雷克(RadovanKocurek),出生于1986年2月12日，捷克国籍，身高179厘米，体重72公斤，场上位置前锋，现在效力于贾洛内足球俱乐部\n",
      "Predicted SPOs:  [('科库雷克', '国籍', '捷克')]\n",
      "Ground Truth SPOs:  [['科库雷克', '国籍', '捷克']]\n",
      "Text:  《外国民间歌曲选》是2004年人民音乐出版社出版的图书，作者是温恒泰\n",
      "Predicted SPOs:  [('外国民间歌曲选', '作者', '温恒泰')]\n",
      "Ground Truth SPOs:  [['外国民间歌曲选', '作者', '温恒泰']]\n",
      "Text:  平清盛随后在治承四年（1180年）二月迫使高仓天皇退位，拥立自己的孙子，平德子之子即位，是为安德天皇\n",
      "Predicted SPOs:  []\n",
      "Ground Truth SPOs:  [['安德天皇', '父亲', '高仓天皇'], ['安德天皇', '母亲', '平德子'], ['安德天皇', '父亲', '平德子']]\n",
      "Text:  《恋着多喜欢》是梁静茹2005年发行的一首《亲亲》专辑预售单曲，同时也收录在非大陆版的新版《神雕侠侣》原声带中\n",
      "Predicted SPOs:  []\n",
      "Ground Truth SPOs:  [['亲亲', '歌手', '梁静茹'], ['恋着多喜欢', '歌手', '梁静茹']]\n",
      "Text:  今天分享一段很火的bgm，《何以笙箫默》的插曲《好久不见》，视频很短，真的因为只有前奏的谱子呀\n",
      "Predicted SPOs:  []\n",
      "Ground Truth SPOs:  [['何以笙箫默', '主题曲', '好久不见']]\n",
      "Text:  赵允弼（1007—1069），宋太宗赵炅之孙，镇恭懿王赵元偓之子\n",
      "Predicted SPOs:  []\n",
      "Ground Truth SPOs:  [['赵元偓', '朝代', '宋']]\n",
      "Text:  嘉兴中润光学科技有限公司于2012年08月27日在嘉兴市工商局经济开发区分局登记成立\n",
      "Predicted SPOs:  [('嘉兴中润光学科技有限公司', '成立日期', '2012年08月27日')]\n",
      "Ground Truth SPOs:  [['嘉兴中润光学科技有限公司', '成立日期', '2012年08月27日']]\n",
      "Text:  1997年与拍档关咏荷凭借《醉打金枝》在万千星辉颁奖典礼中夺得“最佳惹笑冤家大奖”\n",
      "Predicted SPOs:  []\n",
      "Ground Truth SPOs:  [['醉打金枝', '上映时间', '1997年'], ['醉打金枝', '主演', '关咏荷']]\n",
      "Text:  当然这次旺旺新品营销之所以这么成功，也与产品本身自带热度有关：1)产品拥有知名度旺旺集团起家于1962年成立的宜兰食品，1979年自创品牌“旺旺”(英文名 Want Want)，并打造吉祥物“旺仔\n",
      "Predicted SPOs:  [('旺旺集团', '成立日期', '1962年')]\n",
      "Ground Truth SPOs:  [['旺旺集团', '成立日期', '1962年']]\n",
      "Text:  《我的父亲是板凳》是由中国国际电视总公司出品的电视剧，黄文利和张景坤执导，王宝强、陶虹、张子枫、傅程鹏、午马等主演1\n",
      "Predicted SPOs:  [('我的父亲是板凳', '主演', '王宝强'), ('我的父亲是板凳', '出品公司', '中国国际电视总公司'), ('我的父亲是板凳', '导演', '黄文利'), ('我的父亲是板凳', '导演', '张景坤'), ('我的父亲是板凳', '主演', '张子枫'), ('我的父亲是板凳', '主演', '陶虹'), ('我的父亲是板凳', '主演', '午马'), ('我的父亲是板凳', '主演', '傅程鹏')]\n",
      "Ground Truth SPOs:  [['我的父亲是板凳', '主演', '午马'], ['我的父亲是板凳', '主演', '王宝强'], ['我的父亲是板凳', '主演', '陶虹'], ['我的父亲是板凳', '主演', '傅程鹏'], ['我的父亲是板凳', '导演', '黄文利'], ['我的父亲是板凳', '主演', '张子枫'], ['我的父亲是板凳', '出品公司', '中国国际电视总公司']]\n",
      "Text:  《玻璃爱情》是2006年播出的剧情电视剧，由 宋祖德、王丽娜等主演\n",
      "Predicted SPOs:  [('玻璃爱情', '主演', '宋祖德'), ('玻璃爱情', '主演', '王丽娜')]\n",
      "Ground Truth SPOs:  [['玻璃爱情', '上映时间', '2006年']]\n",
      "Text:  秘鲁共和国（西班牙语：La República del Perú），简称秘鲁（Peru），是南美洲西部的一个国家，北邻厄瓜多尔和哥伦比亚，东与巴西和玻利维亚接壤，南接智利，西濒太平洋\n",
      "Predicted SPOs:  []\n",
      "Ground Truth SPOs:  [['秘鲁', '官方语言', '西班牙语']]\n"
     ]
    }
   ],
   "source": [
    "to_print = 20\n",
    "for cnt, d in enumerate(dev_data):\n",
    "    if cnt > to_print:\n",
    "        break\n",
    "    print('Text: ', d['text'])\n",
    "    print('Predicted SPOs: ', extract_items(d['text']))\n",
    "    print('Ground Truth SPOs: ', d['spo_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590735bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the model\n",
    "writer.add_graph(s_m, torch.from_numpy(x).float())\n",
    "writer.add_graph(po_m, torch.fr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
