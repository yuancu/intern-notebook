{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1a146ef",
   "metadata": {},
   "source": [
    "# Query Mapping - Natural Language Understanding Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c6478a",
   "metadata": {},
   "source": [
    "## Manually create data for natural language understanding\n",
    "\n",
    "In order to map natural language query to formatted query, we first need to create some labeled data.\n",
    "\n",
    "For validation purpose, I'll only create data for several simple queries with a predefined pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40be4b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b714aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_path = 'data/dev.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c718561a",
   "metadata": {},
   "source": [
    "Labels are character based! This means a number '431' would be separated into 3 tokens, an English word 'lemon' would become 5 tokens. This is for implementation convinience. Please take this into account when you load texts and tokenize them with other methods.\n",
    "\n",
    "We'll create a small number of data from dev dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d856bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import sample\n",
    "\n",
    "# number of dialogues to sample our data\n",
    "num_dial = 5000\n",
    "\n",
    "# count total number of dev\n",
    "with open(dev_path) as f:\n",
    "    line_cnt = sum(1 for _ in f)\n",
    "\n",
    "random.seed(42)\n",
    "dial_idxs = sample(range(line_cnt), num_dial)\n",
    "# print(\"sampled dialogue indexes:\", dial_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df120f21",
   "metadata": {},
   "source": [
    "For dataset generation, I defined some naive patterns for every task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57f1a2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slot labels: ['PAD', 'UNK', 'O', 'B_name', 'I_name', 'B_school', 'I_school', 'B_book', 'I_book', 'B_film', 'I_film', 'B_country', 'I_country']\n",
      "intentions: ['UNK', 'ask_school', 'ask_alumni', 'ask_author', 'ask_books', 'ask_wife', 'ask_husband', 'ask_director', 'ask_films', 'ask_nationality']\n"
     ]
    }
   ],
   "source": [
    "used_predicate_set = set(['毕业院校', '作者', '妻子', '丈夫', '导演', '国籍'])\n",
    "# subject means \"query about subject\"\n",
    "question_templates = {\n",
    "    # {\"object_type\": {\"@value\": \"学校\"}, \"predicate\": \"毕业院校\", \"subject_type\": \"人物\"}\n",
    "    '毕业院校':\n",
    "    {\n",
    "        'ask_subject': [\"有哪些人从{}毕业\", \"{}有哪些知名校友\", \"从{}毕业的名人有哪些\"],\n",
    "        'ask_object': [\"{}毕业于哪里\", \"{}从哪里毕业\", \"{}以前在哪里读书\", \"{}的毕业院校是哪里\", \"{}的毕业院校是什么\"]\n",
    "    },\n",
    "    # {\"object_type\": {\"@value\": \"人物\"}, \"predicate\": \"作者\", \"subject_type\": \"图书作品\"}\n",
    "    '作者':\n",
    "    {\n",
    "        'ask_subject': [\"{}有哪些作品\", \"{}写了什么书\", \"{}写了哪些书\", \"{}有什么著作\"],\n",
    "        'ask_object': [\"{}是谁的作品\", \"{}是谁写的\", \"谁写了{}\", \"{}的作者是谁\"]\n",
    "    },\n",
    "    '妻子':\n",
    "    {\n",
    "        'ask_object': [\"{}的妻子是谁\", \"{}的老婆是谁\", \"{}的配偶是谁\", \"{}和谁结婚了\"]\n",
    "    },\n",
    "    '丈夫':\n",
    "    {\n",
    "        'ask_object': [\"{}的丈夫是谁\", \"{}的老公是谁\", \"{}的配偶是谁\", \"{}和谁结婚了\"]\n",
    "    },\n",
    "    # {\"object_type\": {\"@value\": \"人物\"}, \"predicate\": \"导演\", \"subject_type\": \"影视作品\"}\n",
    "    '导演':\n",
    "    {\n",
    "        'ask_subject': [\"{}有哪些影视作品\", \"{}导演了哪些电影\", \"{}导演了哪些电视剧\"],\n",
    "        'ask_object': [\"{}是谁导演的\", \"{}是谁的作品\", \"{}的导演是谁\"]\n",
    "    },\n",
    "    #{\"object_type\": {\"@value\": \"国家\"}, \"predicate\": \"国籍\", \"subject_type\": \"人物\"}\n",
    "    '国籍':\n",
    "    {\n",
    "        'ask_object': [\"{}来自于哪个国家\", \"{}是哪个国家的人\", \"{}的国籍是什么\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "domain_specific_slot_labels = {\n",
    "    # {\"object_type\": {\"@value\": \"学校\"}, \"predicate\": \"毕业院校\", \"subject_type\": \"人物\"}\n",
    "    '毕业院校': {'subject_label': ['B_name', 'I_name'], 'object_label': ['B_school', 'I_school']},\n",
    "    # {\"object_type\": {\"@value\": \"人物\"}, \"predicate\": \"作者\", \"subject_type\": \"图书作品\"}\n",
    "    '作者': {'subject_label': ['B_book', 'I_book'], 'object_label': ['B_name', 'I_name']},\n",
    "    '妻子': {'subject_label': ['B_name', 'I_name'], 'object_label': ['B_name', 'I_name']},\n",
    "    '丈夫': {'subject_label': ['B_name', 'I_name'], 'object_label': ['B_name', 'I_name']},\n",
    "    # {\"object_type\": {\"@value\": \"人物\"}, \"predicate\": \"导演\", \"subject_type\": \"影视作品\"}\n",
    "    '导演': {'subject_label': ['B_film', 'I_film'], 'object_label': ['B_name', 'I_name']},\n",
    "    #{\"object_type\": {\"@value\": \"国家\"}, \"predicate\": \"国籍\", \"subject_type\": \"人物\"}\n",
    "    '国籍': {'subject_label': ['B_name', 'I_name'], 'object_label': ['B_country', 'I_country']}\n",
    "}\n",
    "\n",
    "domain_specific_intentions = {\n",
    "    '毕业院校':\n",
    "    {\n",
    "        'ask_subject': 'ask_alumni',\n",
    "        'ask_object': 'ask_school'\n",
    "    },\n",
    "    '作者':\n",
    "    {\n",
    "        'ask_subject': 'ask_books',\n",
    "        'ask_object': 'ask_author'\n",
    "    },\n",
    "    '妻子':\n",
    "    {\n",
    "        'ask_object': 'ask_wife'\n",
    "    },\n",
    "    '丈夫':\n",
    "    {\n",
    "        'ask_object': 'ask_husband'\n",
    "    },\n",
    "    '导演':\n",
    "    {\n",
    "        'ask_subject': 'ask_films',\n",
    "        'ask_object': 'ask_director'\n",
    "    },\n",
    "    '国籍':\n",
    "    {\n",
    "        'ask_object': 'ask_nationality'\n",
    "    }\n",
    "}\n",
    "\n",
    "all_slot_labels = ['PAD', 'UNK', 'O', 'B_name', 'I_name'] # name refers to human name\n",
    "all_intentions = ['UNK']\n",
    "for labels in domain_specific_slot_labels.values():\n",
    "    # this keeps orders (compared with using set)\n",
    "    labels = labels['subject_label'] + labels['object_label']\n",
    "    all_slot_labels += [label for label in labels if label not in all_slot_labels]\n",
    "for intentions in domain_specific_intentions.values():\n",
    "    if intentions['ask_object'] not in all_intentions:\n",
    "        all_intentions.append(intentions['ask_object'])\n",
    "    if 'ask_subject' in intentions and intentions['ask_subject'] not in all_intentions:\n",
    "        all_intentions.append(intentions['ask_subject'])\n",
    "print(\"slot labels:\", all_slot_labels)\n",
    "print(\"intentions:\", all_intentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef21bc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated 3584 questions\n"
     ]
    }
   ],
   "source": [
    "import linecache\n",
    "from random import choice, random\n",
    "\n",
    "questions = []\n",
    "question_bios = [] #boundary, inside, outside\n",
    "question_intentions = []\n",
    "\n",
    "for dial_idx in dial_idxs:\n",
    "    line = linecache.getline(dev_path, dial_idx)\n",
    "    spo_list = json.loads(line)['spo_list']\n",
    "    for spo in spo_list:\n",
    "        if spo['predicate'] in used_predicate_set:\n",
    "            question_template = question_templates[spo['predicate']]\n",
    "            # query object with known subject\n",
    "            question_object_template = choice(question_template['ask_object']) # randomly choose a question template\n",
    "            question_object = question_object_template.format(spo['subject'])\n",
    "            # fill bio sequence\n",
    "            question_object_bio = ['O'] * len(question_object)\n",
    "            subject_idx = question_object.find(spo['subject'])\n",
    "            subject_label = domain_specific_slot_labels[spo['predicate']]['subject_label']\n",
    "            question_object_bio[subject_idx] = subject_label[0]\n",
    "            for i in range(subject_idx+1, subject_idx+len(spo['subject'])):\n",
    "                question_object_bio[i] = subject_label[1]\n",
    "            questions.append(question_object)\n",
    "            question_bios.append(question_object_bio)\n",
    "            question_intentions.append(domain_specific_intentions[spo['predicate']]['ask_object'])\n",
    "            # since it is less frequent to query subject, we generate less such query\n",
    "            if random() < 0.5 and 'ask_subject' in question_template:\n",
    "                # query subject with known object\n",
    "                question_subject_template = choice(question_template['ask_subject'])\n",
    "                question_subject = question_subject_template.format(spo['object'])\n",
    "                # fill bio sequence\n",
    "                question_subject_bio = ['O'] * len(question_subject)\n",
    "                object_idx = question_subject.find(spo['object'])\n",
    "                object_label = domain_specific_slot_labels[spo['predicate']]['object_label']\n",
    "                try:\n",
    "                    question_subject_bio[object_idx] = object_label[0]\n",
    "                except:\n",
    "                    print('object_idx', object_idx)\n",
    "                    print('question subject:', question_subject)\n",
    "                    print('question template:', question_template)\n",
    "                    continue\n",
    "                for i in range(object_idx+1, object_idx+len(spo['object'])):\n",
    "                    question_subject_bio[i] = object_label[1]\n",
    "                questions.append(question_subject)\n",
    "                question_bios.append(question_subject_bio)\n",
    "                question_intentions.append(domain_specific_intentions[spo['predicate']]['ask_subject'])\n",
    "print(f\"generated {len(questions)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7c92a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['卿恩彬毕业于哪里',\n",
       " '人与人的相遇是谁的作品',\n",
       " '王怡红写了什么书',\n",
       " '芈月传是谁导演的',\n",
       " '河上的爱情是谁的作品',\n",
       " '贾樟柯有哪些影视作品',\n",
       " '陈巧生的国籍是什么',\n",
       " '巧生炉的国籍是什么',\n",
       " '王柯敏的毕业院校是什么',\n",
       " '王柯敏来自于哪个国家']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14aad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_bios[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241d1d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_intentions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea94f00b",
   "metadata": {},
   "source": [
    "### Split train dev and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be514f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = 0.8\n",
    "dev_fraction = 0.1\n",
    "test_fraction = 0.1\n",
    "num_total = len(questions)\n",
    "num_train = int(train_fraction * num_total)\n",
    "num_dev = int(dev_fraction * num_total)\n",
    "num_test = num_total - num_train - num_dev\n",
    "print(f\"Samples for training: {num_train}, for dev: {num_dev}, for test: {num_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1136f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_questions = questions[:num_train]\n",
    "train_bios = question_bios[:num_train]\n",
    "train_intentions = question_intentions[:num_train]\n",
    "\n",
    "dev_questions = questions[num_train:num_train+num_dev]\n",
    "dev_bios = question_bios[num_train:num_train+num_dev]\n",
    "dev_intentions = question_intentions[num_train:num_train+num_dev]\n",
    "\n",
    "test_questions = questions[num_train+num_dev:]\n",
    "test_bios = question_bios[num_train+num_dev:]\n",
    "test_intentions = question_intentions[num_train+num_dev:]\n",
    "\n",
    "data_dict = {'train': (train_questions, train_bios, train_intentions),\n",
    "            'dev': (dev_questions, dev_bios, dev_intentions),\n",
    "            'test': (test_questions, test_bios, test_intentions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22545a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(train_bios[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0d7b13",
   "metadata": {},
   "source": [
    "Save the data. We name this dataset `naive`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb6b6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p data/naive/train\n",
    "mkdir -p data/naive/dev\n",
    "mkdir -p data/naive/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de210e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/naive/intent_label.txt', 'w') as f:\n",
    "    for intention in all_intentions:\n",
    "        f.write(\"%s\\n\" % intention)\n",
    "with open('data/naive/slot_label.txt', 'w') as f:\n",
    "    for slot_label in all_slot_labels:\n",
    "        f.write(\"%s\\n\" % slot_label)\n",
    "for item in ['train', 'dev', 'test']:\n",
    "    with open(f\"data/naive/{item}/seq.in\", 'w') as f:\n",
    "        for question in data_dict[item][0]:\n",
    "            f.write(\"%s\\n\" % question)\n",
    "    with open(f\"data/naive/{item}/seq.out\", 'w') as f:\n",
    "        for bio in data_dict[item][1]:\n",
    "            f.write(\"%s\\n\" % ' '.join(bio))\n",
    "    with open(f\"data/naive/{item}/label\", 'w') as f:\n",
    "        for intent in data_dict[item][2]:\n",
    "            f.write(\"%s\\n\" % intent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2260c1",
   "metadata": {},
   "source": [
    "## Train JointBERT Model\n",
    "\n",
    "First you need to install all required python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2285620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423359a0",
   "metadata": {},
   "source": [
    "Then run following script for training. Available options for `--task` is defined in `data_loader.py`. `--model_dir` specifies where to store trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340f3999",
   "metadata": {},
   "outputs": [],
   "source": [
    "%% bash\n",
    "\n",
    "python3 main.py --task naive \\\n",
    "                  --model_type bert \\\n",
    "                  --model_dir naive_model \\\n",
    "                  --do_train --do_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed407055",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Run script `predict.py` to evaluated trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f568d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python predict.py --input_file data/naive/test/seq.in --output output/naive_test.out --model_dir naive_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9a6c1b",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a99f628",
   "metadata": {},
   "source": [
    "#### Query with `curl` through https REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ccca934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"detailedMessage\":\"Failed to interpret Gremlin query: Query parsing failed at line 1, character position at 10, error message : token recognition error at: 'per'\",\"code\":\"MalformedQueryException\",\"requestId\":\"62008f80-170b-473d-a354-69d2e586a3b7\"}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   319  100   248  100    71   8857   2535 --:--:-- --:--:-- --:--:-- 11392\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "curl -X POST -d '{\"gremlin\":\"g.V().has('person','name','陈巧生').out('国籍').values('name')\"}' https://database-2-instance-1.c2ycbhkszo5s.us-east-1.neptune.amazonaws.com:8182/gremlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6d7f365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"requestId\":\"989143bc-3d58-4521-9864-7fede108dec1\",\"status\":{\"message\":\"\",\"code\":200,\"attributes\":{\"@type\":\"g:Map\",\"@value\":[]}},\"result\":{\"data\":{\"@type\":\"g:List\",\"@value\":[{\"@type\":\"g:Int64\",\"@value\":188028}]},\"meta\":{\"@type\":\"g:Map\",\"@value\":[]}}}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   278  100   251  100    27   1394    150 --:--:-- --:--:-- --:--:--  1544\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "curl -X POST -d '{\"gremlin\":\"g.E().count()\"}' https://database-2-instance-1.c2ycbhkszo5s.us-east-1.neptune.amazonaws.com:8182/gremlin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83da033",
   "metadata": {},
   "source": [
    "#### Query through Python API\n",
    "Define query templates for every query type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c993f802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(expr):\n",
    "    graph = Graph()\n",
    "    remoteConn = DriverRemoteConnection('wss://database-2.cluster-ro-c2ycbhkszo5s.us-east-1.neptune.amazonaws.com:8182/gremlin','g')\n",
    "    g = graph.traversal().withRemote(remoteConn)\n",
    "    print(eval(expr))\n",
    "    remoteConn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "98f8b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_templates = {\n",
    "    'ask_alumni': \"g.V().has('学校', 'name', '{}').inE().hasLabel('毕业院校').outV().values('name').toList()\",\n",
    "    'ask_school': \"g.V().has('人物','name','{}').out('毕业院校').values('name').next()\",\n",
    "    'ask_books': \"g.V().has('人物', 'name', '{}').inE().hasLabel('作者').outV().values('name').toList()\", \n",
    "    'ask_author': \"g.V().has('图书作品','name','{}').out('作者').values('name').next()\",\n",
    "    'ask_wife': \"g.V().has('人物','name','{}').out('妻子').values('name').next()\",\n",
    "    'ask_husband': \"g.V().has('人物','name','{}').out('丈夫').values('name').next()\",\n",
    "    'ask_films': \"g.V().has('人物', 'name', '{}').inE().hasLabel('导演').outV().values('name').toList()\",\n",
    "    'ask_director': \"g.V().has('影视作品','name','{}').out('导演').values('name').next()\",\n",
    "    'ask_nationality': \"g.V().has('人物','name','{}').out('国籍').values('name').next()\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e5ff2a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_templates = {\n",
    "    'ask_alumni': \"{}的校友有{}\",\n",
    "    'ask_school': \"{}毕业于{}\",\n",
    "    'ask_books': \"{}的著作有{}\", \n",
    "    'ask_author': \"{}的作者是{}\",\n",
    "    'ask_wife': \"{}的妻子是{}\",\n",
    "    'ask_husband': \"{}的丈夫是{}\",\n",
    "    'ask_films': \"{}导演的作品有{}\",\n",
    "    'ask_director': \"{}的导演是{}\",\n",
    "    'ask_nationality': \"{}的国籍是{}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "902f93c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v[node_机构_嘉兴中润光学科技有限公司], v[node_机构_厦门博乐德平台拍卖有限公司], v[node_机构_北京泡泡玛特文化创意有限公司], v[node_机构_大卫博士有限公司], v[node_机构_山东金天牛矿山机械有限公司]]\n"
     ]
    }
   ],
   "source": [
    "query(\"g.V().hasLabel('机构').limit(5).toList()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3962109d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "尼古拉斯·凯奇\n"
     ]
    }
   ],
   "source": [
    "query(\"g.V().has('影视作品', 'name', '末日迷踪').out('主演').values('name').next()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "04d04b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "捷克\n"
     ]
    }
   ],
   "source": [
    "query(query_templates['ask_nationality'].format('科库雷克'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3cb22e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ask_author'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "intent_pattern = re.compile(\"<(.*)> -> .*\")\n",
    "intent_pattern.search(\"<ask_author> -> [摩:B_book] [登:I_book] [时:I_book] [代:I_book] 是 谁 写 的\").group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6f39b62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['摩:B_book', '登:I_book', '时:I_book', '代:I_book']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slot_pattern = re.compile(r'\\[(.*?)\\]')\n",
    "slot_pattern.findall(\"<ask_author> -> [摩:B_book] [登:I_book] [时:I_book] [代:I_book] 是 谁 写 的\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ea8651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
