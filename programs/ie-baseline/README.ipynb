{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a97ab27",
   "metadata": {},
   "source": [
    "# Knowledge Extraction and Graph Generation\n",
    "\n",
    "This repository details how to extract relations from unstructured texts, and how to bulkload extracted relations into Amazon Neptune.\n",
    "\n",
    "Run the Jupyter notebook version of this file: [README.ipynb](./README.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48daa453",
   "metadata": {},
   "source": [
    "## Knowledge Extraction\n",
    "\n",
    "Knowledge extraction programs is in `programs/ie-baseline/`. If you are using SageMaker notebook, it is advised to use a pytorch kernel like `pytorch_latest_p36` or `pytorch_p36`.\n",
    "Note: The model used in this repo requires torch >= 1.9.0\n",
    "\n",
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb0db59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# just make sure you are in programs/ie-baseline\n",
    "# cd programs/ie-baseline\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551c9b4f",
   "metadata": {},
   "source": [
    "### Download and process training data\n",
    "Skip this step if you have already downloaded it. Unzipped data is placed at folder `data`, this is hard-coded now. In a future version it would become an argument of training script. Transformed data is placed at folder `generated`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76b6369",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# download DuIE dataset\n",
    "wget https://dataset-bj.cdn.bcebos.com/qianyan/DuIE_2_0.zip\n",
    "unzip -j DuIE_2_0.zip -d data\n",
    "# transform data and place it in generated\n",
    "mkdir generated\n",
    "python trans.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0219702",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Check `main.py` or [main.ipynb](main.ipynb) for more detail. It takes around 8 mintues for an epoch on a p3.2xl machine (evaluation is currently sequential and can't be parallized, so it takes even more time than training). You can specify batch size with `--batch_size`, specify tensorboard log subfolder name with `--logname`. If you want to load a previously trained, use flag `--loadweight weight_name`. `weight_name` is the part after `subject_` and `object_`, i.e. the `weight_name` for `subject_att1_195` and `object_att1_195` is `att1_195`.\n",
    "\n",
    "Warning: it may stop training once this notebook is terminated (since the traing process is killed as a subprocess of this terminal). You can run it in terminal with deamon protection to keep it running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0488e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --logname att1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd7c4f0",
   "metadata": {},
   "source": [
    "Running statistics are logged with tensorboard, and saved in folder `logs`. You can lauch tensor board to track training status. (You may need to run this in a separate cli window.) Visit `https://[notebook_addr].sagemaker.aws/proxy/6006/` to access tensorboard. The slash at end is **necessary**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518cf97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a6f8cd",
   "metadata": {},
   "source": [
    "### Load the model for evaluation / inference\n",
    "Models are saved at `save` folder. Subject models are saved as `subject_[logname]_[epoch]`, object prediction models are saved as `object_[logname]_[epoch]`, where `[logname]` is the logname you specified in parameters, `[epoch]` is the epoch num when it was saved.\n",
    "\n",
    "I uploaded one of my trained model weights to Google drive, it can be accessed at [weight_att1_195.zip](https://drive.google.com/file/d/1YTFvOXCSJaUlj745XZ-LNQsv0xuVq7wv/view?usp=sharing). You can download it and extract the weights to `save/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b58da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import config\n",
    "from model_origin import SubjectModel, ObjectModel\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#specify the model to load with epoch x\n",
    "# breakpoint_epoch = 195 # 210 is saved in repo\n",
    "model_dir = 'save'\n",
    "weight_name = 'att3_295'\n",
    "subject_model = SubjectModel(config.bert_dict_len, config.word_emb_size).to(device)\n",
    "object_model = ObjectModel(config.word_emb_size, config.num_classes).to(device)\n",
    "subject_model.load_state_dict(torch.load(f\"./{model_dir}/subject_{weight_name}\", map_location=device))\n",
    "object_model.load_state_dict(torch.load(f\"./{model_dir}/object_{weight_name}\", map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174736a8",
   "metadata": {},
   "source": [
    "### Load data for evaluation\n",
    "\n",
    "Data are loaded into json objects, related dictionaries are also loaded for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedd964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dev_path = 'generated/dev_data_me.json'\n",
    "train_path = 'generated/train_data_me.json'\n",
    "dev_data = json.load(open(dev_path))\n",
    "train_data = json.load(open(train_path))\n",
    "generated_char_path = 'generated/all_chars_me.json'\n",
    "id2char, char2id = json.load(open(generated_char_path))\n",
    "generated_schema_path =  'generated/schemas_me.json'\n",
    "id2predicate, predicate2id = json.load(open(generated_schema_path))\n",
    "id2predicate = {int(i): j for i, j in id2predicate.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b577eec",
   "metadata": {},
   "source": [
    "### Evaluation and Inference\n",
    "Extract relations text by text with `extract_items` function. Here we write extracted relations to `pandas` frame first, then write to a csv file.\n",
    "\n",
    "Previously loaded `subject_model` and `object_model` will be utilized here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e15a372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import extract_spoes\n",
    "from data_gen import MyDevDataset, dev_collate_fn\n",
    "\n",
    "dev_dataset = MyDevDataset(dev_data, config.bert_model_name)\n",
    "dev_loader = DataLoader(\n",
    "    dataset=dev_dataset,  \n",
    "    batch_size=256, \n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=dev_collate_fn,\n",
    "    multiprocessing_context='spawn',\n",
    ")\n",
    "train_dataset = MyDevDataset(train_data, config.bert_model_name)\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,  \n",
    "    batch_size=256, \n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=dev_collate_fn,\n",
    "    multiprocessing_context='spawn',\n",
    ")\n",
    "rel_df = pd.DataFrame({'subject':[], 'predicate':[], 'object':[]})\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dev_loader, desc=\"Extracting relations from dev\"):\n",
    "        texts, tokens, spoes, att_masks, offset_mappings = batch\n",
    "        items = extract_spoes(texts, tokens, offset_mappings, subject_model, object_model, id2predicate, attention_mask=att_masks)\n",
    "        for item in items:\n",
    "            rel_df.loc[len(rel_df)] = item\n",
    "    num_rel_dev = len(rel_df)\n",
    "    print(\"num of extracted relations from dev set is:\", num_rel_dev)\n",
    "    for batch in tqdm(train_loader, desc=\"Extracting relations from train\"):\n",
    "        texts, tokens, spoes, att_masks, offset_mappings = batch\n",
    "        items = extract_spoes(texts, tokens, offset_mappings, subject_model, object_model, id2predicate, attention_mask=att_masks)\n",
    "        for item in items:\n",
    "            rel_df.loc[len(rel_df)] = item\n",
    "    num_rel_train = len(rel_df) - num_rel_dev\n",
    "    print(\"num of extracted relations from dev set is:\", num_rel_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca80ea94",
   "metadata": {},
   "source": [
    "Save extracted relations to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4a2fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_df.to_csv('generated/triplets_att3.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d1bfe4",
   "metadata": {},
   "source": [
    "Count and compare with gold triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1447a87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spo = []\n",
    "dev_spo = []\n",
    "for item in train_data:\n",
    "    train_spo += item['spo_list']\n",
    "for item in dev_data:\n",
    "    dev_spo += item['spo_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a0c742",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_spo = train_spo + dev_spo\n",
    "gold_spo = [tuple(spo) for spo in gold_spo]\n",
    "len(gold_spo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3f4d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "rel_df = pd.read_csv('generated/triplets.csv', names=['subject', 'predicate', 'object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7067e519",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_spo = []\n",
    "for idx, row in rel_df.iterrows():\n",
    "    extracted_spo.append((row['subject'], row['predicate'], row['object']))\n",
    "len(extracted_spo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e9ca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_spo_set = set(gold_spo)\n",
    "extracted_spo_set = set(extracted_spo)\n",
    "overlap = len(gold_spo_set & extracted_spo_set)\n",
    "recall = overlap / len(gold_spo_set)\n",
    "precision = overlap / len(extracted_spo_set)\n",
    "f1 = overlap * 2 / (len(gold_spo_set) + len(extracted_spo_set))\n",
    "print(f\"#extracted_pos: {len(extracted_spo)}, #gold_spo: {len(gold_spo)}\")\n",
    "print(f\"#extracted_pos_set: {len(extracted_spo_set)}, #gold_spo_set: {len(gold_spo_set)}\")\n",
    "print(f\"f1: {f1}, recall: {recall}, precision: {precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7989b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_spo_set = set(gold_spo)\n",
    "extracted_spo_set = set(extracted_spo)\n",
    "overlap = len(gold_spo_set & extracted_spo_set)\n",
    "recall = overlap / len(gold_spo_set)\n",
    "precision = overlap / len(extracted_spo_set)\n",
    "f1 = overlap * 2 / (len(gold_spo_set) + len(extracted_spo_set))\n",
    "print(f\"#extracted_pos: {len(extracted_spo)}, #gold_spo: {len(gold_spo)}\")\n",
    "print(f\"#extracted_pos_set: {len(extracted_spo_set)}, #gold_spo_set: {len(gold_spo_set)}\")\n",
    "print(f\"f1: {f1}, recall: {recall}, precision: {precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca7e42d",
   "metadata": {},
   "source": [
    "### Tranform relation triplets to nodes and edges\n",
    "Create relation dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55be9bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_dict = {}\n",
    "schema_path = 'data/schema.json'\n",
    "with open(schema_path) as f:\n",
    "    for l in tqdm(f):\n",
    "        rel = json.loads(l)\n",
    "        #schemas.add(a['predicate'])\n",
    "        predicate = rel['predicate']\n",
    "        sub_type = rel['subject_type']\n",
    "        obj_type = rel['object_type']['@value']\n",
    "        rel_dict[predicate] = {'subject_type': sub_type, 'object_type': obj_type}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb30388",
   "metadata": {},
   "source": [
    "In order to transform entities and edges to a gremlin-compatible format, we need to assign ID to each of them. ID is currently constructed in a very simple way:\n",
    "```python\n",
    "node_id = 'node_' + node_type + '_' + node_name\n",
    "edge_id = 'edge_' + predicate + '_' + from + '_' + to\n",
    "```\n",
    "\n",
    "Again, we use a dataframe to store transformed edges and nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c6e099",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df = pd.DataFrame({'~id':[], '~label':[], 'name': []})\n",
    "edge_df = pd.DataFrame({'~id':[], '~from':[], '~to':[], '~label':[]})\n",
    "\n",
    "node_dict = {}\n",
    "\n",
    "# currently id is constructed naively.\n",
    "def node_name2id(entity_type, entity_name):\n",
    "    return 'node_' + entity_type + '_' + entity_name\n",
    "\n",
    "for idx, row in rel_df.iterrows():\n",
    "    sub = row['subject']\n",
    "    obj = row['object']\n",
    "    rel = row['predicate']\n",
    "    sub_type = rel_dict[rel]['subject_type']\n",
    "    obj_type = rel_dict[rel]['object_type']\n",
    "    sub_id = 'node_' + sub_type + '_' + sub\n",
    "    obj_id = 'node_' + obj_type + '_' + obj\n",
    "    # order matter: ~id, ~label, name\n",
    "    node_dict[sub_id] = [sub_type, sub]\n",
    "    node_dict[obj_id] = [obj_type, obj]\n",
    "    edge_id = 'edge_' + rel + '_' + sub_id + '_' + obj_id\n",
    "    edge_df.loc[len(edge_df)] = [edge_id, sub_id, obj_id, rel]\n",
    "    \n",
    "for key, val in node_dict.items():\n",
    "    node_df.loc[len(node_df)] = [key, val[0], val[1]]  \n",
    "\n",
    "print(\"We have scanned {} nodes and {} relations\".format(len(node_df), len(edge_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36ee40c",
   "metadata": {},
   "source": [
    "Save nodes and relations to csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811b6445",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df.to_csv('generated/nodes.csv', index=False)\n",
    "edge_df.to_csv('generated/edges.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab30a510",
   "metadata": {},
   "source": [
    "Upload nodes and edges files to S3 for bulkloading into Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880eafd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# You need to relace this with your own S3 buckets and paths\n",
    "export S3_SAVE_BUCKET=\"sm-nlp-data\"\n",
    "export SAVE_PATH=\"ie-baseline/outputs\"\n",
    "aws s3 cp ./generated/edges.csv s3://$S3_SAVE_BUCKET/$SAVE_PATH/edges.csv\n",
    "aws s3 cp ./generated/nodes.csv s3://$S3_SAVE_BUCKET/$SAVE_PATH/nodes.csv\n",
    "\n",
    "echo \"The path for the Property Graph bulk loading step is 's3://$S3_SAVE_BUCKET/$SAVE_PATH/'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d080c9",
   "metadata": {},
   "source": [
    "## Load Graph Data into Neptune\n",
    "\n",
    "You need to find your Netune endpoint and port in the Neptune database instance detail page. Here I paste mine.\n",
    "\n",
    "- Neptune endpoint & port: database-1-instance-1.c2ycbhkszo5s.us-east-1.neptune.amazonaws.com:8182 [info](https://console.aws.amazon.com/neptune/home?region=us-east-1#database:id=database-1-instance-1;is-cluster=false;tab=connectivity)\n",
    "- Source:\n",
    "    - s3://sm-nlp-data/ie-baseline/outputs/nodes.csv\n",
    "    - s3://sm-nlp-data/ie-baseline/outputs/edges.csv\n",
    "- IAM role ARN: arn:aws:iam::093729152554:role/service-role/AWSNeptuneNotebookRole-NepTestRole [link](https://console.aws.amazon.com/iam/home?region=us-east-1#/roles/AWSNeptuneNotebookRole-NepTestRole)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f859811e",
   "metadata": {},
   "source": [
    "*Trouble shooting*:\n",
    "\n",
    "- You have to create an endpoint following the section 'Creating an Amazon S3 VPC Endpoint' in this [post](https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-data.html).\n",
    "- Choose the endpoint type as 'Gateway'.\n",
    "- Do select the check box next to the route tables that are associated "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99a7e4c",
   "metadata": {},
   "source": [
    "Bulkload nodes and edges into Neptune using `loader` provided by Neptune with `curl` command. You need to specify neptune database and port, namely this part `https://database-2-instance-1.c2ycbhkszo5s.us-east-1.neptune.amazonaws.com:8182/`, as well as `source`, `iamRoleArn` and `region`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae3e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "curl -X POST \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    https://database-2-instance-1.c2ycbhkszo5s.us-east-1.neptune.amazonaws.com:8182/loader -d '\n",
    "    {\n",
    "      \"source\" : \"s3://sm-nlp-data/ie-baseline/outputs/\",\n",
    "      \"format\" : \"csv\",\n",
    "      \"iamRoleArn\" : \"arn:aws:iam::093729152554:role/NeptuneLoadFromS3\",\n",
    "      \"region\" : \"us-east-1\",\n",
    "      \"failOnError\" : \"FALSE\",\n",
    "      \"parallelism\" : \"MEDIUM\",\n",
    "      \"updateSingleCardinalityProperties\" : \"FALSE\",\n",
    "      \"queueRequest\" : \"TRUE\",\n",
    "      \"dependencies\" : []\n",
    "    }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ae3a08",
   "metadata": {},
   "source": [
    "Now, you can query this database within the same VPC using `curl` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b2dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "curl -X POST -d '{\"gremlin\":\"g.V().limit(5)\"}' https://database-2-instance-1.c2ycbhkszo5s.us-east-1.neptune.amazonaws.com:8182/gremlin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac66170",
   "metadata": {},
   "source": [
    "## Access Neptune from Outside the VPC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045289af",
   "metadata": {},
   "source": [
    "We set up a load balancer to redirect traffics from outside the VPC to the neptune endpoints.\n",
    "\n",
    "Architectures and best practices of connecting to Neptune with load balancers are detailed in this post: [Connecting to Amazon Neptune from Clients Outside the Neptune VPC](https://github.com/aws-samples/aws-dbs-refarch-graph/tree/master/src/connecting-using-a-load-balancer).\n",
    "\n",
    "This [answer](https://stackoverflow.com/a/52622164) from stackoverflow may also help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66c0316",
   "metadata": {},
   "source": [
    "### Steps\n",
    "\n",
    "1. Find out Neptune cluster's master IP address. `dig +short <your cluster endpoint>` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5610a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dig +short database-2-instance-1.c2ycbhkszo5s.us-east-1.neptune.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349d2813",
   "metadata": {},
   "source": [
    "2. Create an Application Load Balancer (ALB)\n",
    "    \n",
    "    - In EC2's left panel, click 'Load Balancer'. \n",
    "    - In availablity zone, make sure you select at least the zone where your Neptune DB instance is located. \n",
    "    - In Configure Security Groups, create a security group that allows inbound traffic from everywhere. i.e. Inbound TCP rule for 0.0.0.0 on 80.\n",
    "    - In Configure routing, choose target type as IP, protocal as HTTP, port as 80\n",
    "    - In register targets, add the IP Address obtained for step #1, and the port as 8182, then click \"add to list\".\n",
    "\n",
    "3. Access!\n",
    "\n",
    "After configuring an ALB(application load balancer), you can find the DNS name of the it in load balancers, the accessing port is as you set in \"configure routing\", which is 80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa2cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "curl -X POST -d '{\"gremlin\":\"g.V().limit(5)\"}' alb-neptune-test-62758122.us-east-1.elb.amazonaws.com/gremlin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
