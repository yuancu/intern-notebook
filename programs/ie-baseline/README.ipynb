{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69dadbe8",
   "metadata": {},
   "source": [
    "# Knowledge Extraction and Graph Generation\n",
    "\n",
    "This repository details how to extract relations from unstructured texts, and how to bulkload extracted relations into Amazon Neptune.\n",
    "\n",
    "Run the Jupyter notebook version of this file: [README.ipynb](./README.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7755ff",
   "metadata": {},
   "source": [
    "## Knowledge Extraction\n",
    "\n",
    "Knowledge extraction programs is in `programs/ie-baseline/`. If you are using SageMaker notebook, it is advised to use a pytorch kernel like `pytorch_latest_p36` or `pytorch_p36`.\n",
    "\n",
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070b0c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# just make sure you are in programs/ie-baseline\n",
    "# cd programs/ie-baseline\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df9baf7",
   "metadata": {},
   "source": [
    "### Download and process training data\n",
    "Skip this step if you have already downloaded it. Unzipped data is placed at folder `data`, this is hard-coded now. In a future version it would become an argument of training script. Transformed data is placed at folder `generated`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfcf377",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# download DuIE dataset\n",
    "wget https://dataset-bj.cdn.bcebos.com/qianyan/DuIE_2_0.zip\n",
    "unzip -j DuIE_2_0.zip -d data\n",
    "# transform data and place it in generated\n",
    "mkdir generated\n",
    "python trans.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c325b5b",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Check `main.py` or [main.ipynb](main.ipynb) for more detail. It takes around 8 mintues for an epoch on a p3.2xl machine (evaluation is currently sequential and can't be parallized, so it takes even more time than training).\n",
    "\n",
    "Warning: it may stop training once this notebook is terminated (since the traing process is killed as a subprocess of this terminal). You can run it in terminal with deamon protection to keep it running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe66d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253ab1a9",
   "metadata": {},
   "source": [
    "Running statistics are logged with tensorboard, and saved in folder `logs`. You can lauch tensor board to track training status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18842798",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bb7fa6",
   "metadata": {},
   "source": [
    "### Load the model for evaluation / inference\n",
    "Models are saved at `models_real` folder. Subject models are saved as `s_x.pkl`, object prediction models are saved as `po_x.pkl`, where `x` is the epoch num when it was saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43d14fa4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'programs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4b163fb253e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mprograms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mie_baseline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'programs'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils import load_model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#specify the model to load with epoch x\n",
    "breakpoint_epoch = 210 # 210 is saved in repo\n",
    "model_dir = 'models_real'\n",
    "subject_model, object_model = load_model(model_dir, breakpoint_epoch, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e2807",
   "metadata": {},
   "source": [
    "Models are packed in `DataParallel` class, so here we extracte the plain models from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d55885",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_model = subject_model.module\n",
    "object_model = object_model.module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bd8d05",
   "metadata": {},
   "source": [
    "### Load data for evaluation\n",
    "\n",
    "Data are loaded into json objects, related dictionaries are also loaded for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1681a69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dev_path = 'generated/dev_data_me.json'\n",
    "train_path = 'generated/train_data_me'\n",
    "dev_data = json.load(open(dev_path))\n",
    "generated_char_path = 'generated/all_chars_me.json'\n",
    "id2char, char2id = json.load(open(generated_char_path))\n",
    "generated_schema_path =  'generated/schemas_me.json'\n",
    "id2predicate, predicate2id = json.load(open(generated_schema_path))\n",
    "id2predicate = {int(i): j for i, j in id2predicate.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889243e6",
   "metadata": {},
   "source": [
    "### Evaluation and Inference\n",
    "Extract relations text by text with `extract_items` function. Here we write extracted relations to `pandas` frame first, then write to a csv file.\n",
    "\n",
    "Previously loaded `subject_model` and `object_model` will be utilized here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7418e522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from utils import extract_items\n",
    "\n",
    "rel_df = pd.DataFrame({'subject':[], 'predicate':[], 'object':[]})\n",
    "for d in tqdm(iter(dev_data), desc=\"Extracting relations\"):\n",
    "    items = extract_items(d['text'], subject_model, object_model, char2id, id2predicate)\n",
    "    for item in items:\n",
    "        rel_df.loc[len(df)] = item\n",
    "\n",
    "print(\"num of extracted relations from dev set is:\", len(rel_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9483c2",
   "metadata": {},
   "source": [
    "Save extracted relations to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983da887",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_df.to_csv('generated/triplets.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb45bf4",
   "metadata": {},
   "source": [
    "### Tranform relation triplets to nodes and edges\n",
    "Create relation dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbb928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_dict = {}\n",
    "schema_path = 'data/schema.json'\n",
    "with open(schema_path) as f:\n",
    "    for l in tqdm(f):\n",
    "        rel = json.loads(l)\n",
    "        #schemas.add(a['predicate'])\n",
    "        predicate = rel['predicate']\n",
    "        sub_type = rel['subject_type']\n",
    "        obj_type = rel['object_type']['@value']\n",
    "        rel_dict[predicate] = {'subject_type': sub_type, 'object_type': obj_type}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b4bb18",
   "metadata": {},
   "source": [
    "In order to transform entities and edges to a gremlin-compatible format, we need to assign ID to each of them. ID is currently constructed in a very simple way:\n",
    "```python\n",
    "node_id = 'node_' + node_type + '_' + node_name\n",
    "edge_id = 'edge_' + predicate + '_' + from + '_' + to\n",
    "```\n",
    "\n",
    "Again, we use a dataframe to store transformed edges and nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d476019",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df = pd.DataFrame({'~id':[], '~label':[], 'name': []})\n",
    "edge_df = pd.DataFrame({'~id':[], '~from':[], '~to':[], '~label':[]})\n",
    "\n",
    "node_dict = {}\n",
    "\n",
    "# currently id is constructed naively.\n",
    "def node_name2id(entity_type, entity_name):\n",
    "    return 'node_' + entity_type + '_' + entity_name\n",
    "\n",
    "for idx, row in rel_df.iterrows():\n",
    "    sub = row['subject']\n",
    "    obj = row['object']\n",
    "    rel = row['predicate']\n",
    "    sub_type = rel_dict[rel]['subject_type']\n",
    "    obj_type = rel_dict[rel]['object_type']\n",
    "    sub_id = 'node_' + sub_type + '_' + sub\n",
    "    obj_id = 'node_' + obj_type + '_' + obj\n",
    "    # order matter: ~id, ~label, name\n",
    "    node_dict[sub_id] = [sub_type, sub]\n",
    "    node_dict[obj_id] = [obj_type, obj]\n",
    "    edge_id = 'edge_' + rel + '_' + sub_id + '_' + obj_id\n",
    "    edge_df.loc[len(edge_df)] = [edge_id, sub_id, obj_id, rel]\n",
    "    \n",
    "for key, val in node_dict.items():\n",
    "    node_df.loc[len(node_df)] = [key, val[0], val[1]]  \n",
    "\n",
    "print(\"We have scanned {} nodes and {} relations\".format(len(node_df), len(edge_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532c2120",
   "metadata": {},
   "source": [
    "Save nodes and relations to csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc9453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df.to_csv('generated/nodes.csv', index=False)\n",
    "edge_df.to_csv('generated/edges.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0a8292",
   "metadata": {},
   "source": [
    "Upload nodes and edges files to S3 for bulkloading into Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd1c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# You need to relace this with your own S3 buckets and paths\n",
    "export S3_SAVE_BUCKET=\"sm-nlp-data\"\n",
    "export SAVE_PATH=\"ie-baseline/outputs\"\n",
    "aws s3 cp ./generated/edges.csv s3://$S3_SAVE_BUCKET/$SAVE_PATH/edges.csv\n",
    "aws s3 cp ./generated/nodes.csv s3://$S3_SAVE_BUCKET/$SAVE_PATH/nodes.csv\n",
    "\n",
    "echo \"The path for the Property Graph bulk loading step is 's3://$S3_SAVE_BUCKET/$SAVE_PATH/'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0561861",
   "metadata": {},
   "source": [
    "## Load Graph Data into Neptune\n",
    "\n",
    "You need to find your Netune endpoint and port in the Neptune database instance detail page. Here I paste mine.\n",
    "\n",
    "- Neptune endpoint & port: database-1-instance-1.c2ycbhkszo5s.us-east-1.neptune.amazonaws.com:8182 [info](https://console.aws.amazon.com/neptune/home?region=us-east-1#database:id=database-1-instance-1;is-cluster=false;tab=connectivity)\n",
    "- Source:\n",
    "    - s3://sm-nlp-data/ie-baseline/outputs/nodes.csv\n",
    "    - s3://sm-nlp-data/ie-baseline/outputs/edges.csv\n",
    "- IAM role ARN: arn:aws:iam::093729152554:role/service-role/AWSNeptuneNotebookRole-NepTestRole [link](https://console.aws.amazon.com/iam/home?region=us-east-1#/roles/AWSNeptuneNotebookRole-NepTestRole)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850c6955",
   "metadata": {},
   "source": [
    "*Trouble shooting*:\n",
    "\n",
    "- You have to create an endpoint following the section 'Creating an Amazon S3 VPC Endpoint' in this [post](https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-data.html).\n",
    "- Choose the endpoint type as 'Gateway'.\n",
    "- Do select the check box next to the route tables that are associated "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed3f48",
   "metadata": {},
   "source": [
    "Bulkload nodes and edges into Neptune using `loader` provided by Neptune with `curl` command. You need to specify neptune database and port, namely this part `https://database-2-instance-1.c2ycbhkszo5s.us-east-1.neptune.amazonaws.com:8182/`, as well as `source`, `iamRoleArn` and `region`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33970fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "curl -X POST \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    https://database-2-instance-1.c2ycbhkszo5s.us-east-1.neptune.amazonaws.com:8182/loader -d '\n",
    "    {\n",
    "      \"source\" : \"s3://sm-nlp-data/ie-baseline/outputs/\",\n",
    "      \"format\" : \"csv\",\n",
    "      \"iamRoleArn\" : \"arn:aws:iam::093729152554:role/NeptuneLoadFromS3\",\n",
    "      \"region\" : \"us-east-1\",\n",
    "      \"failOnError\" : \"FALSE\",\n",
    "      \"parallelism\" : \"MEDIUM\",\n",
    "      \"updateSingleCardinalityProperties\" : \"FALSE\",\n",
    "      \"queueRequest\" : \"TRUE\",\n",
    "      \"dependencies\" : []\n",
    "    }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8939374e",
   "metadata": {},
   "source": [
    "Now, you can query this database within the same VPC using `curl` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100e430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "curl -X POST -d '{\"gremlin\":\"g.V().limit(5)\"}' https://database-2-instance-1.c2ycbhkszo5s.us-east-1.neptune.amazonaws.com:8182/gremlin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
