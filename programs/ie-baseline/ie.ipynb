{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04f2c667",
   "metadata": {},
   "source": [
    "# Knowledge Extraction and Graph Generation\n",
    "\n",
    "This repository details how to extract relations from unstructured texts, and how to bulkload extracted relations into Amazon Neptune.\n",
    "\n",
    "Run the Jupyter notebook version of this file: [README.ipynb](./README.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1dbf77",
   "metadata": {},
   "source": [
    "## Knowledge Extraction\n",
    "\n",
    "Knowledge extraction programs is in `programs/ie-baseline/`. If you are using SageMaker notebook, it is advised to use a pytorch kernel like `pytorch_latest_p36` or `pytorch_p36`.\n",
    "Note: The model used in this repo requires torch >= 1.9.0\n",
    "\n",
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa5c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# just make sure you are in programs/ie-baseline\n",
    "# cd programs/ie-baseline\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107c0192",
   "metadata": {},
   "source": [
    "### Download and process training data\n",
    "Skip this step if you have already downloaded it. Unzipped data is placed at folder `data`, this is hard-coded now. In a future version it would become an argument of training script. Transformed data is placed at folder `generated`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76472e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# download DuIE dataset\n",
    "wget https://dataset-bj.cdn.bcebos.com/qianyan/DuIE_2_0.zip\n",
    "unzip -j DuIE_2_0.zip -d data\n",
    "# transform data and place it in generated\n",
    "mkdir generated\n",
    "python trans.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f7a746",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Check `main.py` or [main.ipynb](main.ipynb) for more detail. It takes around 8 mintues for an epoch on a p3.2xl machine (evaluation is currently sequential and can't be parallized, so it takes even more time than training). You can specify batch size with `--batch_size`, specify tensorboard log subfolder name with `--logname`. If you want to load a previously trained, use flag `--loadweight weight_name`. `weight_name` is the part after `subject_` and `object_`, i.e. the `weight_name` for `subject_att1_195` and `object_att1_195` is `att1_195`.\n",
    "\n",
    "Warning: it may stop training once this notebook is terminated (since the traing process is killed as a subprocess of this terminal). You can run it in terminal with deamon protection to keep it running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c81cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --logname att1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26af6663",
   "metadata": {},
   "source": [
    "Running statistics are logged with tensorboard, and saved in folder `logs`. You can lauch tensor board to track training status. (You may need to run this in a separate cli window.) Visit `https://[notebook_addr].sagemaker.aws/proxy/6006/` to access tensorboard. The slash at end is **necessary**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c8e148",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a1eb3f",
   "metadata": {},
   "source": [
    "### Load the model for evaluation / inference\n",
    "Models are saved at `save` folder. Subject models are saved as `subject_[logname]_[epoch]`, object prediction models are saved as `object_[logname]_[epoch]`, where `[logname]` is the logname you specified in parameters, `[epoch]` is the epoch num when it was saved.\n",
    "\n",
    "I uploaded one of my trained model weights to Google drive, it can be accessed at [weight_att1_195.zip](https://drive.google.com/file/d/1YTFvOXCSJaUlj745XZ-LNQsv0xuVq7wv/view?usp=sharing). You can download it and extract the weights to `save/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f836d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import config\n",
    "from model_origin import SubjectModel, ObjectModel\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#specify the model to load with epoch x\n",
    "# breakpoint_epoch = 195 # 210 is saved in repo\n",
    "model_dir = 'save'\n",
    "weight_name = 'att3_295'\n",
    "subject_model = SubjectModel(config.bert_dict_len, config.word_emb_size).to(device)\n",
    "object_model = ObjectModel(config.word_emb_size, config.num_classes).to(device)\n",
    "subject_model.load_state_dict(torch.load(f\"./{model_dir}/subject_{weight_name}\", map_location=device))\n",
    "object_model.load_state_dict(torch.load(f\"./{model_dir}/object_{weight_name}\", map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb8a3a7",
   "metadata": {},
   "source": [
    "### Load data for evaluation\n",
    "\n",
    "Data are loaded into json objects, related dictionaries are also loaded for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dd0d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dev_path = 'generated/dev_data_me.json'\n",
    "train_path = 'generated/train_data_me.json'\n",
    "dev_data = json.load(open(dev_path))\n",
    "train_data = json.load(open(train_path))\n",
    "generated_char_path = 'generated/all_chars_me.json'\n",
    "id2char, char2id = json.load(open(generated_char_path))\n",
    "generated_schema_path =  'generated/schemas_me.json'\n",
    "id2predicate, predicate2id = json.load(open(generated_schema_path))\n",
    "id2predicate = {int(i): j for i, j in id2predicate.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382d3f5c",
   "metadata": {},
   "source": [
    "### Evaluation and Inference\n",
    "Extract relations text by text with `extract_items` function. Here we write extracted relations to `pandas` frame first, then write to a csv file.\n",
    "\n",
    "Previously loaded `subject_model` and `object_model` will be utilized here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3976e3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import extract_spoes\n",
    "from data_gen import MyDevDataset, dev_collate_fn\n",
    "\n",
    "dev_dataset = MyDevDataset(dev_data, config.bert_model_name)\n",
    "dev_loader = DataLoader(\n",
    "    dataset=dev_dataset,  \n",
    "    batch_size=256, \n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=dev_collate_fn,\n",
    "    multiprocessing_context='spawn',\n",
    ")\n",
    "train_dataset = MyDevDataset(train_data, config.bert_model_name)\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,  \n",
    "    batch_size=256, \n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=dev_collate_fn,\n",
    "    multiprocessing_context='spawn',\n",
    ")\n",
    "rel_df = pd.DataFrame({'subject':[], 'predicate':[], 'object':[]})\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dev_loader, desc=\"Extracting relations from dev\"):\n",
    "        texts, tokens, spoes, att_masks, offset_mappings = batch\n",
    "        items = extract_spoes(texts, tokens, offset_mappings, subject_model, object_model, id2predicate, attention_mask=att_masks)\n",
    "        for item in items:\n",
    "            rel_df.loc[len(rel_df)] = item\n",
    "    num_rel_dev = len(rel_df)\n",
    "    print(\"num of extracted relations from dev set is:\", num_rel_dev)\n",
    "    for batch in tqdm(train_loader, desc=\"Extracting relations from train\"):\n",
    "        texts, tokens, spoes, att_masks, offset_mappings = batch\n",
    "        items = extract_spoes(texts, tokens, offset_mappings, subject_model, object_model, id2predicate, attention_mask=att_masks)\n",
    "        for item in items:\n",
    "            rel_df.loc[len(rel_df)] = item\n",
    "    num_rel_train = len(rel_df) - num_rel_dev\n",
    "    print(\"num of extracted relations from dev set is:\", num_rel_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09e8f55",
   "metadata": {},
   "source": [
    "Save extracted relations to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd30a689",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_df.to_csv('generated/triplets_att3.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0359764a",
   "metadata": {},
   "source": [
    "Count and compare with gold triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "825f4e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spo = []\n",
    "dev_spo = []\n",
    "for item in train_data:\n",
    "    train_spo += item['spo_list']\n",
    "for item in dev_data:\n",
    "    dev_spo += item['spo_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8999504f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "348534"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_spo = train_spo + dev_spo\n",
    "gold_spo = [tuple(spo) for spo in gold_spo]\n",
    "len(gold_spo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "952940e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "rel_df = pd.read_csv('generated/triplets.csv', names=['subject', 'predicate', 'object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cbc0f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228796"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_spo = []\n",
    "for idx, row in rel_df.iterrows():\n",
    "    extracted_spo.append((row['subject'], row['predicate'], row['object']))\n",
    "len(extracted_spo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bf24980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#extracted_pos: 228796, #gold_spo: 348534\n",
      "#extracted_pos_set: 168526, #gold_spo_set: 225479\n",
      "f1: 0.7273968604459334, recall: 0.6355314685624825, precision: 0.8503079643497146\n"
     ]
    }
   ],
   "source": [
    "gold_spo_set = set(gold_spo)\n",
    "extracted_spo_set = set(extracted_spo)\n",
    "overlap = len(gold_spo_set & extracted_spo_set)\n",
    "recall = overlap / len(gold_spo_set)\n",
    "precision = overlap / len(extracted_spo_set)\n",
    "f1 = overlap * 2 / (len(gold_spo_set) + len(extracted_spo_set))\n",
    "print(f\"#extracted_pos: {len(extracted_spo)}, #gold_spo: {len(gold_spo)}\")\n",
    "print(f\"#extracted_pos_set: {len(extracted_spo_set)}, #gold_spo_set: {len(gold_spo_set)}\")\n",
    "print(f\"f1: {f1}, recall: {recall}, precision: {precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1899299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_spo_set = set(gold_spo)\n",
    "extracted_spo_set = set(extracted_spo)\n",
    "overlap = len(gold_spo_set & extracted_spo_set)\n",
    "recall = overlap / len(gold_spo_set)\n",
    "precision = overlap / len(extracted_spo_set)\n",
    "f1 = overlap * 2 / (len(gold_spo_set) + len(extracted_spo_set))\n",
    "print(f\"#extracted_pos: {len(extracted_spo)}, #gold_spo: {len(gold_spo)}\")\n",
    "print(f\"#extracted_pos_set: {len(extracted_spo_set)}, #gold_spo_set: {len(gold_spo_set)}\")\n",
    "print(f\"f1: {f1}, recall: {recall}, precision: {precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e48f2d",
   "metadata": {},
   "source": [
    "### Tranform relation triplets to nodes and edges\n",
    "Create relation dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df132387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [00:00, 19837.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "rel_dict = {}\n",
    "schema_path = 'data/schema.json'\n",
    "with open(schema_path) as f:\n",
    "    for l in tqdm(f):\n",
    "        rel = json.loads(l)\n",
    "        #schemas.add(a['predicate'])\n",
    "        predicate = rel['predicate']\n",
    "        sub_type = rel['subject_type']\n",
    "        obj_type = rel['object_type']['@value']\n",
    "        rel_dict[predicate] = {'subject_type': sub_type, 'object_type': obj_type}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4cf769a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'毕业院校': {'subject_type': '人物', 'object_type': '学校'},\n",
       " '嘉宾': {'subject_type': '电视综艺', 'object_type': '人物'},\n",
       " '配音': {'subject_type': '娱乐人物', 'object_type': '人物'},\n",
       " '主题曲': {'subject_type': '影视作品', 'object_type': '歌曲'},\n",
       " '代言人': {'subject_type': '企业/品牌', 'object_type': '人物'},\n",
       " '所属专辑': {'subject_type': '歌曲', 'object_type': '音乐专辑'},\n",
       " '父亲': {'subject_type': '人物', 'object_type': '人物'},\n",
       " '作者': {'subject_type': '图书作品', 'object_type': '人物'},\n",
       " '上映时间': {'subject_type': '影视作品', 'object_type': 'Date'},\n",
       " '母亲': {'subject_type': '人物', 'object_type': '人物'},\n",
       " '专业代码': {'subject_type': '学科专业', 'object_type': 'Text'},\n",
       " '占地面积': {'subject_type': '机构', 'object_type': 'Number'},\n",
       " '邮政编码': {'subject_type': '行政区', 'object_type': 'Text'},\n",
       " '票房': {'subject_type': '影视作品', 'object_type': 'Number'},\n",
       " '注册资本': {'subject_type': '企业', 'object_type': 'Number'},\n",
       " '主角': {'subject_type': '文学作品', 'object_type': '人物'},\n",
       " '妻子': {'subject_type': '人物', 'object_type': '人物'},\n",
       " '编剧': {'subject_type': '影视作品', 'object_type': '人物'},\n",
       " '气候': {'subject_type': '行政区', 'object_type': '气候'},\n",
       " '歌手': {'subject_type': '歌曲', 'object_type': '人物'},\n",
       " '获奖': {'subject_type': '娱乐人物', 'object_type': '奖项'},\n",
       " '校长': {'subject_type': '学校', 'object_type': '人物'},\n",
       " '创始人': {'subject_type': '企业', 'object_type': '人物'},\n",
       " '首都': {'subject_type': '国家', 'object_type': '城市'},\n",
       " '丈夫': {'subject_type': '人物', 'object_type': '人物'},\n",
       " '朝代': {'subject_type': '历史人物', 'object_type': 'Text'},\n",
       " '饰演': {'subject_type': '娱乐人物', 'object_type': '人物'},\n",
       " '面积': {'subject_type': '行政区', 'object_type': 'Number'},\n",
       " '总部地点': {'subject_type': '企业', 'object_type': '地点'},\n",
       " '祖籍': {'subject_type': '人物', 'object_type': '地点'},\n",
       " '人口数量': {'subject_type': '行政区', 'object_type': 'Number'},\n",
       " '制片人': {'subject_type': '影视作品', 'object_type': '人物'},\n",
       " '修业年限': {'subject_type': '学科专业', 'object_type': 'Number'},\n",
       " '所在城市': {'subject_type': '景点', 'object_type': '城市'},\n",
       " '董事长': {'subject_type': '企业', 'object_type': '人物'},\n",
       " '作词': {'subject_type': '歌曲', 'object_type': '人物'},\n",
       " '改编自': {'subject_type': '影视作品', 'object_type': '作品'},\n",
       " '出品公司': {'subject_type': '影视作品', 'object_type': '企业'},\n",
       " '导演': {'subject_type': '影视作品', 'object_type': '人物'},\n",
       " '作曲': {'subject_type': '歌曲', 'object_type': '人物'},\n",
       " '主演': {'subject_type': '影视作品', 'object_type': '人物'},\n",
       " '主持人': {'subject_type': '电视综艺', 'object_type': '人物'},\n",
       " '成立日期': {'subject_type': '机构', 'object_type': 'Date'},\n",
       " '简称': {'subject_type': '机构', 'object_type': 'Text'},\n",
       " '海拔': {'subject_type': '地点', 'object_type': 'Number'},\n",
       " '号': {'subject_type': '历史人物', 'object_type': 'Text'},\n",
       " '国籍': {'subject_type': '人物', 'object_type': '国家'},\n",
       " '官方语言': {'subject_type': '国家', 'object_type': '语言'}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26efa23",
   "metadata": {},
   "source": [
    "In order to transform entities and edges to a gremlin-compatible format, we need to assign ID to each of them. ID is currently constructed in a very simple way:\n",
    "```python\n",
    "node_id = 'node_' + node_type + '_' + node_name\n",
    "edge_id = 'edge_' + predicate + '_' + from + '_' + to\n",
    "```\n",
    "\n",
    "Again, we use a dataframe to store transformed edges and nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4bede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df = pd.DataFrame({'~id':[], '~label':[], 'name': []})\n",
    "edge_df = pd.DataFrame({'~id':[], '~from':[], '~to':[], '~label':[]})\n",
    "\n",
    "node_dict = {}\n",
    "\n",
    "# currently id is constructed naively.\n",
    "def node_name2id(entity_type, entity_name):\n",
    "    return 'node_' + entity_type + '_' + entity_name\n",
    "\n",
    "for idx, row in tqdm(rel_df.iterrows(), total=rel_df.shape[0]):\n",
    "    sub = row['subject']\n",
    "    obj = row['object']\n",
    "    rel = row['predicate']\n",
    "    sub_type = rel_dict[rel]['subject_type']\n",
    "    obj_type = rel_dict[rel]['object_type']\n",
    "    sub_id = 'node_' + sub_type + '_' + sub\n",
    "    obj_id = 'node_' + obj_type + '_' + obj\n",
    "    # order matter: ~id, ~label, name\n",
    "    node_dict[sub_id] = [sub_type, sub]\n",
    "    node_dict[obj_id] = [obj_type, obj]\n",
    "    edge_id = 'edge_' + rel + '_' + sub_id + '_' + obj_id\n",
    "    edge_df.loc[len(edge_df)] = [edge_id, sub_id, obj_id, rel]\n",
    "    \n",
    "for key, val in node_dict.items():\n",
    "    node_df.loc[len(node_df)] = [key, val[0], val[1]]  \n",
    "\n",
    "print(\"We have scanned {} nodes and {} relations\".format(len(node_df), len(edge_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae771eb8",
   "metadata": {},
   "source": [
    "Save nodes and relations to csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d1cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df.to_csv('generated/nodes.csv', index=False)\n",
    "edge_df.to_csv('generated/edges.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93a8c2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ec2-user ec2-user 28M Aug  3 08:20 generated/edges.csv\n"
     ]
    }
   ],
   "source": [
    "!ls -lh generated/edges.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a276bb",
   "metadata": {},
   "source": [
    "Upload nodes and edges files to S3 for bulkloading into Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32352b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: generated/edges.csv to s3://sm-nlp-data/ie-baseline/outputs/edges.csv\n",
      "upload: generated/nodes.csv to s3://sm-nlp-data/ie-baseline/outputs/nodes.csv\n",
      "The path for the Property Graph bulk loading step is 's3://sm-nlp-data/ie-baseline/outputs/'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# You need to relace this with your own S3 buckets and paths\n",
    "export S3_SAVE_BUCKET=\"sm-nlp-data\"\n",
    "export SAVE_PATH=\"ie-baseline/outputs\"\n",
    "aws s3 cp ./generated/edges.csv s3://$S3_SAVE_BUCKET/$SAVE_PATH/edges.csv\n",
    "aws s3 cp ./generated/nodes.csv s3://$S3_SAVE_BUCKET/$SAVE_PATH/nodes.csv\n",
    "\n",
    "echo \"The path for the Property Graph bulk loading step is 's3://$S3_SAVE_BUCKET/$SAVE_PATH/'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135a69f0",
   "metadata": {},
   "source": [
    "## Load Graph Data into Neptune\n",
    "\n",
    "You need to find your Netune endpoint and port in the Neptune database instance detail page. Here I paste mine.\n",
    "\n",
    "- Neptune endpoint & port: database-1-instance-1.c2ycbhkszo5s.us-east-1.neptune.amazonaws.com:8182 [info](https://console.aws.amazon.com/neptune/home?region=us-east-1#database:id=database-1-instance-1;is-cluster=false;tab=connectivity)\n",
    "- Source:\n",
    "    - s3://sm-nlp-data/ie-baseline/outputs/nodes.csv\n",
    "    - s3://sm-nlp-data/ie-baseline/outputs/edges.csv\n",
    "- IAM role ARN: arn:aws:iam::093729152554:role/service-role/AWSNeptuneNotebookRole-NepTestRole [link](https://console.aws.amazon.com/iam/home?region=us-east-1#/roles/AWSNeptuneNotebookRole-NepTestRole)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc31025",
   "metadata": {},
   "source": [
    "*Trouble shooting*:\n",
    "\n",
    "- You have to create an endpoint following the section 'Creating an Amazon S3 VPC Endpoint' in this [post](https://docs.aws.amazon.com/neptune/latest/userguide/bulk-load-data.html).\n",
    "- Choose the endpoint type as 'Gateway'.\n",
    "- Do select the check box next to the route tables that are associated "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f61e5a",
   "metadata": {},
   "source": [
    "Bulkload nodes and edges into Neptune using `loader` provided by Neptune with `curl` command. You need to specify neptune database and port, namely this part `https://database-2-instance-1.c2ycbhkszo5s.us-east-1.neptune.amazonaws.com:8182/`, as well as `source`, `iamRoleArn` and `region`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4abc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "database-2.cluster-c2ycbhkszo5s.us-east-1.neptune.amazonaws.com\n",
    "database-2-instance-1.c2ycbhkszo5s.us-east-1.neptune.amazonaws.com:8182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e20b124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"status\" : \"200 OK\",\n",
      "    \"payload\" : {\n",
      "        \"loadId\" : \"6ad96976-1b80-4e33-88d1-74faa308dba3\"\n",
      "    }\n",
      "}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   480  100   110  100   370    873   2936 --:--:-- --:--:-- --:--:--  3840\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "curl -X POST \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    https://database-2.cluster-c2ycbhkszo5s.us-east-1.neptune.amazonaws.com:8182/loader -d '\n",
    "    {\n",
    "      \"source\" : \"s3://sm-nlp-data/ie-baseline/outputs/\",\n",
    "      \"format\" : \"csv\",\n",
    "      \"iamRoleArn\" : \"arn:aws:iam::093729152554:role/NeptuneLoadFromS3\",\n",
    "      \"region\" : \"us-east-1\",\n",
    "      \"failOnError\" : \"FALSE\",\n",
    "      \"parallelism\" : \"MEDIUM\",\n",
    "      \"updateSingleCardinalityProperties\" : \"FALSE\",\n",
    "      \"queueRequest\" : \"TRUE\",\n",
    "      \"dependencies\" : []\n",
    "    }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71892ebd",
   "metadata": {},
   "source": [
    "Check load status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c76d6475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"status\" : \"200 OK\",\n",
      "    \"payload\" : {\n",
      "        \"feedCount\" : [\n",
      "            {\n",
      "                \"LOAD_COMPLETED\" : 2\n",
      "            }\n",
      "        ],\n",
      "        \"overallStatus\" : {\n",
      "            \"fullUri\" : \"s3://sm-nlp-data/ie-baseline/outputs/\",\n",
      "            \"runNumber\" : 8,\n",
      "            \"retryNumber\" : 0,\n",
      "            \"status\" : \"LOAD_COMPLETED\",\n",
      "            \"totalTimeSpent\" : 43,\n",
      "            \"startTime\" : 1627980773,\n",
      "            \"totalRecords\" : 572294,\n",
      "            \"totalDuplicates\" : 403768,\n",
      "            \"parsingErrors\" : 0,\n",
      "            \"datatypeMismatchErrors\" : 0,\n",
      "            \"insertErrors\" : 0\n",
      "        }\n",
      "    }\n",
      "}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   612  100   612    0     0  12750      0 --:--:-- --:--:-- --:--:-- 13021\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "curl -G 'https://database-2.cluster-c2ycbhkszo5s.us-east-1.neptune.amazonaws.com:8182/loader/6ad96976-1b80-4e33-88d1-74faa308dba3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958cdedb",
   "metadata": {},
   "source": [
    "Now, you can query this database within the same VPC using `curl` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bb392b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"requestId\":\"63cbad10-0674-4ecb-abfb-64d459eae351\",\"status\":{\"message\":\"\",\"code\":200,\"attributes\":{\"@type\":\"g:Map\",\"@value\":[]}},\"result\":{\"data\":{\"@type\":\"g:List\",\"@value\":[{\"@type\":\"g:Vertex\",\"@value\":{\"id\":\"node_人物_范琳琳\",\"label\":\"人物\",\"properties\":{\"name\":[{\"@type\":\"g:VertexProperty\",\"@value\":{\"id\":{\"@type\":\"g:Int32\",\"@value\":309923863},\"value\":\"范琳琳\",\"label\":\"name\"}}]}}},{\"@type\":\"g:Vertex\",\"@value\":{\"id\":\"node_人物_伍翠珍\",\"label\":\"人物\",\"properties\":{\"name\":[{\"@type\":\"g:VertexProperty\",\"@value\":{\"id\":{\"@type\":\"g:Int32\",\"@value\":-96177417},\"value\":\"伍翠珍\",\"label\":\"name\"}}]}}},{\"@type\":\"g:Vertex\",\"@value\":{\"id\":\"node_人物_捷克\",\"label\":\"人物\",\"properties\":{\"name\":[{\"@type\":\"g:VertexProperty\",\"@value\":{\"id\":{\"@type\":\"g:Int32\",\"@value\":-1571363459},\"value\":\"捷克\",\"label\":\"name\"}}]}}},{\"@type\":\"g:Vertex\",\"@value\":{\"id\":\"node_人物_许绍洋\",\"label\":\"人物\",\"properties\":{\"name\":[{\"@type\":\"g:VertexProperty\",\"@value\":{\"id\":{\"@type\":\"g:Int32\",\"@value\":381103735},\"value\":\"许绍洋\",\"label\":\"name\"}}]}}},{\"@type\":\"g:Vertex\",\"@value\":{\"id\":\"node_人物_郑智化\",\"label\":\"人物\",\"properties\":{\"name\":[{\"@type\":\"g:VertexProperty\",\"@value\":{\"id\":{\"@type\":\"g:Int32\",\"@value\":414845271},\"value\":\"郑智化\",\"label\":\"name\"}}]}}}]},\"meta\":{\"@type\":\"g:Map\",\"@value\":[]}}}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1344  100  1316  100    28  29909    636 --:--:-- --:--:-- --:--:-- 30545\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# show the total nodes on the current neptune instance\n",
    "curl -X POST -d '{\"gremlin\":\"g.V().limit(5)\"}' https://database-2.cluster-ro-c2ycbhkszo5s.us-east-1.neptune.amazonaws.com:8182/gremlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a21f5d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"code\":\"MalformedQueryException\",\"requestId\":\"68e366ae-6ec6-441f-b682-c7fc64fafb6f\",\"detailedMessage\":\"Failed to interpret Gremlin query: Query parsing failed at line 1, character position at 11, error message : token recognition error at: 'Text)'\"}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   288  100   250  100    38   5952    904 --:--:-- --:--:-- --:--:--  6857\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "curl -X POST -d '{\"gremlin\":\"g.hasLabel(Text).count()\"}' https://database-2.cluster-ro-c2ycbhkszo5s.us-east-1.neptune.amazonaws.com:8182/gremlin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d057bc78",
   "metadata": {},
   "source": [
    "## Access Neptune from Outside the VPC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a057ce3",
   "metadata": {},
   "source": [
    "We set up a load balancer to redirect traffics from outside the VPC to the neptune endpoints.\n",
    "\n",
    "Architectures and best practices of connecting to Neptune with load balancers are detailed in this post: [Connecting to Amazon Neptune from Clients Outside the Neptune VPC](https://github.com/aws-samples/aws-dbs-refarch-graph/tree/master/src/connecting-using-a-load-balancer).\n",
    "\n",
    "This [answer](https://stackoverflow.com/a/52622164) from stackoverflow may also help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423461ad",
   "metadata": {},
   "source": [
    "### Steps\n",
    "\n",
    "1. Find out Neptune cluster's master IP address. `dig +short <your cluster endpoint>` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7a84a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dig +short database-2-instance-1.c2ycbhkszo5s.us-east-1.neptune.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925738f0",
   "metadata": {},
   "source": [
    "2. Create an Application Load Balancer (ALB)\n",
    "    \n",
    "    - In EC2's left panel, click 'Load Balancer'. \n",
    "    - In availablity zone, make sure you select at least the zone where your Neptune DB instance is located. \n",
    "    - In Configure Security Groups, create a security group that allows inbound traffic from everywhere. i.e. Inbound TCP rule for 0.0.0.0 on 80.\n",
    "    - In Configure routing, choose target type as IP, protocal as HTTP, port as 80\n",
    "    - In register targets, add the IP Address obtained for step #1, and the port as 8182, then click \"add to list\".\n",
    "\n",
    "3. Access!\n",
    "\n",
    "After configuring an ALB(application load balancer), you can find the DNS name of the it in load balancers, the accessing port is as you set in \"configure routing\", which is 80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6743a005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"requestId\":\"a6133143-6e85-4c82-ac43-251335a74eeb\",\"status\":{\"message\":\"\",\"code\":200,\"attributes\":{\"@type\":\"g:Map\",\"@value\":[]}},\"result\":{\"data\":{\"@type\":\"g:List\",\"@value\":[{\"@type\":\"g:Vertex\",\"@value\":{\"id\":\"node_人物_范琳琳\",\"label\":\"人物\",\"properties\":{\"name\":[{\"@type\":\"g:VertexProperty\",\"@value\":{\"id\":{\"@type\":\"g:Int32\",\"@value\":309923863},\"value\":\"范琳琳\",\"label\":\"name\"}}]}}},{\"@type\":\"g:Vertex\",\"@value\":{\"id\":\"node_人物_伍翠珍\",\"label\":\"人物\",\"properties\":{\"name\":[{\"@type\":\"g:VertexProperty\",\"@value\":{\"id\":{\"@type\":\"g:Int32\",\"@value\":-96177417},\"value\":\"伍翠珍\",\"label\":\"name\"}}]}}},{\"@type\":\"g:Vertex\",\"@value\":{\"id\":\"node_人物_捷克\",\"label\":\"人物\",\"properties\":{\"name\":[{\"@type\":\"g:VertexProperty\",\"@value\":{\"id\":{\"@type\":\"g:Int32\",\"@value\":-1571363459},\"value\":\"捷克\",\"label\":\"name\"}}]}}},{\"@type\":\"g:Vertex\",\"@value\":{\"id\":\"node_人物_许绍洋\",\"label\":\"人物\",\"properties\":{\"name\":[{\"@type\":\"g:VertexProperty\",\"@value\":{\"id\":{\"@type\":\"g:Int32\",\"@value\":381103735},\"value\":\"许绍洋\",\"label\":\"name\"}}]}}},{\"@type\":\"g:Vertex\",\"@value\":{\"id\":\"node_人物_郑智化\",\"label\":\"人物\",\"properties\":{\"name\":[{\"@type\":\"g:VertexProperty\",\"@value\":{\"id\":{\"@type\":\"g:Int32\",\"@value\":414845271},\"value\":\"郑智化\",\"label\":\"name\"}}]}}}]},\"meta\":{\"@type\":\"g:Map\",\"@value\":[]}}}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1344  100  1316  100    28  47000   1000 --:--:-- --:--:-- --:--:-- 48000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "curl -X POST -d '{\"gremlin\":\"g.V().limit(5)\"}' alb-neptune-test-62758122.us-east-1.elb.amazonaws.com/gremlin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e809adf0",
   "metadata": {},
   "source": [
    "Set up the Gremlin console to connect to a Neptune DB instance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71819302",
   "metadata": {},
   "source": [
    "#### Use ipython-gremlin extension \n",
    "\n",
    "Install ipython gremlin by `!pip install ipython-gremlin --user`\n",
    "\n",
    "A detailed documentation can be found [here](https://ipython-gremlin.readthedocs.io/en/latest/usage.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9acb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext gremlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1e2a52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alias-- alb-neptune-test-62758122.us-east-1.elb.amazonaws.com --created for database at ws://alb-neptune-test-62758122.us-east-1.elb.amazonaws.com/gremlin\n",
      "Now using connection at ws://alb-neptune-test-62758122.us-east-1.elb.amazonaws.com/gremlin\n"
     ]
    }
   ],
   "source": [
    "%gremlin.connection.set_current ws://alb-neptune-test-62758122.us-east-1.elb.amazonaws.com/gremlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a11300",
   "metadata": {},
   "outputs": [],
   "source": [
    "verts = %gremlin g.V().limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3caabe",
   "metadata": {},
   "source": [
    "#### Use python API\n",
    "\n",
    "Ipython runs in a loop, this may cause problem for graph traversal, since it can not run in another loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2683094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77d3c1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v[node_机构_嘉兴中润光学科技有限公司], v[node_机构_厦门博乐德平台拍卖有限公司], v[node_机构_北京泡泡玛特文化创意有限公司], v[node_机构_大卫博士有限公司], v[node_机构_山东金天牛矿山机械有限公司]]\n"
     ]
    }
   ],
   "source": [
    "from __future__  import print_function  # Python 2/3 compatibility\n",
    "\n",
    "from gremlin_python import statics\n",
    "from gremlin_python.structure.graph import Graph\n",
    "from gremlin_python.process.graph_traversal import __\n",
    "from gremlin_python.process.strategies import *\n",
    "from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection\n",
    "\n",
    "graph = Graph()\n",
    "\n",
    "remoteConn = DriverRemoteConnection('wss://database-2.cluster-ro-c2ycbhkszo5s.us-east-1.neptune.amazonaws.com:8182/gremlin','g')\n",
    "g = graph.traversal().withRemote(remoteConn)\n",
    "\n",
    "print(g.V().hasLabel('机构').limit(5).toList())\n",
    "remoteConn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab754a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
