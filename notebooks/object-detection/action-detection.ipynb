{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral Object Detection\n",
    "\n",
    "This notebook details how to use [Detectron2](https://github.com/facebookresearch/detectron2) to build a POC object detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "import s3fs\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fs = s3fs.S3FileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 path to manifest file\n",
    "BUCKET = 'behavior-images'\n",
    "FOLDER = 'fps2-output/driver-actions'\n",
    "MANIFEST = 'manifests/output/output.manifest'\n",
    "\n",
    "# Define object dictionary\n",
    "objects = { '0': 'phone',\n",
    "            '1': 'cigarette',\n",
    "            '2': 'phub',\n",
    "            '4': 'smoke'\n",
    "          }\n",
    "\n",
    "# Define manifest files\n",
    "manifest = 's3://{}/{}/{}'.format(BUCKET, FOLDER, MANIFEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate and confirm the access to the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the bucket is in the same region as this notebook.\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.session.Session().region_name\n",
    "s3 = boto3.client('s3')\n",
    "bucket_region = s3.head_bucket(Bucket=BUCKET)['ResponseMetadata']['HTTPHeaders']['x-amz-bucket-region']\n",
    "assert bucket_region == region, \"Your S3 bucket {} and this notebook need to be in the same region. (notebook region: {}, bucket region: {})\".format(BUCKET, region, bucket_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data\n",
    "\n",
    "We will convert VOC data format to Json format for SageMaker API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fs.open(manifest) as f:\n",
    "    manifest_list = [json.loads(line.strip()) for line in f.readlines()]\n",
    "\n",
    "print(\"{} items are found in the manifest file.\".format(len(manifest_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_list[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset from s3 using s3 high level cli command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://behavior-images/fps2-input/ data/fs2-input/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVJoOm6LVJwW"
   },
   "source": [
    "Register the dataset to detectron2, following the [detectron2 custom dataset tutorial](https://detectron2.readthedocs.io/tutorials/datasets.html).\n",
    "Here, the dataset is in its custom format, therefore we write a function to parse it and prepare it into detectron2's standard format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.structures import BoxMode\n",
    "\n",
    "def get_data_dicts(manifest_list, img_dir):\n",
    "    \"\"\"Prepare datasets for detectron2 with manifest list and image directory\n",
    "\n",
    "    Args:\n",
    "        - manifest_list (list): annotation list\n",
    "        - img_dir (str): local directory where images are stored\n",
    "    Returns:\n",
    "        - dataset_dicts: list(dict)\n",
    "    \"\"\"\n",
    "    dataset_dicts = []\n",
    "    for idx, v in enumerate(manifest_list):\n",
    "        record = {}\n",
    "        file_name = v['source-ref'].rsplit('/', 1)[-1]\n",
    "        img_size = v['driver-actions']['image_size'][0] # idk why img_size is a list of more than one xywh\n",
    "        annotations = v['driver-actions']['annotations']\n",
    "        \n",
    "        record['file_name'] = os.path.join(img_dir, file_name)\n",
    "        record['image_id'] = idx\n",
    "        record['height'] = img_size['height']\n",
    "        record['width'] = img_size['width']\n",
    "        \n",
    "        objs = []\n",
    "        for annot in annotations:\n",
    "            x = annot['left']\n",
    "            y = annot['top']\n",
    "            w = annot['width']\n",
    "            h = annot['height']\n",
    "            category_id = annot['class_id']\n",
    "            obj = {\n",
    "                'bbox': [x, y, w, h],\n",
    "                'bbox_mode': BoxMode.XYWH_ABS,\n",
    "                'category_id': category_id\n",
    "            }\n",
    "            objs.append(obj)\n",
    "        record['annotations'] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register dataset and metadata. Read more in the same (tutorial)[https://detectron2.readthedocs.io/tutorials/datasets.html] about custom dataset. If you already registered it and you want to redo it, use `DatasetCatalog.remove('driver_action')` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetCatalog.remove('driver_action')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "DatasetCatalog.register(\"driver_action\", lambda : get_data_dicts(manifest_list, 'data/fs2-input'))\n",
    "MetadataCatalog.get(\"driver_actioin\").set(thing_classes=['phone', 'cigarette', 'phub', 'smoke'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ljbWTX0Wi8E"
   },
   "source": [
    "To verify the data loading is correct, let's visualize the annotations of randomly selected samples in the training set:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv2_imshow(image):\n",
    "    # set size\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # convert color from CV2 BGR back to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkNbUzUOLYf0"
   },
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import Visualizer\n",
    "\n",
    "dataset_dicts = get_data_dicts(manifest_list, 'data/fs2-input')\n",
    "for d in random.sample(dataset_dicts, 3):\n",
    "    print(d['file_name'])\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=MetadataCatalog.get(\"driver_actioin\"), scale=0.5)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a model and TRAIN it\n",
    "\n",
    "In this step, we finetued a COCO-pretrained R50 FPN Faster RCNN model. First we need to specify configurations for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")) #Get the basic model configuration from the model zoo \n",
    "#Passing the Train and Validation sets\n",
    "cfg.DATASETS.TRAIN = (\"driver_action\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "# Number of data loading threads\n",
    "cfg.DATALOADER.NUM_WORKERS = 4\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "# Number of images per batch across all machines.\n",
    "cfg.SOLVER.IMS_PER_BATCH = 4\n",
    "cfg.SOLVER.BASE_LR = 0.0125  # pick a good LearningRate\n",
    "cfg.SOLVER.MAX_ITER = 300  #No. of iterations   \n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128  \n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4 \n",
    "cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS = False\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we simply train it by defining a trainer and call its `train()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check training curves.\n",
    "Following magic commands have problem runing in SageMaker, follow this [post](https://stackoverflow.com/questions/47818822/can-i-use-tensorboard-with-google-colab) for trouble shooting.\n",
    "\n",
    "Also, you can launch it in a seperate console, and access it with `https://<YOUR_URL>.studio.region.sagemaker.aws/jupyter/default/proxy/6006/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference & evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a predictor using the model we just trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultPredictor\n",
    "\n",
    "# Inference should use the config with parameters that are used in training\n",
    "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWq1XHfDWiXO"
   },
   "source": [
    "Then, we randomly select several samples to visualize the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "U5LhISJqWXgM",
    "outputId": "4ac7a79d-9920-4783-abda-a011af783811"
   },
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import ColorMode\n",
    "# use the same dataset for training\n",
    "for d in random.sample(dataset_dicts, 3):    \n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=MetadataCatalog.get(\"driver_actioin\"), \n",
    "                   scale=0.5, \n",
    "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "    )\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use AP metrics for evaluation\n",
    "\n",
    "(this part can't run currently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "h9tECBQCvMv3",
    "outputId": "ddc6f792-f088-46c4-bb25-baa242b4bc1f"
   },
   "outputs": [],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "\n",
    "evaluator = COCOEvaluator(\"driver_action\", (\"bbox\"), False, output_dir=\"./output/\")\n",
    "val_loader = build_detection_test_loader(cfg, \"driver_action\")\n",
    "print(inference_on_dataset(trainer.model, val_loader, evaluator))\n",
    "# another equivalent way to evaluate the model is to use `trainer.test`"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/pytorch-1.6-gpu-py36-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
