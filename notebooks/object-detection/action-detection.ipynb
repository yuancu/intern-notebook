{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral Object Detection\n",
    "\n",
    "This notebook details how to use [Detectron2](https://github.com/facebookresearch/detectron2) to build a POC object detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "import s3fs\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fs = s3fs.S3FileSystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to access the manifest file generated by SageMaker Groundtruth. If you have it locally, replace the manifest with a local file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 path to manifest file\n",
    "BUCKET = 'behavior-images'\n",
    "FOLDER = 'fps2-output/driver-actions'\n",
    "MANIFEST = 'manifests/output/output.manifest'\n",
    "\n",
    "# Define object dictionary\n",
    "objects = { '0': 'phone',\n",
    "            '1': 'cigarette',\n",
    "            '2': 'phub',\n",
    "            '4': 'smoke'\n",
    "          }\n",
    "\n",
    "# Define manifest files\n",
    "manifest = 's3://{}/{}/{}'.format(BUCKET, FOLDER, MANIFEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate and confirm the access to the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the bucket is in the same region as this notebook.\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.session.Session().region_name\n",
    "s3 = boto3.client('s3')\n",
    "bucket_region = s3.head_bucket(Bucket=BUCKET)['ResponseMetadata']['HTTPHeaders']['x-amz-bucket-region']\n",
    "assert bucket_region == region, \"Your S3 bucket {} and this notebook need to be in the same region. (notebook region: {}, bucket region: {})\".format(BUCKET, region, bucket_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data\n",
    "\n",
    "We need to convert the manifest to the format that Detectron2 requires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace fs.open(manifest) with open(manifest_local) if you have it locally\n",
    "with fs.open(manifest) as f:\n",
    "    manifest_list = [json.loads(line.strip()) for line in f.readlines()]\n",
    "\n",
    "print(\"{} items are found in the manifest file.\".format(len(manifest_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_list[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset from s3 using s3 high level cli command. Skip this step if you have your dataset locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://behavior-images/fps2-input/ data/fs2-input/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVJoOm6LVJwW"
   },
   "source": [
    "Register the dataset to detectron2, following the [detectron2 custom dataset tutorial](https://detectron2.readthedocs.io/tutorials/datasets.html).\n",
    "Here, the dataset is in its custom format, therefore we write a function to parse it and prepare it into detectron2's standard format. It should contain `file_name`, `height`, `width`, `image_id`, `annotations` fields at least, the `annotations` is a list of `{bbox, bbox_mode, category_id}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.structures import BoxMode\n",
    "\n",
    "def get_data_dicts(manifest_list, img_dir):\n",
    "    \"\"\"Prepare datasets for detectron2 with manifest list and image directory\n",
    "\n",
    "    Args:\n",
    "        - manifest_list (list): annotation list\n",
    "        - img_dir (str): local directory where images are stored\n",
    "    Returns:\n",
    "        - dataset_dicts: list(dict)\n",
    "    \"\"\"\n",
    "    dataset_dicts = []\n",
    "    for idx, v in enumerate(manifest_list):\n",
    "        record = {}\n",
    "        file_name = v['source-ref'].rsplit('/', 1)[-1]\n",
    "        img_size = v['driver-actions']['image_size'][0] # idk why img_size is a list of more than one xywh\n",
    "        annotations = v['driver-actions']['annotations']\n",
    "        \n",
    "        record['file_name'] = os.path.join(img_dir, file_name)\n",
    "        record['image_id'] = idx\n",
    "        record['height'] = img_size['height']\n",
    "        record['width'] = img_size['width']\n",
    "        \n",
    "        objs = []\n",
    "        for annot in annotations:\n",
    "            x = annot['left']\n",
    "            y = annot['top']\n",
    "            w = annot['width']\n",
    "            h = annot['height']\n",
    "            category_id = annot['class_id']\n",
    "            obj = {\n",
    "                'bbox': [x, y, w, h],\n",
    "                'bbox_mode': BoxMode.XYWH_ABS,\n",
    "                'category_id': category_id\n",
    "            }\n",
    "            objs.append(obj)\n",
    "        record['annotations'] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register dataset and metadata. Read more in the same [tutorial](https://detectron2.readthedocs.io/tutorials/datasets.html) about custom dataset. If you already registered it and you want to redo it, use `DatasetCatalog.remove('driver_action')` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DatasetCatalog.remove('driver_action')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "# change data/fs2-input to where you store your images\n",
    "img_input_path = 'data/fs2-input'\n",
    "DatasetCatalog.register(\"driver_action\", lambda : get_data_dicts(manifest_list, img_input_path))\n",
    "MetadataCatalog.get(\"driver_action\").set(thing_classes=['phone', 'cigarette', 'phub', 'smoke'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ljbWTX0Wi8E"
   },
   "source": [
    "To verify the data loading is correct, let's visualize the annotations of randomly selected samples in the training set:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv2_imshow(image):\n",
    "    # set size\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # convert color from CV2 BGR back to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkNbUzUOLYf0"
   },
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import Visualizer\n",
    "\n",
    "dataset_dicts = get_data_dicts(manifest_list, img_input_path)\n",
    "for d in random.sample(dataset_dicts, 3):\n",
    "    print(d['file_name'])\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=MetadataCatalog.get(\"driver_action\"), scale=0.5)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a model and TRAIN it\n",
    "\n",
    "In this step, we finetued a COCO-pretrained R50 FPN Faster RCNN model. First we need to specify configurations for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")) #Get the basic model configuration from the model zoo \n",
    "#Passing the Train and Validation sets\n",
    "cfg.DATASETS.TRAIN = (\"driver_action\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "# Number of data loading threads\n",
    "cfg.DATALOADER.NUM_WORKERS = 4\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "# Number of images per batch across all machines.\n",
    "cfg.SOLVER.IMS_PER_BATCH = 4\n",
    "cfg.SOLVER.BASE_LR = 0.0125  # pick a good LearningRate\n",
    "cfg.SOLVER.MAX_ITER = 500  #No. of iterations   \n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128  \n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4 \n",
    "cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS = False\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we simply train it by defining a trainer and call its `train()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check training curves.\n",
    "Following magic commands have problem runing in SageMaker, follow this [post](https://stackoverflow.com/questions/47818822/can-i-use-tensorboard-with-google-colab) for trouble shooting.\n",
    "\n",
    "Also, you can launch it in a seperate console, and access it with `https://<YOUR_URL>.studio.region.sagemaker.aws/jupyter/default/proxy/6006/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference & evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a predictor using the model we just trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultPredictor\n",
    "\n",
    "# Inference should use the config with parameters that are used in training\n",
    "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWq1XHfDWiXO"
   },
   "source": [
    "Then, we randomly select several samples to visualize the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "U5LhISJqWXgM",
    "outputId": "4ac7a79d-9920-4783-abda-a011af783811"
   },
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import ColorMode\n",
    "# use the same dataset for training\n",
    "for d in random.sample(dataset_dicts, 3):    \n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=MetadataCatalog.get(\"driver_action\"), \n",
    "                   scale=0.5, \n",
    "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "    )\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use AP metrics for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create COCO format json file for evaluation. The functions are from detectron2 [coco dataset](https://github.com/facebookresearch/detectron2/blob/2c346e0b6ed95a6966564d588da8746f66adab38/detectron2/data/datasets/coco.py#L400).\n",
    "\n",
    "This the `xxx_coco_format.json` file should actually be automatically generated, however it fails. Here we manually create it and save it at `./output/driver_action_coco_format.json`, where the evaluation would later access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.structures import Boxes, BoxMode\n",
    "from fvcore.common.file_io import PathManager, file_lock\n",
    "import datetime\n",
    "\n",
    "def convert_to_coco_dict(dataset_name):\n",
    "    \"\"\"\n",
    "    Convert an instance detection/segmentation or keypoint detection dataset\n",
    "    in detectron2's standard format into COCO json format.\n",
    "    Generic dataset description can be found here:\n",
    "    https://detectron2.readthedocs.io/tutorials/datasets.html#register-a-dataset\n",
    "    COCO data format description can be found here:\n",
    "    http://cocodataset.org/#format-data\n",
    "    Args:\n",
    "        dataset_name (str):\n",
    "            name of the source dataset\n",
    "            Must be registered in DatastCatalog and in detectron2's standard format.\n",
    "            Must have corresponding metadata \"thing_classes\"\n",
    "    Returns:\n",
    "        coco_dict: serializable dict in COCO json format\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_dicts = DatasetCatalog.get(dataset_name)\n",
    "    metadata = MetadataCatalog.get(dataset_name)\n",
    "\n",
    "    # unmap the category mapping ids for COCO\n",
    "    if hasattr(metadata, \"thing_dataset_id_to_contiguous_id\"):\n",
    "        reverse_id_mapping = {v: k for k, v in metadata.thing_dataset_id_to_contiguous_id.items()}\n",
    "        reverse_id_mapper = lambda contiguous_id: reverse_id_mapping[contiguous_id]  # noqa\n",
    "    else:\n",
    "        reverse_id_mapper = lambda contiguous_id: contiguous_id  # noqa\n",
    "\n",
    "    categories = [\n",
    "        {\"id\": reverse_id_mapper(id), \"name\": name}\n",
    "        for id, name in enumerate(metadata.thing_classes)\n",
    "    ]\n",
    "\n",
    "    print(\"Converting dataset dicts into COCO format\")\n",
    "    coco_images = []\n",
    "    coco_annotations = []\n",
    "\n",
    "    for image_id, image_dict in enumerate(dataset_dicts):\n",
    "        coco_image = {\n",
    "            \"id\": image_dict.get(\"image_id\", image_id),\n",
    "            \"width\": image_dict[\"width\"],\n",
    "            \"height\": image_dict[\"height\"],\n",
    "            \"file_name\": image_dict[\"file_name\"],\n",
    "        }\n",
    "        coco_images.append(coco_image)\n",
    "\n",
    "        anns_per_image = image_dict[\"annotations\"]\n",
    "        for annotation in anns_per_image:\n",
    "            # create a new dict with only COCO fields\n",
    "            coco_annotation = {}\n",
    "\n",
    "            # COCO requirement: XYWH box format\n",
    "            bbox = annotation[\"bbox\"]\n",
    "            bbox_mode = annotation[\"bbox_mode\"]\n",
    "            bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYWH_ABS)\n",
    "\n",
    "            # COCO requirement: instance area\n",
    "            if \"segmentation\" in annotation:\n",
    "                # Computing areas for instances by counting the pixels\n",
    "                segmentation = annotation[\"segmentation\"]\n",
    "                # TODO: check segmentation type: RLE, BinaryMask or Polygon\n",
    "                if isinstance(segmentation, list):\n",
    "                    polygons = PolygonMasks([segmentation])\n",
    "                    area = polygons.area()[0].item()\n",
    "                elif isinstance(segmentation, dict):  # RLE\n",
    "                    area = mask_util.area(segmentation)\n",
    "                else:\n",
    "                    raise TypeError(f\"Unknown segmentation type {type(segmentation)}!\")\n",
    "            else:\n",
    "                # Computing areas using bounding boxes\n",
    "                bbox_xy = BoxMode.convert(bbox, BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)\n",
    "                area = Boxes([bbox_xy]).area()[0].item()\n",
    "\n",
    "            if \"keypoints\" in annotation:\n",
    "                keypoints = annotation[\"keypoints\"]  # list[int]\n",
    "                for idx, v in enumerate(keypoints):\n",
    "                    if idx % 3 != 2:\n",
    "                        # COCO's segmentation coordinates are floating points in [0, H or W],\n",
    "                        # but keypoint coordinates are integers in [0, H-1 or W-1]\n",
    "                        # For COCO format consistency we substract 0.5\n",
    "                        # https://github.com/facebookresearch/detectron2/pull/175#issuecomment-551202163\n",
    "                        keypoints[idx] = v - 0.5\n",
    "                if \"num_keypoints\" in annotation:\n",
    "                    num_keypoints = annotation[\"num_keypoints\"]\n",
    "                else:\n",
    "                    num_keypoints = sum(kp > 0 for kp in keypoints[2::3])\n",
    "\n",
    "            # COCO requirement:\n",
    "            #   linking annotations to images\n",
    "            #   \"id\" field must start with 1\n",
    "            coco_annotation[\"id\"] = len(coco_annotations) + 1\n",
    "            coco_annotation[\"image_id\"] = coco_image[\"id\"]\n",
    "            coco_annotation[\"bbox\"] = [round(float(x), 3) for x in bbox]\n",
    "            coco_annotation[\"area\"] = area\n",
    "            coco_annotation[\"iscrowd\"] = annotation.get(\"iscrowd\", 0)\n",
    "            coco_annotation[\"category_id\"] = reverse_id_mapper(annotation[\"category_id\"])\n",
    "\n",
    "            # Add optional fields\n",
    "            if \"keypoints\" in annotation:\n",
    "                coco_annotation[\"keypoints\"] = keypoints\n",
    "                coco_annotation[\"num_keypoints\"] = num_keypoints\n",
    "\n",
    "            if \"segmentation\" in annotation:\n",
    "                coco_annotation[\"segmentation\"] = annotation[\"segmentation\"]\n",
    "\n",
    "            coco_annotations.append(coco_annotation)\n",
    "\n",
    "    print(\n",
    "        \"Conversion finished, \"\n",
    "        f\"num images: {len(coco_images)}, num annotations: {len(coco_annotations)}\"\n",
    "    )\n",
    "\n",
    "    info = {\n",
    "        \"date_created\": str(datetime.datetime.now()),\n",
    "        \"description\": \"Automatically generated COCO json file for Detectron2.\",\n",
    "    }\n",
    "    coco_dict = {\n",
    "        \"info\": info,\n",
    "        \"images\": coco_images,\n",
    "        \"annotations\": coco_annotations,\n",
    "        \"categories\": categories,\n",
    "        \"licenses\": None,\n",
    "    }\n",
    "    return coco_dict\n",
    "\n",
    "def convert_to_coco_json(dataset_name, output_file, allow_cached=True):\n",
    "    \"\"\"\n",
    "    Converts dataset into COCO format and saves it to a json file.\n",
    "    dataset_name must be registered in DatasetCatalog and in detectron2's standard format.\n",
    "    Args:\n",
    "        dataset_name:\n",
    "            reference from the config file to the catalogs\n",
    "            must be registered in DatasetCatalog and in detectron2's standard format\n",
    "        output_file: path of json file that will be saved to\n",
    "        allow_cached: if json file is already present then skip conversion\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: The dataset or the conversion script *may* change,\n",
    "    # a checksum would be useful for validating the cached data\n",
    "\n",
    "    PathManager.mkdirs(os.path.dirname(output_file))\n",
    "    with file_lock(output_file):\n",
    "        if PathManager.exists(output_file) and allow_cached:\n",
    "            logger.warning(\n",
    "                f\"Using previously cached COCO format annotations at '{output_file}'. \"\n",
    "                \"You need to clear the cache file if your dataset has been modified.\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Converting annotations of dataset '{dataset_name}' to COCO format ...)\")\n",
    "            coco_dict = convert_to_coco_dict(dataset_name)\n",
    "\n",
    "            print(f\"Caching COCO format annotations at '{output_file}' ...\")\n",
    "            with PathManager.open(output_file, \"w\") as f:\n",
    "                json.dump(coco_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_coco_json('driver_action', './output/driver_action_coco_format.json', allow_cached=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "h9tECBQCvMv3",
    "outputId": "ddc6f792-f088-46c4-bb25-baa242b4bc1f"
   },
   "outputs": [],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "\n",
    "evaluator = COCOEvaluator(\"driver_action\", None, False, output_dir=\"./output/\")\n",
    "val_loader = build_detection_test_loader(cfg, \"driver_action\")\n",
    "print(inference_on_dataset(trainer.model, val_loader, evaluator))\n",
    "# another equivalent way to evaluate the model is to use `trainer.test`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually inference on a batch of frams\n",
    "\n",
    "Download frames to convert from S3. Frames are extracted from videos, you can extract your own with `$ffmpeg -i video.mp4 -vf fps=fps=2/1 img_%d.jpg`, remove the filter `-vf fps=fps=2/1` to extract all frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://behavior-images/all-frames-input/frames.tar.gz data/\n",
    "!tar -zxvf data/frames.tar.gz -C data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference frames by frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# make sure this folder exists, or  cv2.imwrite won't write anything\n",
    "Path('output/all_frames').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for img_path in tqdm(glob.glob('./data/frames/*.jpg'), desc=\"Inferencing\"):\n",
    "    im = cv2.imread(img_path)\n",
    "    outputs = predictor(im)\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=MetadataCatalog.get(\"driver_action\"), \n",
    "                   scale=0.5, \n",
    "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "    )\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    \n",
    "    cv2.imwrite(os.path.join(\"output\", \"all_frames\", img_path.rsplit('/', 1)[-1]), out.get_image())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compress inference results and move it to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -zcf inferred_frames.tar.gz output/all_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 mv inferred_frames.tar.gz s3://behavior-images/all-frames-output/"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/pytorch-1.6-gpu-py36-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
