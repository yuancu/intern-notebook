{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5822560",
   "metadata": {},
   "source": [
    "# Train the sequence model on raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b663bc9",
   "metadata": {},
   "source": [
    "### Embed arts and shadings\n",
    "\n",
    "1. train the embed model with custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116cc1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we requires latest version of pytorch and torchvision\n",
    "!pip install torch==1.9.0\n",
    "!pip install torchvision==0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e059de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: how the transformed image looks like\n",
    "from dataset.rawdata import ImageDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((100, 100))\n",
    "    ])\n",
    "data = ImageDataset(transform, '../data/asset/art', '../data/asset/shading')\n",
    "plt.imshow(data[0].permute((1, 2, 0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0031ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with how the original picture looks like\n",
    "from skimage import io\n",
    "img0 = io.imread(data.img_paths[0])\n",
    "plt.imshow(img0[:,:,:3]) # remove alpha channel\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ea9292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python embedding/embedding_main.py --data-set raw --lr 0.001 --epochs 400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9864e1b",
   "metadata": {},
   "source": [
    "2. load encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f14b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from embedding.models.embed_model import ConvEncoderDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62282fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "convcoder = ConvEncoderDecoder().to(device)\n",
    "convcoder.load_state_dict(torch.load('output/convcoder_raw.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637a002",
   "metadata": {},
   "source": [
    "3. get embeddings of foreground images and background images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df3806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from skimage import io\n",
    "from torchvision import transforms \n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((100, 100))\n",
    "    ])\n",
    "\n",
    "def encode(image, convcoder):\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(dim=0).to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = convcoder.encoder(image)\n",
    "        emb = torch.flatten(emb, start_dim=1, end_dim=-1)\n",
    "        emb = convcoder.embedder(emb)\n",
    "        emb = emb.cpu()\n",
    "    return emb\n",
    "\n",
    "def get_embeddings(image_dir, encoder):\n",
    "    '''\n",
    "    Parameters:\n",
    "    image_dir: str\n",
    "    encoder: nn.Module\n",
    "\n",
    "    Returns:\n",
    "    a dict of {image_number:embedding} pair\n",
    "    '''\n",
    "    \n",
    "    image_paths = glob.glob(image_dir + '/*.png')\n",
    "    assert len(image_paths) != 0, 'No png image found'\n",
    "    # TODO: use scikitimage to read image to keep rgb order\n",
    "    images = {int(path.split('/')[-1].split('.')[0]): io.imread(path) for path in image_paths}\n",
    "    encoded_images = {image_number: encode(image[:,:,:3], encoder)[0] for image_number, image in images.items()}\n",
    "    return encoded_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa55180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_embs = get_embeddings('../data/asset/art', convcoder)\n",
    "bg_embs = get_embeddings('../data/asset/shading', convcoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f41924",
   "metadata": {},
   "source": [
    "## Load data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189cccd",
   "metadata": {},
   "source": [
    "A backup: 1st verison definition of `build_raw_data`.\n",
    "```python\n",
    "def build_raw_data(bg_embs, fg_embs, img_manifest_path):\n",
    "    with open(img_manifest_path) as f:\n",
    "        img_list = [json.loads(line.strip()) for line in f.readlines()]\n",
    "    training_data = []\n",
    "    for img_info in img_list:\n",
    "        # record last x and y for calculating relative position\n",
    "        x_last = int(img_info['fg'][0][2])\n",
    "        y_last = int(img_info['fg'][0][3])\n",
    "        fg_reps = []\n",
    "        for fg in img_info['fg']:\n",
    "            number, _, x, y, scale, rotate, opaque = fg\n",
    "            # convert rotate in [0, 360] to [0, 1]\n",
    "            rotate /= 360\n",
    "            fg_emb = fg_embs[number]\n",
    "            x_rel, y_rel = x - x_last, y - y_last\n",
    "            x_last, y_last = x, y\n",
    "            # normalize x_rel and y_rel to [-1, 1]\n",
    "            fg_meta = torch.tensor([*normalize_relative_xy(x_rel, y_rel), scale, rotate, opaque])\n",
    "            fg_reps.append(torch.cat((fg_emb, fg_meta)))\n",
    "        fg_reps = torch.stack(fg_reps)\n",
    "        fg_reps = fg_reps.unsqueeze(dim=1) # (steps, mb_size=1, fg_emb_dim+5), 1 is the batch_size\n",
    "        # normalize class label from [-1, 1] to [0, 1] (TODO: -1 and 0 are both 0)\n",
    "        cls_label = 0 if int(img_info['flag']) < 0 else 1\n",
    "        cls_label = torch.tensor(cls_label, dtype=torch.long).unsqueeze(dim=0) # (mb_size=1, fg_emb_dim)\n",
    "        bk_emb = bg_embs[img_info['bg']].unsqueeze(dim=0)\n",
    "        training_data.append((fg_reps, bk_emb, cls_label))\n",
    "    return training_data\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0eb5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def truncate(x, lower, upper):\n",
    "    if x < lower:\n",
    "        return lower\n",
    "    if x > upper:\n",
    "        return upper\n",
    "    return x\n",
    "\n",
    "def normalize_relative_xy(x, y):\n",
    "    '''\n",
    "    normalize x, y from [-402, 402] and [-600, 600] to [-1, 1]\n",
    "    magic number comes from inspection of raw data\n",
    "    '''\n",
    "    x = truncate(x/402, -1, 1)\n",
    "    y = truncate(y/600, -1, 1)\n",
    "    return x, y\n",
    "\n",
    "def rescale_fgs(fgs):\n",
    "    '''\n",
    "    Make raw foreground meta data compatible with model\n",
    "    x, y: convert to relative to last position, scale to [-1, 1]\n",
    "    rotate: convert from [0, 360] to [0, 1]\n",
    "    \n",
    "    Parameters:\n",
    "    fgs: a list of (number, rank, x, y, scale, rotate, opaque)\n",
    "    '''\n",
    "    rescaled_fgs = []\n",
    "    for i, fg in enumerate(fgs):\n",
    "        number, rank, x, y, scale, rotate, opaque = fg\n",
    "        if i == 0:\n",
    "            x_last, y_last = x, y\n",
    "        # convert rotate in [0, 360] to [0, 1]\n",
    "        rescaled_rotate = rotate / 360\n",
    "        x_rel, y_rel = x - x_last, y - y_last\n",
    "        # normalize x_rel and y_rel to [-1, 1]\n",
    "        x_rel, y_rel = normalize_relative_xy(x_rel, y_rel)\n",
    "        # update last records\n",
    "        x_last, y_last = x, y\n",
    "        rescaled_fgs.append((number, rank, x_rel, y_rel, scale, rescaled_rotate, opaque))\n",
    "    return rescaled_fgs\n",
    "    \n",
    "\n",
    "def build_raw_data(bg_embs, fg_embs, img_list, batch_size):\n",
    "    '''\n",
    "    Generate training data with a meta file and corresponding embeddings\n",
    "    \n",
    "    Parameters:\n",
    "    bg_embs: dict \n",
    "        background embedding dictionary. key: number; entry: tensor of shape (emb_size, )\n",
    "    fg_embs: dict\n",
    "        foreground embedding dictionary. key: number; entry: tensor of shape (emb_size, )\n",
    "    img_manifest_path: str\n",
    "    batch_size: int\n",
    "    \n",
    "    Returns:\n",
    "    data_batch: list of (in_seqs, bk_embs, cls_labels) tuples\n",
    "        in_seqs: torch.Tensor of (seq_len, batch_size, fg_emb_size + 5)\n",
    "        bk_embs: torch.Tensor of (batch_size, bk_emb_size)\n",
    "        cls_labels: torch.Tensor of (batch_size)\n",
    "    '''\n",
    "    # group image by foreground art sequence length\n",
    "    img_group = defaultdict(list)\n",
    "    for img_info in img_list:\n",
    "        img_group[len(img_info['fg'])].append(img_info)\n",
    "    \n",
    "    training_data = []\n",
    "    # process data according to their different foreground sequence length\n",
    "    for fg_len, imgs in img_group.items():\n",
    "        # create mini-batch for data of a certain foreground sequence length\n",
    "        for i in range(0, len(imgs), batch_size):\n",
    "            fg_reps_batch = []\n",
    "            cls_label_batch = []\n",
    "            bk_emb_batch = []\n",
    "            for img_info in imgs[i:i+batch_size]:\n",
    "                # record last x and y for calculating relative position\n",
    "                x_last = int(img_info['fg'][0][2])\n",
    "                y_last = int(img_info['fg'][0][3])\n",
    "                fg_reps = []\n",
    "                for fg in img_info['fg']:\n",
    "                    number, _, x, y, scale, rotate, opaque = fg\n",
    "                    # convert rotate in [0, 360] to [0, 1]\n",
    "                    rotate /= 360\n",
    "                    fg_emb = fg_embs[number]\n",
    "                    x_rel, y_rel = x - x_last, y - y_last\n",
    "                    x_last, y_last = x, y\n",
    "                    # normalize x_rel and y_rel to [-1, 1]\n",
    "                    fg_meta = torch.tensor([*normalize_relative_xy(x_rel, y_rel), scale, rotate, opaque])\n",
    "                    fg_reps.append(torch.cat((fg_emb, fg_meta)))\n",
    "                fg_reps = torch.stack(fg_reps)\n",
    "                fg_reps = fg_reps.unsqueeze(dim=1) # (steps, mb_size=1, fg_emb_dim+5), 1 is the batch_size\n",
    "                # covert class label from [-1, 1] to [0, 2] (0 bad, 1 neutral, 2 good)\n",
    "                cls_label = int(img_info['flag']) + 1\n",
    "                cls_label = torch.tensor(cls_label, dtype=torch.long).unsqueeze(dim=0) # (mb_size=1, 1)\n",
    "                bk_emb = bg_embs[img_info['bg']].unsqueeze(dim=0)\n",
    "                fg_reps_batch.append(fg_reps)\n",
    "                cls_label_batch.append(cls_label)\n",
    "                bk_emb_batch.append(bk_emb)\n",
    "            fg_reps_batch = torch.cat(fg_reps_batch, dim=1)\n",
    "            cls_label_batch = torch.cat(cls_label_batch, dim=0)\n",
    "            bk_emb_batch = torch.cat(bk_emb_batch, dim=0)\n",
    "            training_data.append((fg_reps_batch, bk_emb_batch, cls_label_batch))\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c05a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/data.txt') as f:\n",
    "    img_manifest = [json.loads(line.strip()) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcacc21f",
   "metadata": {},
   "source": [
    "run this cell to split data into train, dev and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf180da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_manifest, test_manifest = train_test_split(img_manifest, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130069a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = build_raw_data(bg_embs, fg_embs, train_manifest, 128)\n",
    "test_data = build_raw_data(bg_embs, fg_embs, test_manifest, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05159a3",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "0. define hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccb224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict()\n",
    "args['seq_in_dim'] = 581\n",
    "args['input_hid_size'] = 576\n",
    "args['hid_dim'] = 256\n",
    "args['num_layers'] = 4\n",
    "args['lr'] = 0.001\n",
    "args['wd'] = 1e-6\n",
    "args['epochs'] = 800"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45615dbc",
   "metadata": {},
   "source": [
    "1. examine training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8544dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input sequence has shape {}\".format(train_data[0][0].shape))\n",
    "print(\"Background embedding has shape {}\".format(train_data[0][1].shape))\n",
    "print(\"Class labels has shape {}\".format(train_data[0][2].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce933457",
   "metadata": {},
   "source": [
    "2. define models(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b31175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequence.models.seq_model import sequence_model, seq_loss_fn\n",
    "\n",
    "seq_model = sequence_model(input_size=args['seq_in_dim'],\n",
    "                           input_hid_size=args['input_hid_size'],\n",
    "                           hidden_size=args['hid_dim'],\n",
    "                           num_layers=args['num_layers'])\n",
    "seq_model = seq_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a875018",
   "metadata": {},
   "source": [
    "3. define loss and optimizer\n",
    "   \n",
    "   will direct use the customized loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6330572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(seq_model.parameters(), lr=args['lr'], weight_decay=args['wd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8914d09",
   "metadata": {},
   "source": [
    "    define tensorboard writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4efe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "now = datetime.today()\n",
    "dt= now.strftime(\"%m_%d_%H_%M\")\n",
    "writer = SummaryWriter(os.path.join('./runs', dt))\n",
    "writer.add_text('Parameters', str(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687e4c32",
   "metadata": {},
   "source": [
    "4. training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac7b1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_data, seq_model):\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    seq_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (in_seqs, bk_embs, cls_labels) in enumerate(test_data):\n",
    "            in_seqs = in_seqs.to(device)\n",
    "            bk_embs = bk_embs.to(device)\n",
    "            cls_labels = cls_labels.to(device)\n",
    "\n",
    "            h_0 = torch.stack([bk_embs for _ in range(args['num_layers'])]).to(device)\n",
    "\n",
    "            c_0 = torch.zeros_like(h_0).to(device)\n",
    "\n",
    "            out_seqs_logits, cls_logits = seq_model(in_seqs[:-1,], (h_0, c_0), return_last_hidden=False)\n",
    "\n",
    "            loss, seq_loss, cls_loss = seq_loss_fn(out_seqs_logits, in_seqs[1:,], cls_logits, cls_labels, alpha=0.2, return_details=True)\n",
    "            total_loss += loss.item()\n",
    "            total_samples += len(cls_labels)\n",
    "    return total_loss / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d78571",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(args['epochs']):\n",
    "    seq_model.train()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    for i, (in_seqs, bk_embs, cls_labels) in enumerate(train_data):\n",
    "        in_seqs = in_seqs.to(device)\n",
    "        bk_embs = bk_embs.to(device)\n",
    "        cls_labels = cls_labels.to(device)\n",
    "\n",
    "        h_0 = torch.stack([bk_embs for _ in range(args['num_layers'])]).to(device)\n",
    "        # print(\"Hidden 0 shape {}\".format(h_0.shape))\n",
    "\n",
    "        c_0 = torch.zeros_like(h_0).to(device)\n",
    "\n",
    "        out_seqs_logits, cls_logits = seq_model(in_seqs[:-1,], (h_0, c_0), return_last_hidden=False)\n",
    "        \n",
    "        loss, seq_loss, cls_loss = seq_loss_fn(out_seqs_logits, in_seqs[1:,], cls_logits, cls_labels, alpha=0.2, return_details=True)\n",
    "        total_loss += loss.item()\n",
    "        total_samples += len(cls_labels)\n",
    "        writer.add_scalar('batch/train_loss', loss.item()/len(cls_labels), global_step=epoch*len(train_data)+i)\n",
    "        writer.add_scalar('batch/seq_loss', seq_loss.item()/len(cls_labels), global_step=epoch*len(train_data)+i)\n",
    "        writer.add_scalar('batch/cls_loss', cls_loss.item()/len(cls_labels), global_step=epoch*len(train_data)+i)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    \n",
    "    test_loss = test(test_data, seq_model)\n",
    "    writer.add_scalar('epoch/train_loss', total_loss/total_samples, global_step=epoch)\n",
    "    writer.add_scalar('epoch/test_loss', test_loss, global_step=epoch)\n",
    "    print(\"In epoch: {:03d} | loss: {:.6f}, test_loss: {:.6f}\".format(epoch, total_loss/total_samples, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a970166b",
   "metadata": {},
   "source": [
    "## Generate Images\n",
    "\n",
    "Define rate function for a generated sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e06f257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_seq(bg, fgs, seq_model, bg_embs, fg_embs):\n",
    "    '''\n",
    "    Rate a given background and foreground sequence.\n",
    "    \n",
    "    Parameters:\n",
    "    bg (int): background sequence number\n",
    "    fgs (list): a list of foreground meta data (number, rank, x, y, scale, rotate, opaque)\n",
    "    seq_model (nn.Module): the trained model for rating\n",
    "    bg_embs (dict): a {number:emb} dict for background\n",
    "    fg_embs (dict): a {number:emb} dict for foreground\n",
    "    '''\n",
    "    # convert (x, y) to relative positions; scale rotate by 1/360\n",
    "    rescaled_fgs = rescale_fgs(fgs)\n",
    "    # construct an art sequence with fgs\n",
    "    in_seqs = [torch.cat((fg_embs[fg[0]], torch.tensor(fg[-5:]))) for fg in rescaled_fgs]\n",
    "    in_seqs = torch.stack(in_seqs).unsqueeze(dim=1)\n",
    "    # unsqueeze dim 0 to imitate a batch behavior\n",
    "    bg_emb = bg_embs[bg].unsqueeze(dim=0)\n",
    "    h_0 = torch.stack([bg_emb]*args['num_layers'])\n",
    "    c_0 = torch.zeros_like(h_0)\n",
    "    \n",
    "    in_seqs = in_seqs.to(device)\n",
    "    h_0 = h_0.to(device)\n",
    "    c_0 = c_0.to(device)\n",
    "    with torch.no_grad():\n",
    "        _, cls_logits = seq_model(in_seqs, (h_0, c_0))\n",
    "    \n",
    "    return torch.argmax(cls_logits).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb323e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_seq(init_bg, fgs, seq_model, bg_embs, fg_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c72ac5",
   "metadata": {},
   "source": [
    "Define metrics for comparing foreground embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc436af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(A, B):\n",
    "    return ((A - B)**2).mean()\n",
    "\n",
    "def find_closest(emb, emb_dict):\n",
    "    diffs = {}\n",
    "    for k, v in emb_dict.items():\n",
    "        diffs[k] = mse(emb, v)\n",
    "    min_k = min(diffs, key=diffs.get)\n",
    "    return min_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b5a9ee",
   "metadata": {},
   "source": [
    "Helper function 1: Randomly generate an initial background and an inital foreground, along with foreground's meta data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ced1086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_inits():\n",
    "    init_bg = random.randint(1, 24) # shading is in [1, 24]\n",
    "    fg_seq_len = random.choice(range(15, 24))\n",
    "    init_fg = random.randint(2, 21) # art is in [2, 21]\n",
    "    init_fg_x = random.randint(10, 300)\n",
    "    init_fg_y = random.randint(10, 300)\n",
    "    init_fg_scale = random.uniform(0.9, 1.1)\n",
    "    init_fg_rotate = random.randint(0, 359)\n",
    "    init_fg_opaque = 1\n",
    "    init_fg_meta = (init_fg,0, init_fg_x,init_fg_y,init_fg_scale,init_fg_rotate,init_fg_opaque)\n",
    "    return init_bg, fg_seq_len, init_fg_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b803be",
   "metadata": {},
   "source": [
    "Helper function 2: Convert initial background and foreground to suitable input for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2606d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_init_model_input(init_bg, init_fg_meta, bg_embs, fg_embs):\n",
    "    fg_emb_0 = torch.cat((fg_embs[init_fg_meta[0]], torch.tensor([0, 0, init_fg_meta[4], init_fg_meta[5]/360, init_fg_meta[6]])))\n",
    "    fg_emb_0 = fg_emb_0.unsqueeze(dim=0).unsqueeze(dim=0).to(device)\n",
    "\n",
    "    bk_emb = bg_embs[init_bg]\n",
    "    h_0 = torch.stack([bk_emb for _ in range(args['num_layers'])]).unsqueeze(dim=1).to(device)\n",
    "    c_0 = torch.zeros_like(h_0).to(device)\n",
    "\n",
    "    # print(f\"h shape:{h_0.shape}; fg_emb.shape:{fg_emb_0.shape}; bk_emb.shape:{bk_emb.shape}\")\n",
    "    \n",
    "    return h_0, c_0, fg_emb_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11d24e2",
   "metadata": {},
   "source": [
    "Helper function 3: Generate a sequence of foreground with given background embedding and initial hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7986ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fg_seqs(init_bg, fg_seq_len, init_fg_meta, bg_embs, fg_embs):\n",
    "    h, c, fg_emb = generate_init_model_input(init_bg, init_fg_meta, bg_embs, fg_embs)\n",
    "    fgs = [init_fg_meta]\n",
    "    with torch.no_grad():\n",
    "        for i in range(1, fg_seq_len):\n",
    "        # when i is 1, use intact model\n",
    "            if i == 1:\n",
    "                seqs_logits, cls_logits, (h, c) = seq_model(fg_emb, (h, c), return_last_hidden=True)\n",
    "            # when i is not 1, use parts of the model to directly feed h and c to rnn\n",
    "            else:\n",
    "                output_seqs, (h, c) = seq_model.rnn_model(fg_emb, (h, c))\n",
    "                seqs_logits = seq_model.seq_transformer(output_seqs)\n",
    "            # process sequence logits to fit corresponding value scales\n",
    "            seqs_logits[:, :, -5:-2] = torch.sigmoid(seqs_logits[:, :, -5:-2])\n",
    "            seqs_logits[:, :, -2:] = torch.tanh(seqs_logits[:, :, -2:])\n",
    "            fg_emb = seqs_logits[0,0,:576] # next fg embedding\n",
    "            fg_meta = seqs_logits[0,0,576:] # (x, y, scale, angle, opaque)\n",
    "            fg_name = find_closest(fg_emb.detach().cpu(), fg_embs)\n",
    "            fg_emb = fg_embs[fg_name].to(device)\n",
    "            fg_emb = torch.cat((fg_emb, fg_meta))\n",
    "            fg_emb = fg_emb.unsqueeze(dim=0).unsqueeze(dim=0).to(device)\n",
    "            # again, magic numbers come from inspection of data\n",
    "            fgs.append((fg_name, i, int(fgs[-1][2]+fg_meta[0].item()*402), int(fgs[-1][3]+fg_meta[1].item()*600), \\\n",
    "                        fg_meta[2].item(), int(fg_meta[3]*360), fg_meta[4].item()))\n",
    "    return fgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f59626",
   "metadata": {},
   "source": [
    "Finally, generate a image within just one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df41b535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_image(bg_embs, fg_embs):\n",
    "    init_bg, fg_seq_len, init_fg_meta = generate_random_inits()\n",
    "    init_fg_x,init_fg_y,init_fg_scale,init_fg_rotate,init_fg_opaque = init_fg_meta[-5:]\n",
    "    print({'bg':init_bg, 'fg_seq_len':fg_seq_len, 'init_fg':{'(x, y)':(init_fg_x, init_fg_y), 'scale':init_fg_scale, \\\n",
    "           'rotate': init_fg_rotate, 'opaque': init_fg_opaque}})\n",
    "    fgs = generate_fg_seqs(init_bg, fg_seq_len, init_fg_meta, bg_embs, fg_embs)\n",
    "    return init_bg, fgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3244eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_bg, fgs = generate_random_image(bg_embs, fg_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fc23a7",
   "metadata": {},
   "source": [
    "Define image transformation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1476a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tile_image(mat, rows, cols):\n",
    "    '''\n",
    "    Tile mat vertically rows times and horizontally cols times\n",
    "    '''\n",
    "    tiled_image = np.tile(mat, (rows, cols, 1))\n",
    "    return tiled_image\n",
    "\n",
    "def vanilla_rotate(mat, angle):\n",
    "    '''\n",
    "    Rotate mat clockwise angle degrees. This operation keeps sizes and scales, which means \n",
    "    there will be information loss, i.e. corners of original image.\n",
    "    \n",
    "    mat: numpy.ndarray,(h, w, c)\n",
    "        Matrix to rotate.\n",
    "    angle: int\n",
    "        Natural number. It will be moded into [0, 360) \n",
    "    '''\n",
    "    rows,cols = mat.shape[:2]\n",
    "    # cols-1 and rows-1 are the coordinate limits.\n",
    "    M = cv2.getRotationMatrix2D(((cols-1)/2.0,(rows-1)/2.0),angle,1)\n",
    "    rotated = cv2.warpAffine(mat, M, (cols,rows))\n",
    "    return rotated\n",
    "\n",
    "def scale_image(mat, scale):\n",
    "    scaled_mat = cv2.resize(mat,None,fx=scale, fy=scale, interpolation = cv2.INTER_CUBIC)\n",
    "    return scaled_mat\n",
    "\n",
    "def transform_image(mat, scale, angle):\n",
    "    '''\n",
    "    scale and rotate image in one step\n",
    "    '''\n",
    "    scaled_mat = scale_image(mat, scale)\n",
    "    rotated_mat = vanilla_rotate(scaled_mat, angle)\n",
    "    return rotated_mat\n",
    "\n",
    "def overlay_transparent(background, overlay, x, y):\n",
    "    '''\n",
    "    Overlay top left coner of 'overlay' onto background at (x, y).\n",
    "    x, y are expected to be integers.\n",
    "    '''\n",
    "\n",
    "    background_width = background.shape[1]\n",
    "    background_height = background.shape[0]\n",
    "    h, w = overlay.shape[0], overlay.shape[1]\n",
    "\n",
    "    # when overlay is totally to the right or bottom of background\n",
    "    if x >= background_width or y >= background_height:\n",
    "        return background      \n",
    "    # when overlay is totally to the left or top of background\n",
    "    if x + w <= 0 or y + h <= 0:\n",
    "        return background\n",
    "\n",
    "    if x + w > background_width:\n",
    "        w = background_width - x\n",
    "        overlay = overlay[:, :w]   # truncate overlay's width right\n",
    "\n",
    "    if y + h > background_height:\n",
    "        h = background_height - y\n",
    "        overlay = overlay[:h]      # truncate overlay's height bottom\n",
    "\n",
    "    if x < 0:\n",
    "        w = x + w\n",
    "        overlay = overlay[:, -w:] # truncate overlay's width left\n",
    "    \n",
    "    if y < 0:\n",
    "        h = y + h\n",
    "        overlay = overlay[-h:]    # truncate overlay's height top\n",
    "\n",
    "    if overlay.shape[2] < 4:\n",
    "        overlay = np.concatenate(\n",
    "            [\n",
    "                overlay,\n",
    "                np.ones((overlay.shape[0], overlay.shape[1], 1), dtype = overlay.dtype) * 255\n",
    "            ],\n",
    "            axis = 2,\n",
    "        )\n",
    "\n",
    "    overlay_image = overlay[..., :4]\n",
    "    mask = overlay[..., 3:] / 255.0\n",
    "\n",
    "    y = max(y, 0)\n",
    "    x = max(x, 0)\n",
    "    background[y:y+h, x:x+w] = (1.0 - mask) * background[y:y+h, x:x+w] + mask * overlay_image\n",
    "\n",
    "    return background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906afc0c",
   "metadata": {},
   "source": [
    "Helper functions for image generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc97237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageColor\n",
    "\n",
    "def hex2rgb(hex_str):\n",
    "    return ImageColor.getcolor(hex_str, \"RGB\")\n",
    "\n",
    "def create_blank(width, height, rgb_color=(0, 0, 0)):\n",
    "    \"\"\"Create new image(numpy array) filled with certain color in RGB\"\"\"\n",
    "    # Create black blank image\n",
    "    image = np.zeros((height, width, 4), np.uint8)\n",
    "\n",
    "    # Since OpenCV uses BGR, convert the color first\n",
    "    color = tuple((*reversed(rgb_color), 255))\n",
    "    # Fill image with color\n",
    "    image[:] = color\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8314f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fg_path(idx):\n",
    "    return f'../data/asset/art/{idx}.png'\n",
    "\n",
    "def get_bg_path(idx):\n",
    "    return f'../data/asset/shading/{idx}.png'\n",
    "\n",
    "with open('../data/data.txt') as f:\n",
    "    img_list = [json.loads(line.strip()) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ef0f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img_info = img_list[0]\n",
    "\n",
    "bg_color = '#FFFFFF'\n",
    "bg_number = init_bg\n",
    "fg_list = fgs\n",
    "\n",
    "cursor = (0, 0)\n",
    "blank = create_blank(1000, 1000, hex2rgb(bg_color))\n",
    "\n",
    "bg = cv2.imread(get_bg_path(bg_number), -1)\n",
    "bg = scale_image(bg, 2)\n",
    "tiled = tile_image(bg, 12, 12)\n",
    "bg = overlay_transparent(blank, tiled[:1000, :1000], 0, 0)\n",
    "for fg_info in fg_list:\n",
    "    fg_idx = fg_info[0]\n",
    "    fg = cv2.imread(get_fg_path(fg_idx), -1)\n",
    "    fg_x, fg_y = fg_info[2], fg_info[3]\n",
    "    cursor = (fg_y, fg_x+200)\n",
    "    scale = float(fg_info[4])\n",
    "    angle = int(fg_info[5])\n",
    "    fg = transform_image(fg, scale, angle)\n",
    "    bg = overlay_transparent(bg, fg, cursor[0], cursor[1])\n",
    "changed_color = cv2.cvtColor(bg, cv2.COLOR_BGRA2RGBA)\n",
    "plt.imshow(changed_color)\n",
    "img2 = bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77553d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('output/gen.png', img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6014f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62404bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(cv2.imwrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8632de7b",
   "metadata": {},
   "source": [
    "## Inspect Data\n",
    "\n",
    "1. check statistics of x and y\n",
    "    x and y in train & test data are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407b8d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @unused\n",
    "def undo_batch(data_batched):\n",
    "    '''Convert batch back to list'''\n",
    "    data_list = []\n",
    "    for in_seqs, bk_embs, cls_labels in data_batched:\n",
    "        seqs = torch.split(in_seqs, 1, dim=1)\n",
    "        embs = torch.split(bk_embs, 1, dim=0)\n",
    "        labels = torch.split(cls_labels, 1, dim=0)\n",
    "        data_list += [sample for sample in zip(seqs, embs, labels)]\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb340de",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = build_raw_data(bg_embs, fg_embs, img_manifest, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c7c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = [], []\n",
    "for data in full_data:   \n",
    "    x += data[0][:, 0, -5]\n",
    "    y += data[0][:, 0, -4]\n",
    "\n",
    "print(f'Relative x and y positions: ({len(x)} records)')\n",
    "print(f'x: [{min(x)},{max(x)}]; \\tscaled: [{min(x)*402}, {max(x)*402}]')\n",
    "print(f'y: [{min(y)},{max(y)}]; \\tscaled: [{min(y)*600}, {max(y)*600}]')\n",
    "print(f'mean x: \\t{np.mean(x)}; \\tscaled: {np.mean(x)*402}')\n",
    "print(f'mean y: \\t{np.mean(y)}; \\tscaled: {np.mean(y)*600}')\n",
    "print(f'variance x: \\t{np.var(x)}; \\tscaled: {np.var(x)*402*402}')\n",
    "print(f'variance y: \\t{np.var(y)}; \\tscaled: {np.var(y)*600*600}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4c7d15",
   "metadata": {},
   "source": [
    "2. check class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa413af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_label = []\n",
    "for data in full_data:   \n",
    "    cls_label += data[2].tolist()\n",
    "cls_label = np.array(cls_label)\n",
    "print(f'negative samples: {(cls_label==0).sum()}')\n",
    "print(f'neutral samples: {(cls_label==1).sum()}')\n",
    "print(f'positive samples: {(cls_label==2).sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422968e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with data from original manifest\n",
    "flags = []\n",
    "for data in img_manifest:   \n",
    "    flags.append(data['flag'])\n",
    "flags = np.array(flags)\n",
    "print(f'negative samples: {(flags==-1).sum()}')\n",
    "print(f'neutral samples: {(flags==0).sum()}')\n",
    "print(f'positive samples: {(flags==1).sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eb8645",
   "metadata": {},
   "source": [
    "### Balance Class Distribution\n",
    "\n",
    "Will generate some more positive and neutral samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f9ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
