{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e9b1bcb",
   "metadata": {},
   "source": [
    "# Train the sequence model on raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5294d7",
   "metadata": {},
   "source": [
    "## Embed arts and shadings\n",
    "\n",
    "#### 1. train the embed model with custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655a2473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we requires latest version of pytorch and torchvision\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d67f2d",
   "metadata": {},
   "source": [
    "`ImageDataset` is a customized dataset that load images from multiple folders, and transform them.\n",
    "\n",
    "Here we examine whether the transformed images looks the same with the original ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c4bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: how the transformed image looks like\n",
    "from dataset.rawdata import ImageDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((100, 100))\n",
    "    ])\n",
    "data = ImageDataset(transform, '../data/asset/art', '../data/asset/shading')\n",
    "plt.imshow(data[0].permute((1, 2, 0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f40595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with how the original picture looks like\n",
    "from skimage import io\n",
    "img0 = io.imread(data.img_paths[0])\n",
    "plt.imshow(img0[:,:,:3]) # remove alpha channel\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b638cb5",
   "metadata": {},
   "source": [
    "Run this script to train an embedding model with images from arts and shadings. \n",
    "\n",
    "The trained model will be saved to `output/convcoder_raw.pt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83039dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python embedding/embedding_main.py --data-set raw --lr 0.001 --epochs 400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869bc813",
   "metadata": {},
   "source": [
    "#### 2. load embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcce152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from embedding.models.embed_model import ConvEncoderDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a68e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "convcoder = ConvEncoderDecoder().to(device)\n",
    "convcoder.load_state_dict(torch.load('output/convcoder_raw.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90505ad",
   "metadata": {},
   "source": [
    "#### 3. pre-load embeddings of foreground images and background images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cfebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from skimage import io\n",
    "from torchvision import transforms \n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((100, 100))\n",
    "    ])\n",
    "\n",
    "def encode(image, convcoder):\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(dim=0).to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = convcoder.encoder(image)\n",
    "        emb = torch.flatten(emb, start_dim=1, end_dim=-1)\n",
    "        emb = convcoder.embedder(emb)\n",
    "        emb = emb.cpu()\n",
    "    return emb\n",
    "\n",
    "def get_embeddings(image_dir, encoder):\n",
    "    '''\n",
    "    Parameters:\n",
    "    image_dir: str\n",
    "    encoder: nn.Module\n",
    "\n",
    "    Returns:\n",
    "    a dict of {image_number:embedding} pair\n",
    "    '''\n",
    "    \n",
    "    image_paths = glob.glob(image_dir + '/*.png')\n",
    "    assert len(image_paths) != 0, 'No png image found'\n",
    "    # TODO: use scikitimage to read image to keep rgb order\n",
    "    images = {int(path.split('/')[-1].split('.')[0]): io.imread(path) for path in image_paths}\n",
    "    encoded_images = {image_number: encode(image[:,:,:3], encoder)[0] for image_number, image in images.items()}\n",
    "    return encoded_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d661f1ab",
   "metadata": {},
   "source": [
    "`fg_embs` and `bg_embs` is a dictionary of fore- and background embeddings, indexed by the number of fore-/background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48e8db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_embs = get_embeddings('../data/asset/art', convcoder)\n",
    "bg_embs = get_embeddings('../data/asset/shading', convcoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7626df1",
   "metadata": {},
   "source": [
    "## Define Preprocess Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c922ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these magic numbers come from inspection of raw data\n",
    "SCALE_FACTOR_X = 402\n",
    "SCALE_FACTOR_Y = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ce79a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from PIL import ImageColor\n",
    "\n",
    "def hex2rgb(hex_str):\n",
    "    return ImageColor.getcolor(hex_str, \"RGB\")\n",
    "\n",
    "def truncate(x, lower, upper):\n",
    "    if x < lower:\n",
    "        return lower\n",
    "    if x > upper:\n",
    "        return upper\n",
    "    return x\n",
    "\n",
    "def normalize_relative_xy(x, y):\n",
    "    '''\n",
    "    normalize x, y from [-402, 402] and [-600, 600] to [-1, 1]\n",
    "    '''\n",
    "    x = truncate(x/SCALE_FACTOR_X, -1, 1)\n",
    "    y = truncate(y/SCALE_FACTOR_Y, -1, 1)\n",
    "    return x, y\n",
    "\n",
    "def rescale_fgs(fgs):\n",
    "    '''\n",
    "    Make raw foreground meta data compatible with model\n",
    "    x, y: convert to relative to last position, scale to [-1, 1]\n",
    "    rotate: convert from [0, 360] to [0, 1]\n",
    "    \n",
    "    Parameters:\n",
    "    fgs: a list of (number, rank, x, y, scale, rotate, opaque)\n",
    "    '''\n",
    "    rescaled_fgs = []\n",
    "    for i, fg in enumerate(fgs):\n",
    "        number, rank, x, y, scale, rotate, opaque = fg\n",
    "        if i == 0:\n",
    "            x_last, y_last = x, y\n",
    "        # convert rotate in [0, 360] to [0, 1]\n",
    "        rescaled_rotate = rotate / 360\n",
    "        x_rel, y_rel = x - x_last, y - y_last\n",
    "        # normalize x_rel and y_rel to [-1, 1]\n",
    "        x_rel, y_rel = normalize_relative_xy(x_rel, y_rel)\n",
    "        # update last records\n",
    "        x_last, y_last = x, y\n",
    "        rescaled_fgs.append((number, rank, x_rel, y_rel, scale, rescaled_rotate, opaque))\n",
    "    return rescaled_fgs\n",
    "    \n",
    "def color_emb(emb, color):\n",
    "    '''\n",
    "    Append color embedding to a given embedding\n",
    "    \n",
    "    Parameters:\n",
    "    emb (tensor): embedding of shape (emb_size, )\n",
    "    color (str or tuple): color in hex or tuple, i.e.'#FFFFFF' or (255, 255, 255)\n",
    "    '''\n",
    "    if isinstance(color, str):\n",
    "        r, g, b = hex2rgb(color)\n",
    "    else:\n",
    "        r, g, b = color\n",
    "    r, g, b = r/255, g/255, b/255\n",
    "    return torch.cat((emb, torch.tensor([r, g, b])))\n",
    "\n",
    "def build_raw_data(bg_embs, fg_embs, img_list, batch_size):\n",
    "    '''\n",
    "    Generate training data with a meta file and corresponding embeddings.\n",
    "    Data is first grouped by foreground sequence lengths. Then mini-batches are\n",
    "    created within each group. Finally all mini-batches are gathered and returned.\n",
    "    \n",
    "    Parameters:\n",
    "    bg_embs: dict \n",
    "        background embedding dictionary. key: number; entry: tensor of shape (emb_size, )\n",
    "    fg_embs: dict\n",
    "        foreground embedding dictionary. key: number; entry: tensor of shape (emb_size, )\n",
    "    img_manifest_path: str\n",
    "    batch_size: int\n",
    "    \n",
    "    Returns:\n",
    "    data_batch: list of (in_seqs, bk_embs, cls_labels) tuples\n",
    "        in_seqs: torch.Tensor of (seq_len, batch_size, fg_emb_size + 5)\n",
    "        bk_embs: torch.Tensor of (batch_size, bk_emb_size)\n",
    "        cls_labels: torch.Tensor of (batch_size)\n",
    "    '''\n",
    "    # group image by foreground art sequence length\n",
    "    img_group = defaultdict(list)\n",
    "    for img_info in img_list:\n",
    "        img_group[len(img_info['fg'])].append(img_info)\n",
    "    \n",
    "    training_data = []\n",
    "    # process data according to their different foreground sequence length\n",
    "    for fg_len, imgs in img_group.items():\n",
    "        # create mini-batch for data of a certain foreground sequence length\n",
    "        for i in range(0, len(imgs), batch_size):\n",
    "            fg_reps_batch = []\n",
    "            cls_label_batch = []\n",
    "            bk_emb_batch = []\n",
    "            for img_info in imgs[i:i+batch_size]:\n",
    "                # record last x and y for calculating relative position\n",
    "                x_last = int(img_info['fg'][0][2])\n",
    "                y_last = int(img_info['fg'][0][3])\n",
    "                fg_reps = []\n",
    "                # convert x, y to relative positions; scale rotate to 1/360\n",
    "                rescaled_fgs = rescale_fgs(img_info['fg'])\n",
    "                fg_reps = [torch.cat((fg_embs[fg[0]], torch.tensor(fg[-5:]))) for fg in rescaled_fgs]\n",
    "                fg_reps = torch.stack(fg_reps)\n",
    "                fg_reps = fg_reps.unsqueeze(dim=1) # (steps, mb_size=1, fg_emb_dim+5), 1 is the batch_size\n",
    "                # covert class label from [-1, 1] to [0, 2] (0 bad, 1 neutral, 2 good)\n",
    "                cls_label = int(img_info['flag']) + 1\n",
    "                cls_label = torch.tensor(cls_label, dtype=torch.long).unsqueeze(dim=0) # (mb_size=1, 1)\n",
    "                # embed background color into \n",
    "                bk_emb = color_emb(bg_embs[img_info['bg']], img_info['bgColor'])\n",
    "                bk_emb = bk_emb.unsqueeze(dim=0)\n",
    "                fg_reps_batch.append(fg_reps)\n",
    "                cls_label_batch.append(cls_label)\n",
    "                bk_emb_batch.append(bk_emb)\n",
    "            fg_reps_batch = torch.cat(fg_reps_batch, dim=1)\n",
    "            cls_label_batch = torch.cat(cls_label_batch, dim=0)\n",
    "            bk_emb_batch = torch.cat(bk_emb_batch, dim=0)\n",
    "            training_data.append((fg_reps_batch, bk_emb_batch, cls_label_batch))\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4afb953",
   "metadata": {},
   "source": [
    "## Inspect Data\n",
    "\n",
    "#### 1. Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccb8ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/data.txt') as f:\n",
    "    img_manifest = [json.loads(line.strip()) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a079972",
   "metadata": {},
   "source": [
    "#### 2. check statistics of x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be2b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @unused\n",
    "def undo_batch(data_batched):\n",
    "    '''Convert batch back to list'''\n",
    "    data_list = []\n",
    "    for in_seqs, bk_embs, cls_labels in data_batched:\n",
    "        seqs = torch.split(in_seqs, 1, dim=1)\n",
    "        embs = torch.split(bk_embs, 1, dim=0)\n",
    "        labels = torch.split(cls_labels, 1, dim=0)\n",
    "        data_list += [sample for sample in zip(seqs, embs, labels)]\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd62e1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def inspect_xy(all_fgs):\n",
    "    x, y = [], []\n",
    "    for fgs in all_fgs:   \n",
    "        x += [fg[-5] for fg in fgs]\n",
    "        y += [fg[-4] for fg in fgs]\n",
    "    print(f'Relative x and y positions: ({len(x)} records)')\n",
    "    print(f'x: [{min(x)},{max(x)}]; \\tscaled: [{min(x)*402}, {max(x)*402}]')\n",
    "    print(f'y: [{min(y)},{max(y)}]; \\tscaled: [{min(y)*600}, {max(y)*600}]')\n",
    "    print(f'mean x: \\t{np.mean(x)}; \\tscaled: {np.mean(x)*402}')\n",
    "    print(f'mean y: \\t{np.mean(y)}; \\tscaled: {np.mean(y)*600}')\n",
    "    print(f'variance x: \\t{np.var(x)}; \\tscaled: {np.var(x)*402*402}')\n",
    "    print(f'variance y: \\t{np.var(y)}; \\tscaled: {np.var(y)*600*600}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275ea146",
   "metadata": {},
   "source": [
    "x and y in train & test data are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4525c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is purely for statistics investigation\n",
    "full_data = build_raw_data(bg_embs, fg_embs, img_manifest, 1)\n",
    "\n",
    "train_fgs = []\n",
    "for data in full_data:\n",
    "    train_fgs += [data[0][:, 0, -5:].tolist()]\n",
    "inspect_xy(train_fgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd088aff",
   "metadata": {},
   "source": [
    "#### 3. Check statistics of scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a08c23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_statistics(data_list, name):\n",
    "    print(f'Records of {name}:\\t{len(data_list)}')\n",
    "    print(f'Range of {name}:  \\t[{min(data_list)},{max(data_list)}];')\n",
    "    print(f'Mean of {name}:   \\t{np.mean(data_list)}')\n",
    "    print(f'Variance of {name}:\\t{np.var(data_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cadc32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = []\n",
    "for img_info in img_manifest:\n",
    "    scales += [fg[-3] for fg in img_info['fg']]\n",
    "show_statistics(scales, 'scale')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad87201",
   "metadata": {},
   "source": [
    "#### 4. check class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d8ffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_label = []\n",
    "for data in full_data:   \n",
    "    cls_label += data[2].tolist()\n",
    "cls_label = np.array(cls_label)\n",
    "print(f'negative samples: {(cls_label==0).sum()}')\n",
    "print(f'neutral samples: {(cls_label==1).sum()}')\n",
    "print(f'positive samples: {(cls_label==2).sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd394c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with data from original manifest\n",
    "flags = []\n",
    "for data in img_manifest:   \n",
    "    flags.append(data['flag'])\n",
    "flags = np.array(flags)\n",
    "print(f'negative samples: {(flags==-1).sum()}')\n",
    "print(f'neutral samples: {(flags==0).sum()}')\n",
    "print(f'positive samples: {(flags==1).sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0d55ca",
   "metadata": {},
   "source": [
    "## Balance Class Distribution\n",
    "\n",
    "We will augment positive and neutral samples to make class distribution more banlanced.\n",
    "\n",
    "Assumption: If we remove the first or the last foreground art, the sequence is still a good/neutral/bad sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1fdfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "\n",
    "def augment_data(image_manifest, increment):\n",
    "    '''\n",
    "    Augment data by extracting part of original forground arts.\n",
    "    \n",
    "    Parameters:\n",
    "    image_manifest (list(dict)): image informations loaded from files, it has keys 'image', 'flag', 'bgColor', 'bg' and 'fg'\n",
    "    increment (int): number of new data to augment\n",
    "    '''\n",
    "    sampled = random.choices(image_manifest, k=increment)\n",
    "    augments = []\n",
    "    for aug in sampled:\n",
    "        aug = copy.deepcopy(aug)\n",
    "        aug['fg'] = aug['fg'][random.randint(0,1):random.choice([-1, None])]\n",
    "        augments.append(aug)\n",
    "    return image_manifest + augments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccb41cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_samples = [sample for sample in img_manifest if sample['flag']==-1]\n",
    "neutral_samples = [sample for sample in img_manifest if sample['flag']==0]\n",
    "positive_samples = [sample for sample in img_manifest if sample['flag']==1]\n",
    "\n",
    "# augment positive samples\n",
    "positive_samples = augment_data(positive_samples, max(len(negative_samples)-len(positive_samples),0))\n",
    "# augment nuetral samples\n",
    "neutral_samples = augment_data(neutral_samples, max(len(negative_samples)-len(neutral_samples),0))\n",
    "# combine them and overwrite original img_manifest\n",
    "img_manifest = negative_samples + neutral_samples + positive_samples\n",
    "\n",
    "print(f'total: {len(img_manifest)}; negative: {len(negative_samples)}; neutral: {len(neutral_samples)}; positive: {len(positive_samples)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fa01ad",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "#### 1. load data for training\n",
    "\n",
    "Run this cell to split data into train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6ead68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_manifest, test_manifest = train_test_split(img_manifest, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9660f3f0",
   "metadata": {},
   "source": [
    "Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5930328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = build_raw_data(bg_embs, fg_embs, train_manifest, 128)\n",
    "test_data = build_raw_data(bg_embs, fg_embs, test_manifest, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53568ee6",
   "metadata": {},
   "source": [
    "#### 2. define hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac96972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict()\n",
    "args['seq_in_dim'] = 581\n",
    "args['input_hid_size'] = 576 + 3 # background embedding + initial color embedding\n",
    "args['hid_dim'] = 256\n",
    "args['num_layers'] = 4\n",
    "args['lr'] = 0.001\n",
    "args['wd'] = 1e-6\n",
    "args['epochs'] = 800"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c284130a",
   "metadata": {},
   "source": [
    "#### 3. examine training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8b197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input sequence has shape {}\".format(train_data[0][0].shape))\n",
    "print(\"Background embedding has shape {}\".format(train_data[0][1].shape))\n",
    "print(\"Class labels has shape {}\".format(train_data[0][2].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234183b",
   "metadata": {},
   "source": [
    "#### 4. define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699fa4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequence.models.seq_model import sequence_model, seq_loss_fn\n",
    "\n",
    "seq_model = sequence_model(input_size=args['seq_in_dim'],\n",
    "                           input_hid_size=args['input_hid_size'],\n",
    "                           hidden_size=args['hid_dim'],\n",
    "                           num_layers=args['num_layers'])\n",
    "seq_model = seq_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05daa691",
   "metadata": {},
   "source": [
    "#### 5. define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d7bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(seq_model.parameters(), lr=args['lr'], weight_decay=args['wd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbf1b1d",
   "metadata": {},
   "source": [
    "**define tensorboard writer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681872f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "now = datetime.today()\n",
    "dt= now.strftime(\"%m_%d_%H_%M\")\n",
    "logname = 'uniform_weight_'\n",
    "writer = SummaryWriter(os.path.join('./runs', logname+dt))\n",
    "writer.add_text('Parameters', str(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b6c45a",
   "metadata": {},
   "source": [
    "#### 6. training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08752526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_data, seq_model):\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    seq_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (in_seqs, bk_embs, cls_labels) in enumerate(test_data):\n",
    "            in_seqs = in_seqs.to(device)\n",
    "            bk_embs = bk_embs.to(device)\n",
    "            cls_labels = cls_labels.to(device)\n",
    "\n",
    "            h_0 = torch.stack([bk_embs for _ in range(args['num_layers'])]).to(device)\n",
    "\n",
    "            c_0 = torch.zeros_like(h_0).to(device)\n",
    "\n",
    "            out_seqs_logits, cls_logits = seq_model(in_seqs[:-1,], (h_0, c_0), return_last_hidden=False)\n",
    "\n",
    "            loss, seq_loss, cls_loss = seq_loss_fn(out_seqs_logits, in_seqs[1:,], cls_logits, cls_labels, alpha=0.2, return_details=True)\n",
    "            total_loss += loss.item()\n",
    "            total_samples += len(cls_labels)\n",
    "    return total_loss / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637db464",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    for epoch in range(args['epochs']):\n",
    "        seq_model.train()\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        for i, (in_seqs, bk_embs, cls_labels) in enumerate(train_data):\n",
    "            in_seqs = in_seqs.to(device)\n",
    "            bk_embs = bk_embs.to(device)\n",
    "            cls_labels = cls_labels.to(device)\n",
    "\n",
    "            h_0 = torch.stack([bk_embs for _ in range(args['num_layers'])]).to(device)\n",
    "            # print(\"Hidden 0 shape {}\".format(h_0.shape))\n",
    "\n",
    "            c_0 = torch.zeros_like(h_0).to(device)\n",
    "\n",
    "            out_seqs_logits, cls_logits = seq_model(in_seqs[:-1,], (h_0, c_0), return_last_hidden=False)\n",
    "\n",
    "            loss, seq_loss, cls_loss = seq_loss_fn(out_seqs_logits, in_seqs[1:,], cls_logits, cls_labels, alpha=0.2, return_details=True)\n",
    "            total_loss += loss.item()\n",
    "            total_samples += len(cls_labels)\n",
    "            writer.add_scalar('batch/train_loss', loss.item()/len(cls_labels), global_step=epoch*len(train_data)+i)\n",
    "            writer.add_scalar('batch/seq_loss', seq_loss.item()/len(cls_labels), global_step=epoch*len(train_data)+i)\n",
    "            writer.add_scalar('batch/cls_loss', cls_loss.item()/len(cls_labels), global_step=epoch*len(train_data)+i)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        test_loss = test(test_data, seq_model)\n",
    "        writer.add_scalar('epoch/train_loss', total_loss/total_samples, global_step=epoch)\n",
    "        writer.add_scalar('epoch/test_loss', test_loss, global_step=epoch)\n",
    "        print(\"In epoch: {:03d} | loss: {:.6f}, test_loss: {:.6f}\".format(epoch, total_loss/total_samples, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06147369",
   "metadata": {},
   "source": [
    "## (Optional) Hyperparameter Search (未完成)\n",
    "\n",
    "we use a variable `config` to save configurable hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac6905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "config = {\n",
    "    'seq_in_dim': 581,\n",
    "    'input_hid_size': 576 + 3,\n",
    "    'hid_dim': 256,\n",
    "    'num_layers': tune.choice([3, 4, 5, 6]),\n",
    "    'epochs': 800,\n",
    "    'lr': tune.loguniform(1e-5, 1e-2),\n",
    "    'wd': tune.loguniform(1e-7, 1e-5)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528b5ed1",
   "metadata": {},
   "source": [
    "We wrap the training script in a function `train_raw(args, checkpoint_dir=None, data_dir=None)`. The `args` parameter will receive the hyperparameters we would like to train with. The `checkpoint_dir` parameter is used to restore checkpoints. The `data_dir` specifies the directory where we load and store the data, so multiple runs can share the same data source.\n",
    "\n",
    "`train_raw` is a self-contained function. Models, optimizers, etc. are re-defined in it.\n",
    "\n",
    "`train` is a helper function that wrpas train logics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8ed227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(seq_model, train_data):\n",
    "    seq_model.train()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    for i, (in_seqs, bk_embs, cls_labels) in enumerate(train_data):\n",
    "        in_seqs = in_seqs.to(device)\n",
    "        bk_embs = bk_embs.to(device)\n",
    "        cls_labels = cls_labels.to(device)\n",
    "\n",
    "        h_0 = torch.stack([bk_embs for _ in range(args['num_layers'])]).to(device)\n",
    "        # print(\"Hidden 0 shape {}\".format(h_0.shape))\n",
    "\n",
    "        c_0 = torch.zeros_like(h_0).to(device)\n",
    "\n",
    "        out_seqs_logits, cls_logits = seq_model(in_seqs[:-1,], (h_0, c_0), return_last_hidden=False)\n",
    "\n",
    "        loss, seq_loss, cls_loss = seq_loss_fn(out_seqs_logits, in_seqs[1:,], cls_logits, cls_labels, alpha=0.2, return_details=True)\n",
    "        total_loss += loss.item()\n",
    "        total_samples += len(cls_labels)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    return total_loss / total_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11418017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_raw(args, train_data, test_data=None, checkpoint_dir=None):\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    seq_model = sequence_model(input_size=args['seq_in_dim'],\n",
    "                           input_hid_size=args['input_hid_size'],\n",
    "                           hidden_size=args['hid_dim'],\n",
    "                           num_layers=args['num_layers'])\n",
    "    seq_model = seq_model.to(device)\n",
    "    \n",
    "    optim = torch.optim.Adam(seq_model.parameters(), lr=args['lr'], weight_decay=args['wd'])\n",
    "    \n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        seq_model.load_state_dict(model_state)\n",
    "        optim.load_state_dict(optimizer_state)\n",
    "    \n",
    "    for epoch in range(args['epochs']):\n",
    "        train_loss = train(seq_model, train_data)\n",
    "        test_loss = test(test_data, seq_model)\n",
    "        print(\"In epoch: {:03d} | loss: {:.6f}, test_loss: {:.6f}\".format(epoch, train_loss, test_loss))\n",
    "\n",
    "    \n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((seq_model.state_dict(), optim.state_dict()), path)\n",
    "            \n",
    "        tune.report(loss=test_loss)\n",
    "\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6eb0b3",
   "metadata": {},
   "source": [
    "Search best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881fbfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=800,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "reporter = CLIReporter(\n",
    "    # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
    "    metric_columns=[\"loss\", \"training_iteration\"])\n",
    "\n",
    "result = tune.run(\n",
    "    partial(train_raw, train_data=train_data, test_data=test_data),\n",
    "    config=config,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter,\n",
    "    checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c3cf50",
   "metadata": {},
   "source": [
    "## Image Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc35201",
   "metadata": {},
   "source": [
    "### 1. Define metrics\n",
    "\n",
    "#### define rate function to rate whether a generated foreground sequence is good or not.\n",
    "\n",
    "**Rate Process**:\n",
    "- Required: background shading, background color, foreground arts meta data, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db3a456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_seq(bg, bg_color, fgs, seq_model, bg_embs, fg_embs):\n",
    "    '''\n",
    "    Rate a given background and foreground sequence.\n",
    "    \n",
    "    Parameters:\n",
    "    bg (int): background sequence number\n",
    "    fgs (list): a list of foreground meta data (number, rank, x, y, scale, rotate, opaque)\n",
    "    seq_model (nn.Module): the trained model for rating\n",
    "    bg_embs (dict): a {number:emb} dict for background\n",
    "    fg_embs (dict): a {number:emb} dict for foreground\n",
    "    '''\n",
    "    # convert (x, y) to relative positions; scale rotate to 1/360\n",
    "    rescaled_fgs = rescale_fgs(fgs)\n",
    "    # construct an art sequence with fgs\n",
    "    in_seqs = [torch.cat((fg_embs[fg[0]], torch.tensor(fg[-5:]))) for fg in rescaled_fgs]\n",
    "    in_seqs = torch.stack(in_seqs).unsqueeze(dim=1)\n",
    "    # unsqueeze dim 0 to imitate a batch behavior\n",
    "    bg_emb = color_emb(bg_embs[bg], bg_color).unsqueeze(dim=0)\n",
    "    h_0 = torch.stack([bg_emb]*args['num_layers'])\n",
    "    c_0 = torch.zeros_like(h_0)\n",
    "    \n",
    "    in_seqs = in_seqs.to(device)\n",
    "    h_0 = h_0.to(device)\n",
    "    c_0 = c_0.to(device)\n",
    "    with torch.no_grad():\n",
    "        _, cls_logits = seq_model(in_seqs, (h_0, c_0))\n",
    "    \n",
    "    return torch.argmax(cls_logits).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bad5ad",
   "metadata": {},
   "source": [
    "#### define metrics for comparing foreground embeddings. \n",
    "\n",
    "This metric is for retrieving corresponding arts from database with generated art embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085fd4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(A, B):\n",
    "    return ((A - B)**2).mean()\n",
    "\n",
    "def find_closest(emb, emb_dict):\n",
    "    diffs = {}\n",
    "    for k, v in emb_dict.items():\n",
    "        diffs[k] = mse(emb, v)\n",
    "    min_k = min(diffs, key=diffs.get)\n",
    "    return min_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5ac448",
   "metadata": {},
   "source": [
    "### 2. Modulize image generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b60b363",
   "metadata": {},
   "source": [
    "**Helper function 1**: Randomly generate an initial background and an inital foreground, along with foreground's meta data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e2a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import colorgram\n",
    "\n",
    "def get_fg_path(idx):\n",
    "    return f'../data/asset/art/{idx}.png'\n",
    "\n",
    "def get_bg_path(idx):\n",
    "    return f'../data/asset/shading/{idx}.png'\n",
    "\n",
    "def generate_random_inits():\n",
    "    init_bg = random.randint(1, 24) # shading is in [1, 24]\n",
    "    candidate_colors = ['#00A99D', '#6ED0F6', '#8393CA', '#8EC73F', '#BC8DBF', '#F26621', '#F68E56']\n",
    "    init_bg_color = random.choice(candidate_colors)\n",
    "    # select again if encountered black color\n",
    "    if init_bg_color == (0, 0, 0):\n",
    "        init_bg_color = random.choice(candidate_colors)\n",
    "    fg_seq_len = random.choice(range(15, 24))\n",
    "    init_fg = random.randint(2, 21) # art is in [2, 21]\n",
    "    init_fg_x = random.randint(10, 300)\n",
    "    init_fg_y = random.randint(10, 300)\n",
    "    init_fg_scale = random.uniform(0.9, 1.1)\n",
    "    init_fg_rotate = random.randint(0, 359)\n",
    "    init_fg_opaque = 1\n",
    "    init_fg_meta = (init_fg,0, init_fg_x,init_fg_y,init_fg_scale,init_fg_rotate,init_fg_opaque)\n",
    "    return init_bg, init_bg_color, fg_seq_len, init_fg_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d21840b",
   "metadata": {},
   "source": [
    "**Helper function 2**: Convert initial background and foreground to suitable input for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053c1e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_init_model_input(init_bg, init_bg_color, init_fg_meta, bg_embs, fg_embs):\n",
    "    fg_emb_0 = torch.cat((fg_embs[init_fg_meta[0]], torch.tensor([0, 0, init_fg_meta[4], init_fg_meta[5]/360, init_fg_meta[6]])))\n",
    "    fg_emb_0 = fg_emb_0.unsqueeze(dim=0).unsqueeze(dim=0).to(device)\n",
    "\n",
    "    bk_emb = color_emb(bg_embs[init_bg], init_bg_color)\n",
    "    h_0 = torch.stack([bk_emb for _ in range(args['num_layers'])]).unsqueeze(dim=1).to(device)\n",
    "    c_0 = torch.zeros_like(h_0).to(device)\n",
    "\n",
    "    # print(f\"h shape:{h_0.shape}; fg_emb.shape:{fg_emb_0.shape}; bk_emb.shape:{bk_emb.shape}\")\n",
    "    \n",
    "    return h_0, c_0, fg_emb_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6926c4d9",
   "metadata": {},
   "source": [
    "**Helper function 3**: Generate a sequence of foreground with given background embedding and initial hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397e9e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fg_seqs(init_bg, init_bg_color, fg_seq_len, init_fg_meta, bg_embs, fg_embs):\n",
    "    h, c, fg_emb = generate_init_model_input(init_bg, init_bg_color, init_fg_meta, bg_embs, fg_embs)\n",
    "    fgs = [init_fg_meta]\n",
    "    with torch.no_grad():\n",
    "        for i in range(1, fg_seq_len):\n",
    "            # when i is 1, use intact model\n",
    "            if i == 1:\n",
    "                seqs_logits, cls_logits, (h, c) = seq_model(fg_emb, (h, c), return_last_hidden=True)           \n",
    "            # when i is int [2, seq_len-1], use parts of the model to directly feed h and c to rnn\n",
    "            else:\n",
    "                output_seqs, (h, c) = seq_model.rnn_model(fg_emb, (h, c))\n",
    "                seqs_logits = seq_model.seq_transformer(output_seqs)\n",
    "                if i == fg_seq_len - 1:\n",
    "                    cls_logits = seq_model.classifier(h[-1, :, :])\n",
    "                    rate = torch.argmax(cls_logits).item()\n",
    "            # process sequence logits to fit the input value scales    \n",
    "            seqs_logits[:, :, -5:-3] = torch.tanh(seqs_logits[:, :, -5:-3]) # x, y\n",
    "            seqs_logits[:, :, -3:] = torch.sigmoid(seqs_logits[:, :, -3:])  # scale, rotation, alpha\n",
    "            fg_emb = seqs_logits[0,0,:576] # next fg embedding\n",
    "            fg_meta = seqs_logits[0,0,576:] # (x, y, scale, angle, opaque)\n",
    "            fg_name = find_closest(fg_emb.detach().cpu(), fg_embs)\n",
    "            fg_emb = fg_embs[fg_name].to(device)\n",
    "            fg_emb = torch.cat((fg_emb, fg_meta))\n",
    "            fg_emb = fg_emb.unsqueeze(dim=0).unsqueeze(dim=0).to(device)\n",
    "            # again, magic numbers come from inspection of data\n",
    "            fgs.append((fg_name, i, int(fgs[-1][2]+fg_meta[0].item()*402), int(fgs[-1][3]+fg_meta[1].item()*600), \\\n",
    "                        fg_meta[2].item(), int(fg_meta[3]*360), fg_meta[4].item()))\n",
    "    return fgs, rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17072e71",
   "metadata": {},
   "source": [
    "**Helper function 4**: wraps above helper functions, generate a random image in one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0100a7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_image(bg_embs, fg_embs):\n",
    "    init_bg, init_bg_color, fg_seq_len, init_fg_meta = generate_random_inits()\n",
    "    # init_fg_x,init_fg_y,init_fg_scale,init_fg_rotate,init_fg_opaque = init_fg_meta[-5:]\n",
    "    # print({'bg':init_bg, 'fg_seq_len':fg_seq_len, 'init_fg':{'(x, y)':(init_fg_x, init_fg_y), 'scale':init_fg_scale, \\\n",
    "    #        'rotate': init_fg_rotate, 'opaque': init_fg_opaque}})\n",
    "    fgs, rate = generate_fg_seqs(init_bg, init_bg_color, fg_seq_len, init_fg_meta, bg_embs, fg_embs)\n",
    "    return init_bg, init_bg_color, fgs, rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2fa93d",
   "metadata": {},
   "source": [
    "### 3. Generate a bunch of images and rate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a388c5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_num = 100\n",
    "generated_images = []\n",
    "for i in range(image_num):\n",
    "    init_bg, init_bg_color, fgs, rate = generate_random_image(bg_embs, fg_embs)\n",
    "    # rate = rate_seq(init_bg, init_bg_color, fgs, seq_model, bg_embs, fg_embs)\n",
    "    if rate == 2:\n",
    "        generated_images.append((init_bg, init_bg_color, fgs))\n",
    "print(f'Generated {len(generated_images)} acceptable images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5632e0",
   "metadata": {},
   "source": [
    "### 4. Inspect generated images\n",
    "\n",
    "`row_plot` plots a row of figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db015e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def row_plot(data, savename=None):\n",
    "    fig, axes = plt.subplots(1, len(data), figsize=(5*len(data),5))\n",
    "    for i, d in enumerate(data):\n",
    "        if len(data) == 1:\n",
    "            sns.kdeplot(d, ax=axes)\n",
    "        else:\n",
    "            sns.kdeplot(d, ax=axes[i])\n",
    "    if savename is not None:\n",
    "        plt.savefig(savename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a171660e",
   "metadata": {},
   "source": [
    "1. Compare x and y (relative positions) distribution from traning data and from generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcaaf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fgs = []\n",
    "for data in full_data:\n",
    "    train_fgs += [data[0][:, 0, -5:].tolist()]\n",
    "print('Statistics of x and y from training images:')\n",
    "inspect_xy(train_fgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefb619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot x and y\n",
    "x_train, y_train = [], []\n",
    "for fgs in train_fgs:   \n",
    "    x_train += [fg[-5] for fg in fgs]\n",
    "    y_train += [fg[-4] for fg in fgs]\n",
    "row_plot([x_train, y_train], savename='output/xy_train.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93d1acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_fgs = [img[-1] for img in generated_images]\n",
    "generated_fgs = [rescale_fgs(fgs) for fgs in generated_fgs]\n",
    "print('Statistics of x and y of generated images:')\n",
    "inspect_xy(generated_fgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12fa32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gen, y_gen = [], []\n",
    "for fgs in generated_fgs:   \n",
    "    x_gen += [fg[-5] for fg in fgs]\n",
    "    y_gen += [fg[-4] for fg in fgs]\n",
    "row_plot([x_gen, y_gen], savename='output/xy_gen.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9a89d6",
   "metadata": {},
   "source": [
    "2. Compare scale distribution from training data and generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b918c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = []\n",
    "for img_info in img_manifest:\n",
    "    scales += [fg[-3] for fg in img_info['fg']]\n",
    "print('Statistics of scales from training images:')\n",
    "show_statistics(scales, 'scale')\n",
    "row_plot([scales])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e377df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_scales = []\n",
    "for fgs in generated_fgs:\n",
    "    generated_scales += [fg[-3] for fg in fgs]\n",
    "print('Statistics of scales from generated images:')\n",
    "show_statistics(generated_scales, 'scale')\n",
    "row_plot([generated_scales])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7679a569",
   "metadata": {},
   "source": [
    "## Visualize Generated Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f53fa",
   "metadata": {},
   "source": [
    "#### 1. Define image transformation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e5743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def tile_image(mat, rows, cols):\n",
    "    '''\n",
    "    Tile mat vertically rows times and horizontally cols times\n",
    "    '''\n",
    "    tiled_image = np.tile(mat, (rows, cols, 1))\n",
    "    return tiled_image\n",
    "\n",
    "def vanilla_rotate(mat, angle):\n",
    "    '''\n",
    "    Rotate mat clockwise angle degrees. This operation keeps sizes and scales, which means \n",
    "    there will be information loss, i.e. corners of original image.\n",
    "    \n",
    "    mat: numpy.ndarray,(h, w, c)\n",
    "        Matrix to rotate.\n",
    "    angle: int\n",
    "        Natural number. It will be moded into [0, 360) \n",
    "    '''\n",
    "    rows,cols = mat.shape[:2]\n",
    "    # cols-1 and rows-1 are the coordinate limits.\n",
    "    M = cv2.getRotationMatrix2D(((cols-1)/2.0,(rows-1)/2.0),angle,1)\n",
    "    rotated = cv2.warpAffine(mat, M, (cols,rows))\n",
    "    return rotated\n",
    "\n",
    "def scale_image(mat, scale):\n",
    "    scaled_mat = cv2.resize(mat,None,fx=scale, fy=scale, interpolation = cv2.INTER_CUBIC)\n",
    "    return scaled_mat\n",
    "\n",
    "def transform_image(mat, scale, angle):\n",
    "    '''\n",
    "    scale and rotate image in one step\n",
    "    '''\n",
    "    scaled_mat = scale_image(mat, scale)\n",
    "    rotated_mat = vanilla_rotate(scaled_mat, angle)\n",
    "    return rotated_mat\n",
    "\n",
    "def overlay_transparent(background, overlay, x, y):\n",
    "    '''\n",
    "    Overlay top left coner of 'overlay' onto background at (x, y).\n",
    "    x, y are expected to be integers.\n",
    "    '''\n",
    "\n",
    "    background_width = background.shape[1]\n",
    "    background_height = background.shape[0]\n",
    "    h, w = overlay.shape[0], overlay.shape[1]\n",
    "\n",
    "    # when overlay is totally to the right or bottom of background\n",
    "    if x >= background_width or y >= background_height:\n",
    "        return background      \n",
    "    # when overlay is totally to the left or top of background\n",
    "    if x + w <= 0 or y + h <= 0:\n",
    "        return background\n",
    "\n",
    "    if x + w > background_width:\n",
    "        w = background_width - x\n",
    "        overlay = overlay[:, :w]   # truncate overlay's width right\n",
    "\n",
    "    if y + h > background_height:\n",
    "        h = background_height - y\n",
    "        overlay = overlay[:h]      # truncate overlay's height bottom\n",
    "\n",
    "    if x < 0:\n",
    "        w = x + w\n",
    "        overlay = overlay[:, -w:] # truncate overlay's width left\n",
    "    \n",
    "    if y < 0:\n",
    "        h = y + h\n",
    "        overlay = overlay[-h:]    # truncate overlay's height top\n",
    "\n",
    "    if overlay.shape[2] < 4:\n",
    "        overlay = np.concatenate(\n",
    "            [\n",
    "                overlay,\n",
    "                np.ones((overlay.shape[0], overlay.shape[1], 1), dtype = overlay.dtype) * 255\n",
    "            ],\n",
    "            axis = 2,\n",
    "        )\n",
    "\n",
    "    overlay_image = overlay[..., :4]\n",
    "    mask = overlay[..., 3:] / 255.0\n",
    "\n",
    "    y = max(y, 0)\n",
    "    x = max(x, 0)\n",
    "    background[y:y+h, x:x+w] = (1.0 - mask) * background[y:y+h, x:x+w] + mask * overlay_image\n",
    "\n",
    "    return background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96040a21",
   "metadata": {},
   "source": [
    "#### 2. Define helper functions for plotting image:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02dbdfa",
   "metadata": {},
   "source": [
    "`grid_plot` plots generated images in grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaa2974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "import math\n",
    "\n",
    "def grid_plot(images, n_col, dpi=80, savename=None, savedpi=200, transparent=True):\n",
    "    n_gen = len(images)\n",
    "    n_row = math.ceil(n_gen / n_col)\n",
    "    figure(figsize=(14, 4 * n_row), dpi=dpi)\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(n_row, n_col, i+1, autoscale_on=True)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "    if savename is not None:\n",
    "        plt.savefig(savename, dpi=savedpi, transparent=transparent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14936f4",
   "metadata": {},
   "source": [
    "#### 3. Define helper functions for combining arts and shadings into images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fbe489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blank(width, height, rgb_color=(255, 255, 255), opacity=255):\n",
    "    \"\"\"Create new image(numpy array) filled with certain color in RGB\"\"\"\n",
    "    # Create black blank image\n",
    "    image = np.zeros((width, height, 4), np.uint8)\n",
    "\n",
    "    # Since OpenCV uses BGR, convert the color first\n",
    "    color = tuple((*reversed(rgb_color), opacity))\n",
    "    # Fill image with color\n",
    "    image[:] = color\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7509b2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @unused\n",
    "def rel2abs(fgs):\n",
    "    '''\n",
    "    Convert foreground images from scaled(relative position, scaled rotation) ones back to drawable format.\n",
    "    '''\n",
    "    absolute_fgs = []\n",
    "    for i, fg in enumerate(fgs):\n",
    "        if i == 0:\n",
    "            x_last, y_last = 0, 0\n",
    "        number, rank, rel_x, rel_y, scale, scaled_rotate, opaque = fg\n",
    "        # constants come from inspection of training data (defined in inspect data section)\n",
    "        x, y = int(x_last + rel_x * SCALE_FACTOR_X), int(y_last + rel_y * SCALE_FACTOR_Y)\n",
    "        x_last, y_last = x, y\n",
    "        rotate = int(scaled_rotate * 360)\n",
    "        absolute_fgs.append((number, rank, x, y, scale, rotate, opaque))\n",
    "    return absolute_fgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ff8f2a",
   "metadata": {},
   "source": [
    "`combine_fgs` combines all sets of foregrounds, and overlay it on to a background:\n",
    "\n",
    "test snipset:\n",
    "```python\n",
    "fg_combined = combine_fgs(fgs)\n",
    "print(fg_combined.shape)\n",
    "plt.imshow(fg_combined)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b15fe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_fgs(fgs):\n",
    "    '''\n",
    "    Combine all foregrounds into a transparent foreground sticker\n",
    "    \n",
    "    Parameters:\n",
    "    fgs (list): a list of (number, rank, x, y, scale, rotate, opaque) tuples.\n",
    "    '''\n",
    "    abs_fgs = np.array(fgs)\n",
    "    min_x, max_x = np.min(abs_fgs[:, 2]), np.max(abs_fgs[:, 2])\n",
    "    min_y, max_y = np.min(abs_fgs[:, 3]), np.max(abs_fgs[:, 3])\n",
    "    # 120 is a buffer for the width and height of the right-/bottom-most image\n",
    "    span_x = int(max_x - min_x) + 120 \n",
    "    span_y = int(max_y - min_y) + 120\n",
    "    # avoid span_x and span_y to be too large\n",
    "    span_x = min(span_x, 2000)\n",
    "    span_x = min(span_y, 2000)\n",
    "    # move the start of foregrounds to topleft\n",
    "    abs_fgs[:, 2] -= min_x\n",
    "    abs_fgs[:, 3] -= min_y\n",
    "    bg = create_blank(span_x, span_y, opacity=0)\n",
    "    for fg in abs_fgs:\n",
    "        number, rank, x, y, scale, rotate, opaque = fg\n",
    "        # convert np.float to int\n",
    "        number, x, y, rotate = int(number), int(x), int(y), int(rotate)\n",
    "        fg = cv2.imread(get_fg_path(number), -1)\n",
    "        cursor = (y, x)\n",
    "        fg = transform_image(fg, scale, rotate)\n",
    "        bg = overlay_transparent(bg, fg, cursor[0], cursor[1])\n",
    "    changed_color = cv2.cvtColor(bg, cv2.COLOR_BGRA2RGBA)\n",
    "    return changed_color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f311d14c",
   "metadata": {},
   "source": [
    "`tile_overlay` tiles foreground images and fully fills the given background.\n",
    "\n",
    "```python\n",
    "# test snipset:\n",
    "background = create_blank(2000, 2000, opacity=0)\n",
    "overlay = fg_combined\n",
    "combined = tile_overlay(background, overlay, 800, 500)\n",
    "plt.imshow(combined)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32ccace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def tile_overlay(background, overlay, x=0, y=0):\n",
    "    '''\n",
    "    Tile overlay multiple times to fully cover background.\n",
    "    \n",
    "    Parameters:\n",
    "    background (mat):\n",
    "    overlay (mat):\n",
    "    x (int): inital transition of overlay's top-left corner on x axis\n",
    "    y (int): inital transition of overlay's top-left corner on y axis\n",
    "    '''\n",
    "    background_width = background.shape[1]\n",
    "    background_height = background.shape[0]\n",
    "    h, w = overlay.shape[0], overlay.shape[1]\n",
    "    \n",
    "    # re-place x (to make sure top is fully filled)\n",
    "    x = x - math.ceil(x / h) * h\n",
    "    # re-place y (to make sure left is fully filled)\n",
    "    y = y - math.ceil(y / w) *  w\n",
    "    \n",
    "    # needed foreground on x axis\n",
    "    num_rows = math.ceil((background_height - x) / h)\n",
    "    # needed foreground on y axis\n",
    "    num_cols = math.ceil((background_width - y) / w)\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            overlay_x = x + i * h\n",
    "            overlay_y = y + j * w\n",
    "            # exchange x and y because overlay_transparent use a different coordinate\n",
    "            background = overlay_transparent(background, overlay, overlay_y, overlay_x)\n",
    "    return background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc17566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_tiled_image(bg, bg_color, fgs):\n",
    "    background_height = 1500\n",
    "    background_width = 1500\n",
    "\n",
    "    if isinstance(bg_color, str):\n",
    "        blank = create_blank(background_width, background_height, hex2rgb(bg_color))\n",
    "    else:\n",
    "        blank = create_blank(background_width, background_height, bg_color)\n",
    "\n",
    "    bg = cv2.imread(get_bg_path(bg), -1) # -1 flag reads alpha channel\n",
    "    bg = scale_image(bg, 2)\n",
    "    tiled = tile_image(bg, 14, 14)\n",
    "    bg = overlay_transparent(blank, tiled[:background_height, :background_width], 0, 0)\n",
    "    combined_fgs = combine_fgs(fgs)\n",
    "    bg = tile_overlay(bg, combined_fgs)\n",
    "    changed_color = cv2.cvtColor(bg, cv2.COLOR_BGRA2RGBA)\n",
    "    return changed_color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd726b1",
   "metadata": {},
   "source": [
    "#### 4. Plot tiled generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e5d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiled_plots = [draw_tiled_image(bg, bg_color, fgs) for bg, bg_color, fgs in generated_images]\n",
    "grid_plot(tiled_plots, 4, savename='output/generated.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a40bf1",
   "metadata": {},
   "source": [
    "## Draw some figures for slides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392c44c8",
   "metadata": {},
   "source": [
    "### Image generation step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d48ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg, bg_color, fgs = generated_images[4]\n",
    "background_height = 1500\n",
    "background_width = 1500\n",
    "if isinstance(bg_color, str):\n",
    "    blank = create_blank(background_width, background_height, hex2rgb(bg_color))\n",
    "else:\n",
    "    blank = create_blank(background_width, background_height, bg_color)\n",
    "blank_changed_color = cv2.cvtColor(blank, cv2.COLOR_BGRA2RGBA)\n",
    "plt.imshow(blank_changed_color)\n",
    "plt.axis('off')\n",
    "plt.savefig('output/blank.png', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5207da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = cv2.imread(get_bg_path(bg), -1) # -1 flag reads alpha channel\n",
    "bg = scale_image(bg, 2)\n",
    "tiled = tile_image(bg, 14, 14)\n",
    "bg = overlay_transparent(blank, tiled[:background_height, :background_width], 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa2ce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_changed_color = cv2.cvtColor(bg, cv2.COLOR_BGRA2RGBA)\n",
    "plt.imshow(bg_changed_color)\n",
    "plt.axis('off')\n",
    "plt.savefig('output/tiled_bg.png', transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78542a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_fgs = combine_fgs(fgs)\n",
    "combined_fgs_changed_color = cv2.cvtColor(combined_fgs, cv2.COLOR_BGRA2RGBA)\n",
    "plt.imshow(combined_fgs_changed_color)\n",
    "plt.axis('off')\n",
    "plt.savefig('output/combined_fgs_transparent.png', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a405b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlayed_img = tile_overlay(bg, combined_fgs)\n",
    "overlayed_img_changed_color = cv2.cvtColor(overlayed_img, cv2.COLOR_BGRA2RGBA)\n",
    "plt.imshow(overlayed_img_changed_color)\n",
    "plt.axis('off')\n",
    "plt.savefig('output/overlayed_bg.png', transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9149c5cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
