{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2feb3497",
   "metadata": {},
   "source": [
    "# Train the sequence model on raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee04268",
   "metadata": {},
   "source": [
    "### Embed arts and shadings\n",
    "\n",
    "1. train the embed model with custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1f8501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: how the transformed image looks like\n",
    "from dataset.rawdata import ImageDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((100, 100))\n",
    "    ])\n",
    "data = ImageDataset(transform, '../data/asset/art', '../data/asset/shading')\n",
    "plt.imshow(data[0].permute((1, 2, 0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154b6c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with how the original picture looks like\n",
    "from skimage import io\n",
    "img0 = io.imread(data.img_paths[0])\n",
    "plt.imshow(img0[:,:,:3]) # remove alpha channel\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba13f83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python embedding/embedding_main.py --data-set raw --lr 0.001 --epochs 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1641ce",
   "metadata": {},
   "source": [
    "2. load encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a6e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.9.0\n",
    "!pip install torchvision==0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f249b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from embedding.models.embed_model import ConvEncoderDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ea8627",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "convcoder = ConvEncoderDecoder().to(device)\n",
    "convcoder.load_state_dict(torch.load('output/convcoder_raw.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a98fe9b",
   "metadata": {},
   "source": [
    "3. get embeddings of foreground images and background images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7a875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from skimage import io\n",
    "from torchvision import transforms \n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((100, 100))\n",
    "    ])\n",
    "\n",
    "def encode(image, convcoder):\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(dim=0).to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = convcoder.encoder(image)\n",
    "        emb = torch.flatten(emb, start_dim=1, end_dim=-1)\n",
    "        emb = convcoder.embedder(emb)\n",
    "        emb = emb.cpu()\n",
    "    return emb\n",
    "\n",
    "def get_embeddings(image_dir, encoder):\n",
    "    '''\n",
    "    Parameters:\n",
    "    image_dir: str\n",
    "    encoder: nn.Module\n",
    "\n",
    "    Returns:\n",
    "    a dict of {image_number:embedding} pair\n",
    "    '''\n",
    "    \n",
    "    image_paths = glob.glob(image_dir + '/*.png')\n",
    "    assert len(image_paths) != 0, 'No png image found'\n",
    "    # TODO: use scikitimage to read image to keep rgb order\n",
    "    images = {int(path.split('/')[-1].split('.')[0]): io.imread(path) for path in image_paths}\n",
    "    encoded_images = {image_number: encode(image[:,:,:3], encoder)[0] for image_number, image in images.items()}\n",
    "    return encoded_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eb2095",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_embs = get_embeddings('../data/asset/art', convcoder)\n",
    "bg_embs = get_embeddings('../data/asset/shading', convcoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27bdee0",
   "metadata": {},
   "source": [
    "## Load data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3660994",
   "metadata": {},
   "source": [
    "A backup: 1st verison definition of `build_raw_data`.\n",
    "```python\n",
    "def build_raw_data(bg_embs, fg_embs, img_manifest_path):\n",
    "    with open(img_manifest_path) as f:\n",
    "        img_list = [json.loads(line.strip()) for line in f.readlines()]\n",
    "    training_data = []\n",
    "    for img_info in img_list:\n",
    "        # record last x and y for calculating relative position\n",
    "        x_last = int(img_info['fg'][0][2])\n",
    "        y_last = int(img_info['fg'][0][3])\n",
    "        fg_reps = []\n",
    "        for fg in img_info['fg']:\n",
    "            number, _, x, y, scale, rotate, opaque = fg\n",
    "            # convert rotate in [0, 360] to [0, 1]\n",
    "            rotate /= 360\n",
    "            fg_emb = fg_embs[number]\n",
    "            x_rel, y_rel = x - x_last, y - y_last\n",
    "            x_last, y_last = x, y\n",
    "            # normalize x_rel and y_rel to [-1, 1]\n",
    "            fg_meta = torch.tensor([*normalize_relative_xy(x_rel, y_rel), scale, rotate, opaque])\n",
    "            fg_reps.append(torch.cat((fg_emb, fg_meta)))\n",
    "        fg_reps = torch.stack(fg_reps)\n",
    "        fg_reps = fg_reps.unsqueeze(dim=1) # (steps, mb_size=1, fg_emb_dim+5), 1 is the batch_size\n",
    "        # normalize class label from [-1, 1] to [0, 1] (TODO: -1 and 0 are both 0)\n",
    "        cls_label = 0 if int(img_info['flag']) < 0 else 1\n",
    "        cls_label = torch.tensor(cls_label, dtype=torch.long).unsqueeze(dim=0) # (mb_size=1, fg_emb_dim)\n",
    "        bk_emb = bg_embs[img_info['bg']].unsqueeze(dim=0)\n",
    "        training_data.append((fg_reps, bk_emb, cls_label))\n",
    "    return training_data\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a75445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def normalize_relative_xy(x, y):\n",
    "    '''\n",
    "    normalize x, y from [-402, 402] and [-600, 600] to [-1, 1]\n",
    "    magic number comes from inspection of raw data\n",
    "    '''\n",
    "    return x/402, y/600\n",
    "\n",
    "def build_raw_data(bg_embs, fg_embs, img_manifest_path, batch_size):\n",
    "    '''\n",
    "    Generate training data with a meta file and corresponding embeddings\n",
    "    \n",
    "    Parameters:\n",
    "    bg_embs: dict \n",
    "        background embedding dictionary. key: number; entry: tensor of shape (emb_size, )\n",
    "    fg_embs: dict\n",
    "        foreground embedding dictionary. key: number; entry: tensor of shape (emb_size, )\n",
    "    img_manifest_path: str\n",
    "    batch_size: int\n",
    "    '''\n",
    "    with open(img_manifest_path) as f:\n",
    "        img_list = [json.loads(line.strip()) for line in f.readlines()]\n",
    "    # group image by foreground art sequence length\n",
    "    img_group = defaultdict(list)\n",
    "    for img_info in img_list:\n",
    "        img_group[len(img_info['fg'])].append(img_info)\n",
    "    \n",
    "    training_data = []\n",
    "    # process data according to their different foreground sequence length\n",
    "    for fg_len, imgs in img_group.items():\n",
    "        # create mini-batch for data of a certain foreground sequence length\n",
    "        for i in range(0, len(imgs), batch_size):\n",
    "            fg_reps_batch = []\n",
    "            cls_label_batch = []\n",
    "            bk_emb_batch = []\n",
    "            for img_info in imgs[i:i+batch_size]:\n",
    "                # record last x and y for calculating relative position\n",
    "                x_last = int(img_info['fg'][0][2])\n",
    "                y_last = int(img_info['fg'][0][3])\n",
    "                fg_reps = []\n",
    "                for fg in img_info['fg']:\n",
    "                    number, _, x, y, scale, rotate, opaque = fg\n",
    "                    # convert rotate in [0, 360] to [0, 1]\n",
    "                    rotate /= 360\n",
    "                    fg_emb = fg_embs[number]\n",
    "                    x_rel, y_rel = x - x_last, y - y_last\n",
    "                    x_last, y_last = x, y\n",
    "                    # normalize x_rel and y_rel to [-1, 1]\n",
    "                    fg_meta = torch.tensor([*normalize_relative_xy(x_rel, y_rel), scale, rotate, opaque])\n",
    "                    fg_reps.append(torch.cat((fg_emb, fg_meta)))\n",
    "                fg_reps = torch.stack(fg_reps)\n",
    "                fg_reps = fg_reps.unsqueeze(dim=1) # (steps, mb_size=1, fg_emb_dim+5), 1 is the batch_size\n",
    "                # normalize class label from [-1, 1] to [0, 1] (TODO: -1 and 0 are both 0)\n",
    "                cls_label = 0 if int(img_info['flag']) < 0 else 1\n",
    "                cls_label = torch.tensor(cls_label, dtype=torch.long).unsqueeze(dim=0) # (mb_size=1, fg_emb_dim)\n",
    "                bk_emb = bg_embs[img_info['bg']].unsqueeze(dim=0)\n",
    "                fg_reps_batch.append(fg_reps)\n",
    "                cls_label_batch.append(cls_label)\n",
    "                bk_emb_batch.append(bk_emb)\n",
    "            fg_reps_batch = torch.cat(fg_reps_batch, dim=1)\n",
    "            cls_label_batch = torch.cat(cls_label_batch, dim=0)\n",
    "            bk_emb_batch = torch.cat(bk_emb_batch, dim=0)\n",
    "            training_data.append((fg_reps_batch, bk_emb_batch, cls_label_batch))\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aee070",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = build_raw_data(bg_embs, fg_embs, '../../02_data/data.txt', 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbcf42f",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "0. define hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7cb534",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict()\n",
    "args['seq_in_dim'] = 581\n",
    "args['input_hid_size'] = 576\n",
    "args['hid_dim'] = 256\n",
    "args['num_layers'] = 3\n",
    "args['lr'] = 0.001\n",
    "args['wd'] = 1e-6\n",
    "args['epochs'] = 700"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be011352",
   "metadata": {},
   "source": [
    "1. examine training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c383f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input sequence has shape {}\".format(train_data[0][0].shape))\n",
    "print(\"Background embedding has shape {}\".format(train_data[0][1].shape))\n",
    "print(\"Class labels has shape {}\".format(train_data[0][2].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e11407",
   "metadata": {},
   "source": [
    "2. define models(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65911093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequence.models.seq_model import sequence_model, seq_loss_fn\n",
    "\n",
    "seq_model = sequence_model(input_size=args['seq_in_dim'],\n",
    "                           input_hid_size=args['input_hid_size'],\n",
    "                           hidden_size=args['hid_dim'],\n",
    "                           num_layers=args['num_layers'])\n",
    "seq_model = seq_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f953bc17",
   "metadata": {},
   "source": [
    "3. define loss and optimizer\n",
    "   \n",
    "   will direct use the customized loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4d65c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(seq_model.parameters(), lr=args['lr'], weight_decay=args['wd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3ed5ca",
   "metadata": {},
   "source": [
    "    define tensorboard writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c2567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "now = datetime.today()\n",
    "dt= now.strftime(\"%m_%d_%H_%M\")\n",
    "writer = SummaryWriter(os.path.join('./runs', dt))\n",
    "writer.add_text('Parameters', str(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00107e4a",
   "metadata": {},
   "source": [
    "4. training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7faf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(args['epochs']):\n",
    "    seq_model.train()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    for i, (in_seqs, bk_embs, cls_labels) in enumerate(train_data):\n",
    "        in_seqs = in_seqs.to(device)\n",
    "        bk_embs = bk_embs.to(device)\n",
    "        cls_labels = cls_labels.to(device)\n",
    "\n",
    "        h_0 = torch.stack([bk_embs for _ in range(args['num_layers'])]).to(device)\n",
    "        # print(\"Hidden 0 shape {}\".format(h_0.shape))\n",
    "\n",
    "        c_0 = torch.zeros_like(h_0).to(device)\n",
    "\n",
    "        out_seqs_logits, cls_logits = seq_model(in_seqs[:-1,], (h_0, c_0), return_last_hidden=False)\n",
    "        \n",
    "        loss = seq_loss_fn(out_seqs_logits, in_seqs[1:,], cls_logits, cls_labels, alpha=0.2)\n",
    "        total_loss += loss.item()\n",
    "        total_samples += len(cls_labels)\n",
    "        writer.add_scalar('batch/train_loss', loss.item()/len(cls_labels), global_step=epoch*len(train_data)+i)\n",
    "        \n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "    writer.add_scalar('epoch/train_loss', total_loss, global_step=epoch)\n",
    "    print(\"In epoch: {:03d} | loss: {:.6f}\".format(epoch, total_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f592717",
   "metadata": {},
   "source": [
    "## Generate Images\n",
    "\n",
    "Define meta data for generating new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940cf7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_seq_len = 20\n",
    "init_bg = 8\n",
    "init_fg_meta = (15,0,-134,374,0.85,327,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a09db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_emb_0 = torch.cat((fg_embs[init_fg_meta[0]], torch.tensor([0, 0, init_fg_meta[4], init_fg_meta[5]/360, init_fg_meta[6]])))\n",
    "fg_emb_0 = fg_emb_0.unsqueeze(dim=0).unsqueeze(dim=0).to(device)\n",
    "\n",
    "bk_emb = bg_embs[init_bg]\n",
    "h_0 = torch.stack([bk_emb for _ in range(args['num_layers'])]).unsqueeze(dim=1).to(device)\n",
    "c_0 = torch.zeros_like(h_0).to(device)\n",
    "\n",
    "print(f\"h shape:{h_0.shape}; fg_emb.shape:{fg_emb_0.shape}; bk_emb.shape:{bk_emb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763a0ddc",
   "metadata": {},
   "source": [
    "Define metrics for comparing foreground embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8880f926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(A, B):\n",
    "    return ((A - B)**2).mean()\n",
    "\n",
    "def find_closest(emb, emb_dict):\n",
    "    diffs = {}\n",
    "    for k, v in emb_dict.items():\n",
    "        diffs[k] = mse(emb, v)\n",
    "    min_k = min(diffs, key=diffs.get)\n",
    "    return min_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263e3d24",
   "metadata": {},
   "source": [
    "Generate a sequence of foreground arts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43006f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, c = h_0, c_0\n",
    "fg_emb = fg_emb_0\n",
    "i = 1\n",
    "fgs = [init_fg_meta]\n",
    "with torch.no_grad():\n",
    "    for i in range(1, fg_seq_len):\n",
    "    # when i is 1, use intact model\n",
    "        if i == 1:\n",
    "            seqs_logits, cls_logits, (h, c) = seq_model(fg_emb, (h, c), return_last_hidden=True)\n",
    "        # when i is not 1, use parts of the model to directly feed h and c to rnn\n",
    "        else:\n",
    "            output_seqs, (h, c) = seq_model.rnn_model(fg_emb, (h, c))\n",
    "            seqs_logits = seq_model.seq_transformer(output_seqs)\n",
    "        # process sequence logits to fit corresponding value scales\n",
    "        seqs_logits[:, :, -5:-2] = torch.sigmoid(seqs_logits[:, :, -5:-2])\n",
    "        seqs_logits[:, :, -2:] = torch.tanh(seqs_logits[:, :, -2:])\n",
    "        fg_emb = seqs_logits[0,0,:576] # next fg embedding\n",
    "        fg_meta = seqs_logits[0,0,576:] # (x, y, scale, angle, opaque)\n",
    "        fg_name = find_closest(fg_emb.detach().cpu(), fg_embs)\n",
    "        fg_emb = fg_embs[fg_name].to(device)\n",
    "        fg_emb = torch.cat((fg_emb, fg_meta))\n",
    "        fg_emb = fg_emb.unsqueeze(dim=0).unsqueeze(dim=0).to(device)\n",
    "        # again, magic numbers come from inspection of data\n",
    "        fgs.append([fg_name, i, fgs[-1][2]+fg_meta[0].item()*402, fgs[-1][3]+fg_meta[1].item()*600] +  [it.item() for it in fg_meta[2:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8571e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971ee3c4",
   "metadata": {},
   "source": [
    "Define image generation related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986df7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_image(mat, angle):\n",
    "    \"\"\"\n",
    "    Rotates an image (angle in degrees) and expands image to avoid cropping\n",
    "    \"\"\"\n",
    "\n",
    "    height, width = mat.shape[:2] # image shape has 3 dimensions\n",
    "    image_center = (width/2, height/2) # getRotationMatrix2D needs coordinates in reverse order (width, height) compared to shape\n",
    "\n",
    "    rotation_mat = cv2.getRotationMatrix2D(image_center, angle, 1.)\n",
    "\n",
    "    # rotation calculates the cos and sin, taking absolutes of those.\n",
    "    abs_cos = abs(rotation_mat[0,0]) \n",
    "    abs_sin = abs(rotation_mat[0,1])\n",
    "\n",
    "    # find the new width and height bounds\n",
    "    bound_w = int(height * abs_sin + width * abs_cos)\n",
    "    bound_h = int(height * abs_cos + width * abs_sin)\n",
    "\n",
    "    # subtract old image center (bringing image back to origo) and adding the new image center coordinates\n",
    "    rotation_mat[0, 2] += bound_w/2 - image_center[0]\n",
    "    rotation_mat[1, 2] += bound_h/2 - image_center[1]\n",
    "\n",
    "    # rotate image with the new bounds and translated rotation matrix\n",
    "    rotated_mat = cv2.warpAffine(mat, rotation_mat, (bound_w, bound_h))\n",
    "    return rotated_mat\n",
    "\n",
    "def scale_image(mat, scale):\n",
    "    scaled_mat = cv2.resize(mat,None,fx=scale, fy=scale, interpolation = cv2.INTER_CUBIC)\n",
    "    return scaled_mat\n",
    "\n",
    "def transform_image(mat, scale, angle):\n",
    "    '''\n",
    "    scale and rotate image in one step\n",
    "    '''\n",
    "    scaled_mat = scale_image(mat, scale)\n",
    "    rotated_mat = vanilla_rotate(scaled_mat, angle)\n",
    "    return rotated_mat\n",
    "\n",
    "def overlay_transparent(background, overlay, x, y):\n",
    "\n",
    "    background_width = background.shape[1]\n",
    "    background_height = background.shape[0]\n",
    "\n",
    "    if x >= background_width or y >= background_height:\n",
    "        return background\n",
    "\n",
    "    h, w = overlay.shape[0], overlay.shape[1]\n",
    "\n",
    "    if x + w > background_width:\n",
    "        w = background_width - x\n",
    "        overlay = overlay[:, :w]\n",
    "\n",
    "    if y + h > background_height:\n",
    "        h = background_height - y\n",
    "        overlay = overlay[:h]\n",
    "\n",
    "    if overlay.shape[2] < 4:\n",
    "        overlay = np.concatenate(\n",
    "            [\n",
    "                overlay,\n",
    "                np.ones((overlay.shape[0], overlay.shape[1], 1), dtype = overlay.dtype) * 255\n",
    "            ],\n",
    "            axis = 2,\n",
    "        )\n",
    "\n",
    "    overlay_image = overlay[..., :4]\n",
    "    mask = overlay[..., 3:] / 255.0\n",
    "\n",
    "    background[y:y+h, x:x+w] = (1.0 - mask) * background[y:y+h, x:x+w] + mask * overlay_image\n",
    "\n",
    "    return background"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
