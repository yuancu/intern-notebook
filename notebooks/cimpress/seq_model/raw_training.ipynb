{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fbd2765",
   "metadata": {},
   "source": [
    "# Train the sequence model on raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfd1bc5",
   "metadata": {},
   "source": [
    "## Embed arts and shadings\n",
    "\n",
    "1. train the embed model with custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdfd998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we requires latest version of pytorch and torchvision\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bf6284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: how the transformed image looks like\n",
    "from dataset.rawdata import ImageDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((100, 100))\n",
    "    ])\n",
    "data = ImageDataset(transform, '../data/asset/art', '../data/asset/shading')\n",
    "plt.imshow(data[0].permute((1, 2, 0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4aeb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with how the original picture looks like\n",
    "from skimage import io\n",
    "img0 = io.imread(data.img_paths[0])\n",
    "plt.imshow(img0[:,:,:3]) # remove alpha channel\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c32c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python embedding/embedding_main.py --data-set raw --lr 0.001 --epochs 400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745224b1",
   "metadata": {},
   "source": [
    "2. load encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b39ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from embedding.models.embed_model import ConvEncoderDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c077e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "convcoder = ConvEncoderDecoder().to(device)\n",
    "convcoder.load_state_dict(torch.load('output/convcoder_raw.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf6ec7f",
   "metadata": {},
   "source": [
    "3. get embeddings of foreground images and background images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55817b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from skimage import io\n",
    "from torchvision import transforms \n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((100, 100))\n",
    "    ])\n",
    "\n",
    "def encode(image, convcoder):\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(dim=0).to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = convcoder.encoder(image)\n",
    "        emb = torch.flatten(emb, start_dim=1, end_dim=-1)\n",
    "        emb = convcoder.embedder(emb)\n",
    "        emb = emb.cpu()\n",
    "    return emb\n",
    "\n",
    "def get_embeddings(image_dir, encoder):\n",
    "    '''\n",
    "    Parameters:\n",
    "    image_dir: str\n",
    "    encoder: nn.Module\n",
    "\n",
    "    Returns:\n",
    "    a dict of {image_number:embedding} pair\n",
    "    '''\n",
    "    \n",
    "    image_paths = glob.glob(image_dir + '/*.png')\n",
    "    assert len(image_paths) != 0, 'No png image found'\n",
    "    # TODO: use scikitimage to read image to keep rgb order\n",
    "    images = {int(path.split('/')[-1].split('.')[0]): io.imread(path) for path in image_paths}\n",
    "    encoded_images = {image_number: encode(image[:,:,:3], encoder)[0] for image_number, image in images.items()}\n",
    "    return encoded_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820f13fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_embs = get_embeddings('../data/asset/art', convcoder)\n",
    "bg_embs = get_embeddings('../data/asset/shading', convcoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0048264",
   "metadata": {},
   "source": [
    "## Define Preprocess Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f5a731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from PIL import ImageColor\n",
    "\n",
    "def hex2rgb(hex_str):\n",
    "    return ImageColor.getcolor(hex_str, \"RGB\")\n",
    "\n",
    "def truncate(x, lower, upper):\n",
    "    if x < lower:\n",
    "        return lower\n",
    "    if x > upper:\n",
    "        return upper\n",
    "    return x\n",
    "\n",
    "def normalize_relative_xy(x, y):\n",
    "    '''\n",
    "    normalize x, y from [-402, 402] and [-600, 600] to [-1, 1]\n",
    "    magic number comes from inspection of raw data\n",
    "    '''\n",
    "    x = truncate(x/402, -1, 1)\n",
    "    y = truncate(y/600, -1, 1)\n",
    "    return x, y\n",
    "\n",
    "def rescale_fgs(fgs):\n",
    "    '''\n",
    "    Make raw foreground meta data compatible with model\n",
    "    x, y: convert to relative to last position, scale to [-1, 1]\n",
    "    rotate: convert from [0, 360] to [0, 1]\n",
    "    \n",
    "    Parameters:\n",
    "    fgs: a list of (number, rank, x, y, scale, rotate, opaque)\n",
    "    '''\n",
    "    rescaled_fgs = []\n",
    "    for i, fg in enumerate(fgs):\n",
    "        number, rank, x, y, scale, rotate, opaque = fg\n",
    "        if i == 0:\n",
    "            x_last, y_last = x, y\n",
    "        # convert rotate in [0, 360] to [0, 1]\n",
    "        rescaled_rotate = rotate / 360\n",
    "        x_rel, y_rel = x - x_last, y - y_last\n",
    "        # normalize x_rel and y_rel to [-1, 1]\n",
    "        x_rel, y_rel = normalize_relative_xy(x_rel, y_rel)\n",
    "        # update last records\n",
    "        x_last, y_last = x, y\n",
    "        rescaled_fgs.append((number, rank, x_rel, y_rel, scale, rescaled_rotate, opaque))\n",
    "    return rescaled_fgs\n",
    "    \n",
    "def color_emb(emb, color):\n",
    "    '''\n",
    "    Append color embedding to a given embedding\n",
    "    \n",
    "    Parameters:\n",
    "    emb (tensor): embedding of shape (emb_size, )\n",
    "    color (str or tuple): color in hex or tuple, i.e.'#FFFFFF' or (255, 255, 255)\n",
    "    '''\n",
    "    if isinstance(color, str):\n",
    "        r, g, b = hex2rgb(color)\n",
    "    else:\n",
    "        r, g, b = color\n",
    "    r, g, b = r/255, g/255, b/255\n",
    "    return torch.cat((emb, torch.tensor([r, g, b])))\n",
    "\n",
    "def build_raw_data(bg_embs, fg_embs, img_list, batch_size):\n",
    "    '''\n",
    "    Generate training data with a meta file and corresponding embeddings\n",
    "    \n",
    "    Parameters:\n",
    "    bg_embs: dict \n",
    "        background embedding dictionary. key: number; entry: tensor of shape (emb_size, )\n",
    "    fg_embs: dict\n",
    "        foreground embedding dictionary. key: number; entry: tensor of shape (emb_size, )\n",
    "    img_manifest_path: str\n",
    "    batch_size: int\n",
    "    \n",
    "    Returns:\n",
    "    data_batch: list of (in_seqs, bk_embs, cls_labels) tuples\n",
    "        in_seqs: torch.Tensor of (seq_len, batch_size, fg_emb_size + 5)\n",
    "        bk_embs: torch.Tensor of (batch_size, bk_emb_size)\n",
    "        cls_labels: torch.Tensor of (batch_size)\n",
    "    '''\n",
    "    # group image by foreground art sequence length\n",
    "    img_group = defaultdict(list)\n",
    "    for img_info in img_list:\n",
    "        img_group[len(img_info['fg'])].append(img_info)\n",
    "    \n",
    "    training_data = []\n",
    "    # process data according to their different foreground sequence length\n",
    "    for fg_len, imgs in img_group.items():\n",
    "        # create mini-batch for data of a certain foreground sequence length\n",
    "        for i in range(0, len(imgs), batch_size):\n",
    "            fg_reps_batch = []\n",
    "            cls_label_batch = []\n",
    "            bk_emb_batch = []\n",
    "            for img_info in imgs[i:i+batch_size]:\n",
    "                # record last x and y for calculating relative position\n",
    "                x_last = int(img_info['fg'][0][2])\n",
    "                y_last = int(img_info['fg'][0][3])\n",
    "                fg_reps = []\n",
    "                # convert x, y to relative positions; scale rotate to 1/360\n",
    "                rescaled_fgs = rescale_fgs(img_info['fg'])\n",
    "                fg_reps = [torch.cat((fg_embs[fg[0]], torch.tensor(fg[-5:]))) for fg in rescaled_fgs]\n",
    "                fg_reps = torch.stack(fg_reps)\n",
    "                fg_reps = fg_reps.unsqueeze(dim=1) # (steps, mb_size=1, fg_emb_dim+5), 1 is the batch_size\n",
    "                # covert class label from [-1, 1] to [0, 2] (0 bad, 1 neutral, 2 good)\n",
    "                cls_label = int(img_info['flag']) + 1\n",
    "                cls_label = torch.tensor(cls_label, dtype=torch.long).unsqueeze(dim=0) # (mb_size=1, 1)\n",
    "                # embed background color into \n",
    "                bk_emb = color_emb(bg_embs[img_info['bg']], img_info['bgColor'])\n",
    "                bk_emb = bk_emb.unsqueeze(dim=0)\n",
    "                fg_reps_batch.append(fg_reps)\n",
    "                cls_label_batch.append(cls_label)\n",
    "                bk_emb_batch.append(bk_emb)\n",
    "            fg_reps_batch = torch.cat(fg_reps_batch, dim=1)\n",
    "            cls_label_batch = torch.cat(cls_label_batch, dim=0)\n",
    "            bk_emb_batch = torch.cat(bk_emb_batch, dim=0)\n",
    "            training_data.append((fg_reps_batch, bk_emb_batch, cls_label_batch))\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cbf194",
   "metadata": {},
   "source": [
    "## Inspect Data\n",
    "\n",
    "1. Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c765de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/data.txt') as f:\n",
    "    img_manifest = [json.loads(line.strip()) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc996593",
   "metadata": {},
   "source": [
    "2. check statistics of x and y\n",
    "    \n",
    "    x and y in train & test data are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f1f9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @unused\n",
    "def undo_batch(data_batched):\n",
    "    '''Convert batch back to list'''\n",
    "    data_list = []\n",
    "    for in_seqs, bk_embs, cls_labels in data_batched:\n",
    "        seqs = torch.split(in_seqs, 1, dim=1)\n",
    "        embs = torch.split(bk_embs, 1, dim=0)\n",
    "        labels = torch.split(cls_labels, 1, dim=0)\n",
    "        data_list += [sample for sample in zip(seqs, embs, labels)]\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = build_raw_data(bg_embs, fg_embs, img_manifest, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73f81c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def inspect_xy(all_fgs):\n",
    "    x, y = [], []\n",
    "    for fgs in all_fgs:   \n",
    "        x += [fg[-5] for fg in fgs]\n",
    "        y += [fg[-4] for fg in fgs]\n",
    "    print(f'Relative x and y positions: ({len(x)} records)')\n",
    "    print(f'x: [{min(x)},{max(x)}]; \\tscaled: [{min(x)*402}, {max(x)*402}]')\n",
    "    print(f'y: [{min(y)},{max(y)}]; \\tscaled: [{min(y)*600}, {max(y)*600}]')\n",
    "    print(f'mean x: \\t{np.mean(x)}; \\tscaled: {np.mean(x)*402}')\n",
    "    print(f'mean y: \\t{np.mean(y)}; \\tscaled: {np.mean(y)*600}')\n",
    "    print(f'variance x: \\t{np.var(x)}; \\tscaled: {np.var(x)*402*402}')\n",
    "    print(f'variance y: \\t{np.var(y)}; \\tscaled: {np.var(y)*600*600}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f919225",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fgs = []\n",
    "for data in full_data:\n",
    "    train_fgs += [data[0][:, 0, -5:].tolist()]\n",
    "inspect_xy(train_fgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c1353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE_FACTOR_X = 402\n",
    "SCALE_FACTOR_Y = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aab7167",
   "metadata": {},
   "source": [
    "Check statistics of scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83de6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_statistics(data_list, name):\n",
    "    print(f'Records of {name}:\\t{len(data_list)}')\n",
    "    print(f'Range of {name}:  \\t[{min(data_list)},{max(data_list)}];')\n",
    "    print(f'Mean of {name}:   \\t{np.mean(data_list)}')\n",
    "    print(f'Variance of {name}:\\t{np.var(data_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e7feec",
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = []\n",
    "for img_info in img_manifest:\n",
    "    scales += [fg[-3] for fg in img_info['fg']]\n",
    "show_statistics(scales, 'scale')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7644ad23",
   "metadata": {},
   "source": [
    "3. check class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a737d61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_label = []\n",
    "for data in full_data:   \n",
    "    cls_label += data[2].tolist()\n",
    "cls_label = np.array(cls_label)\n",
    "print(f'negative samples: {(cls_label==0).sum()}')\n",
    "print(f'neutral samples: {(cls_label==1).sum()}')\n",
    "print(f'positive samples: {(cls_label==2).sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88d0363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with data from original manifest\n",
    "flags = []\n",
    "for data in img_manifest:   \n",
    "    flags.append(data['flag'])\n",
    "flags = np.array(flags)\n",
    "print(f'negative samples: {(flags==-1).sum()}')\n",
    "print(f'neutral samples: {(flags==0).sum()}')\n",
    "print(f'positive samples: {(flags==1).sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c9d4a3",
   "metadata": {},
   "source": [
    "### Balance Class Distribution\n",
    "\n",
    "We will augment positive and neutral samples to make class distribution more banlanced.\n",
    "\n",
    "Assumption: If we remove the first or the last foreground art, the sequence is still a good/neutral/bad sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3ed2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def augment_data(image_manifest, increment):\n",
    "    '''\n",
    "    Augment data by extracting part of original forground arts.\n",
    "    \n",
    "    Parameters:\n",
    "    image_manifest (list(dict)): image informations loaded from files, it has keys 'image', 'flag', 'bgColor', 'bg' and 'fg'\n",
    "    increment (int): number of new data to augment\n",
    "    '''\n",
    "    augments = random.choices(image_manifest, k=increment)\n",
    "    for aug in augments:\n",
    "        aug['fg'] = aug['fg'][random.randint(0,1):random.choice([-1, None])]\n",
    "    return image_manifest + augments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd150a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_samples = [sample for sample in img_manifest if sample['flag']==-1]\n",
    "neutral_samples = [sample for sample in img_manifest if sample['flag']==0]\n",
    "positive_samples = [sample for sample in img_manifest if sample['flag']==1]\n",
    "\n",
    "# augment positive samples\n",
    "positive_samples = augment_data(positive_samples, max(len(negative_samples)-len(positive_samples),0))\n",
    "# augment nuetral samples\n",
    "neutral_samples = augment_data(neutral_samples, max(len(negative_samples)-len(neutral_samples),0))\n",
    "# combine them and overwrite original img_manifest\n",
    "img_manifest = negative_samples + neutral_samples + positive_samples\n",
    "\n",
    "print(f'total: {len(img_manifest)}; negative: {len(negative_samples)}; neutral: {len(neutral_samples)}; positive: {len(positive_samples)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062d7a8e",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "0. Load data for training\n",
    "\n",
    "    Run this cell to split data into train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e6c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_manifest, test_manifest = train_test_split(img_manifest, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d505273",
   "metadata": {},
   "source": [
    "Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f84200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = build_raw_data(bg_embs, fg_embs, train_manifest, 128)\n",
    "test_data = build_raw_data(bg_embs, fg_embs, test_manifest, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695c2ce",
   "metadata": {},
   "source": [
    "1. define hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef43720",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict()\n",
    "args['seq_in_dim'] = 581\n",
    "args['input_hid_size'] = 576 + 3 # background embedding + initial color embedding\n",
    "args['hid_dim'] = 256\n",
    "args['num_layers'] = 4\n",
    "args['lr'] = 0.001\n",
    "args['wd'] = 1e-6\n",
    "args['epochs'] = 800"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6691ba",
   "metadata": {},
   "source": [
    "2. examine training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1939dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input sequence has shape {}\".format(train_data[0][0].shape))\n",
    "print(\"Background embedding has shape {}\".format(train_data[0][1].shape))\n",
    "print(\"Class labels has shape {}\".format(train_data[0][2].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d336353",
   "metadata": {},
   "source": [
    "2. define models(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7991f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequence.models.seq_model import sequence_model, seq_loss_fn\n",
    "\n",
    "seq_model = sequence_model(input_size=args['seq_in_dim'],\n",
    "                           input_hid_size=args['input_hid_size'],\n",
    "                           hidden_size=args['hid_dim'],\n",
    "                           num_layers=args['num_layers'])\n",
    "seq_model = seq_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163dd9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(seq_loss_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc384589",
   "metadata": {},
   "source": [
    "4. define loss and optimizer\n",
    "   \n",
    "   will direct use the customized loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d46f9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(seq_model.parameters(), lr=args['lr'], weight_decay=args['wd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ea8c0e",
   "metadata": {},
   "source": [
    "    define tensorboard writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c41ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "now = datetime.today()\n",
    "dt= now.strftime(\"%m_%d_%H_%M\")\n",
    "writer = SummaryWriter(os.path.join('./runs', dt))\n",
    "writer.add_text('Parameters', str(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8573c119",
   "metadata": {},
   "source": [
    "5. training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108b3e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_data, seq_model):\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    seq_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (in_seqs, bk_embs, cls_labels) in enumerate(test_data):\n",
    "            in_seqs = in_seqs.to(device)\n",
    "            bk_embs = bk_embs.to(device)\n",
    "            cls_labels = cls_labels.to(device)\n",
    "\n",
    "            h_0 = torch.stack([bk_embs for _ in range(args['num_layers'])]).to(device)\n",
    "\n",
    "            c_0 = torch.zeros_like(h_0).to(device)\n",
    "\n",
    "            out_seqs_logits, cls_logits = seq_model(in_seqs[:-1,], (h_0, c_0), return_last_hidden=False)\n",
    "\n",
    "            loss, seq_loss, cls_loss = seq_loss_fn(out_seqs_logits, in_seqs[1:,], cls_logits, cls_labels, alpha=0.2, return_details=True)\n",
    "            total_loss += loss.item()\n",
    "            total_samples += len(cls_labels)\n",
    "    return total_loss / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5795d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    for epoch in range(args['epochs']):\n",
    "        seq_model.train()\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        for i, (in_seqs, bk_embs, cls_labels) in enumerate(train_data):\n",
    "            in_seqs = in_seqs.to(device)\n",
    "            bk_embs = bk_embs.to(device)\n",
    "            cls_labels = cls_labels.to(device)\n",
    "\n",
    "            h_0 = torch.stack([bk_embs for _ in range(args['num_layers'])]).to(device)\n",
    "            # print(\"Hidden 0 shape {}\".format(h_0.shape))\n",
    "\n",
    "            c_0 = torch.zeros_like(h_0).to(device)\n",
    "\n",
    "            out_seqs_logits, cls_logits = seq_model(in_seqs[:-1,], (h_0, c_0), return_last_hidden=False)\n",
    "\n",
    "            loss, seq_loss, cls_loss = seq_loss_fn(out_seqs_logits, in_seqs[1:,], cls_logits, cls_labels, alpha=0.2, return_details=True)\n",
    "            total_loss += loss.item()\n",
    "            total_samples += len(cls_labels)\n",
    "            writer.add_scalar('batch/train_loss', loss.item()/len(cls_labels), global_step=epoch*len(train_data)+i)\n",
    "            writer.add_scalar('batch/seq_loss', seq_loss.item()/len(cls_labels), global_step=epoch*len(train_data)+i)\n",
    "            writer.add_scalar('batch/cls_loss', cls_loss.item()/len(cls_labels), global_step=epoch*len(train_data)+i)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        test_loss = test(test_data, seq_model)\n",
    "        writer.add_scalar('epoch/train_loss', total_loss/total_samples, global_step=epoch)\n",
    "        writer.add_scalar('epoch/test_loss', test_loss, global_step=epoch)\n",
    "        print(\"In epoch: {:03d} | loss: {:.6f}, test_loss: {:.6f}\".format(epoch, total_loss/total_samples, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be2a759",
   "metadata": {},
   "source": [
    "## Image Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401ff19e",
   "metadata": {},
   "source": [
    "### Define metrics\n",
    "\n",
    "1. Define rate function to rate whether a generated foreground sequence is good or not.\n",
    "\n",
    "    **Rate Process**:\n",
    "    - Required: background shading, background color, foreground arts meta data, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151259c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_seq(bg, bg_color, fgs, seq_model, bg_embs, fg_embs):\n",
    "    '''\n",
    "    Rate a given background and foreground sequence.\n",
    "    \n",
    "    Parameters:\n",
    "    bg (int): background sequence number\n",
    "    fgs (list): a list of foreground meta data (number, rank, x, y, scale, rotate, opaque)\n",
    "    seq_model (nn.Module): the trained model for rating\n",
    "    bg_embs (dict): a {number:emb} dict for background\n",
    "    fg_embs (dict): a {number:emb} dict for foreground\n",
    "    '''\n",
    "    # convert (x, y) to relative positions; scale rotate to 1/360\n",
    "    rescaled_fgs = rescale_fgs(fgs)\n",
    "    # construct an art sequence with fgs\n",
    "    in_seqs = [torch.cat((fg_embs[fg[0]], torch.tensor(fg[-5:]))) for fg in rescaled_fgs]\n",
    "    in_seqs = torch.stack(in_seqs).unsqueeze(dim=1)\n",
    "    # unsqueeze dim 0 to imitate a batch behavior\n",
    "    bg_emb = color_emb(bg_embs[bg], bg_color).unsqueeze(dim=0)\n",
    "    h_0 = torch.stack([bg_emb]*args['num_layers'])\n",
    "    c_0 = torch.zeros_like(h_0)\n",
    "    \n",
    "    in_seqs = in_seqs.to(device)\n",
    "    h_0 = h_0.to(device)\n",
    "    c_0 = c_0.to(device)\n",
    "    with torch.no_grad():\n",
    "        _, cls_logits = seq_model(in_seqs, (h_0, c_0))\n",
    "    \n",
    "    return torch.argmax(cls_logits).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863ffbe0",
   "metadata": {},
   "source": [
    "2. Define metrics for comparing foreground embeddings. This is for retrieving output sequence from database with generated embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5377372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(A, B):\n",
    "    return ((A - B)**2).mean()\n",
    "\n",
    "def find_closest(emb, emb_dict):\n",
    "    diffs = {}\n",
    "    for k, v in emb_dict.items():\n",
    "        diffs[k] = mse(emb, v)\n",
    "    min_k = min(diffs, key=diffs.get)\n",
    "    return min_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d233ce",
   "metadata": {},
   "source": [
    "### Modulize image generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1e7aef",
   "metadata": {},
   "source": [
    "Helper function 1: Randomly generate an initial background and an inital foreground, along with foreground's meta data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cf191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import colorgram\n",
    "\n",
    "def get_fg_path(idx):\n",
    "    return f'../data/asset/art/{idx}.png'\n",
    "\n",
    "def get_bg_path(idx):\n",
    "    return f'../data/asset/shading/{idx}.png'\n",
    "\n",
    "def generate_random_inits():\n",
    "    init_bg = random.randint(1, 24) # shading is in [1, 24]\n",
    "    candidate_colors = colorgram.extract(get_bg_path(init_bg), 6)\n",
    "    init_bg_color = random.choice(candidate_colors).rgb\n",
    "    # select again if encountered black color\n",
    "    if init_bg_color == (0, 0, 0):\n",
    "        init_bg_color = random.choice(candidate_colors).rgb\n",
    "    fg_seq_len = random.choice(range(15, 24))\n",
    "    init_fg = random.randint(2, 21) # art is in [2, 21]\n",
    "    init_fg_x = random.randint(10, 300)\n",
    "    init_fg_y = random.randint(10, 300)\n",
    "    init_fg_scale = random.uniform(0.9, 1.1)\n",
    "    init_fg_rotate = random.randint(0, 359)\n",
    "    init_fg_opaque = 1\n",
    "    init_fg_meta = (init_fg,0, init_fg_x,init_fg_y,init_fg_scale,init_fg_rotate,init_fg_opaque)\n",
    "    return init_bg, init_bg_color, fg_seq_len, init_fg_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce1b7a7",
   "metadata": {},
   "source": [
    "Helper function 2: Convert initial background and foreground to suitable input for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83b7a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_init_model_input(init_bg, init_bg_color, init_fg_meta, bg_embs, fg_embs):\n",
    "    fg_emb_0 = torch.cat((fg_embs[init_fg_meta[0]], torch.tensor([0, 0, init_fg_meta[4], init_fg_meta[5]/360, init_fg_meta[6]])))\n",
    "    fg_emb_0 = fg_emb_0.unsqueeze(dim=0).unsqueeze(dim=0).to(device)\n",
    "\n",
    "    bk_emb = color_emb(bg_embs[init_bg], init_bg_color)\n",
    "    h_0 = torch.stack([bk_emb for _ in range(args['num_layers'])]).unsqueeze(dim=1).to(device)\n",
    "    c_0 = torch.zeros_like(h_0).to(device)\n",
    "\n",
    "    # print(f\"h shape:{h_0.shape}; fg_emb.shape:{fg_emb_0.shape}; bk_emb.shape:{bk_emb.shape}\")\n",
    "    \n",
    "    return h_0, c_0, fg_emb_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bee8d98",
   "metadata": {},
   "source": [
    "Helper function 3: Generate a sequence of foreground with given background embedding and initial hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676e4004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fg_seqs(init_bg, init_bg_color, fg_seq_len, init_fg_meta, bg_embs, fg_embs):\n",
    "    h, c, fg_emb = generate_init_model_input(init_bg, init_bg_color, init_fg_meta, bg_embs, fg_embs)\n",
    "    fgs = [init_fg_meta]\n",
    "    with torch.no_grad():\n",
    "        for i in range(1, fg_seq_len):\n",
    "            # when i is 1, use intact model\n",
    "            if i == 1:\n",
    "                seqs_logits, cls_logits, (h, c) = seq_model(fg_emb, (h, c), return_last_hidden=True)           \n",
    "            # when i is int [2, seq_len-1], use parts of the model to directly feed h and c to rnn\n",
    "            else:\n",
    "                output_seqs, (h, c) = seq_model.rnn_model(fg_emb, (h, c))\n",
    "                seqs_logits = seq_model.seq_transformer(output_seqs)\n",
    "                if i == fg_seq_len - 1:\n",
    "                    cls_logits = seq_model.classifier(h[-1, :, :])\n",
    "                    rate = torch.argmax(cls_logits).item()\n",
    "            # process sequence logits to fit the input value scales    \n",
    "            seqs_logits[:, :, -5:-3] = torch.tanh(seqs_logits[:, :, -5:-3]) # x, y\n",
    "            seqs_logits[:, :, -3:] = torch.sigmoid(seqs_logits[:, :, -3:])  # scale, rotation, alpha\n",
    "            fg_emb = seqs_logits[0,0,:576] # next fg embedding\n",
    "            fg_meta = seqs_logits[0,0,576:] # (x, y, scale, angle, opaque)\n",
    "            fg_name = find_closest(fg_emb.detach().cpu(), fg_embs)\n",
    "            fg_emb = fg_embs[fg_name].to(device)\n",
    "            fg_emb = torch.cat((fg_emb, fg_meta))\n",
    "            fg_emb = fg_emb.unsqueeze(dim=0).unsqueeze(dim=0).to(device)\n",
    "            # again, magic numbers come from inspection of data\n",
    "            fgs.append((fg_name, i, int(fgs[-1][2]+fg_meta[0].item()*402), int(fgs[-1][3]+fg_meta[1].item()*600), \\\n",
    "                        fg_meta[2].item(), int(fg_meta[3]*360), fg_meta[4].item()))\n",
    "    return fgs, rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3a47c3",
   "metadata": {},
   "source": [
    "Finally, generate a image within just one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3d5cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_image(bg_embs, fg_embs):\n",
    "    init_bg, init_bg_color, fg_seq_len, init_fg_meta = generate_random_inits()\n",
    "#     init_fg_x,init_fg_y,init_fg_scale,init_fg_rotate,init_fg_opaque = init_fg_meta[-5:]\n",
    "#     print({'bg':init_bg, 'fg_seq_len':fg_seq_len, 'init_fg':{'(x, y)':(init_fg_x, init_fg_y), 'scale':init_fg_scale, \\\n",
    "#            'rotate': init_fg_rotate, 'opaque': init_fg_opaque}})\n",
    "    fgs, rate = generate_fg_seqs(init_bg, init_bg_color, fg_seq_len, init_fg_meta, bg_embs, fg_embs)\n",
    "    return init_bg, init_bg_color, fgs, rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebfef58",
   "metadata": {},
   "source": [
    "### Generate a bunch of images and rate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d54c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_num = 1000\n",
    "generated_images = []\n",
    "for i in range(image_num):\n",
    "    init_bg, init_bg_color, fgs, rate = generate_random_image(bg_embs, fg_embs)\n",
    "    # rate = rate_seq(init_bg, init_bg_color, fgs, seq_model, bg_embs, fg_embs)\n",
    "    if rate == 2:\n",
    "        generated_images.append((init_bg, init_bg_color, fgs))\n",
    "print(f'Generated {len(generated_images)} acceptable images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15536bf",
   "metadata": {},
   "source": [
    "### Inspect generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0822f276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def row_plot(data):\n",
    "    fig, axes = plt.subplots(1, len(data), figsize=(5*len(data),5))\n",
    "    plt.style.use('seaborn')\n",
    "    for i, d in enumerate(data):\n",
    "        if len(data) == 1:\n",
    "            sns.kdeplot(d, ax=axes)\n",
    "        else:\n",
    "            sns.kdeplot(d, ax=axes[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7768e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fgs = []\n",
    "for data in full_data:\n",
    "    train_fgs += [data[0][:, 0, -5:].tolist()]\n",
    "print('Statistics of x and y from training images:')\n",
    "inspect_xy(train_fgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73e3b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot x and y\n",
    "x_train, y_train = [], []\n",
    "for fgs in train_fgs:   \n",
    "    x_train += [fg[-5] for fg in fgs]\n",
    "    y_train += [fg[-4] for fg in fgs]\n",
    "row_plot([x_train, y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb0cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_fgs = [img[-1] for img in generated_images]\n",
    "generated_fgs = [rescale_fgs(fgs) for fgs in generated_fgs]\n",
    "print('Statistics of x and y of generated images:')\n",
    "inspect_xy(generated_fgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262ab5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gen, y_gen = [], []\n",
    "for fgs in generated_fgs:   \n",
    "    x_gen += [fg[-5] for fg in fgs]\n",
    "    y_gen += [fg[-4] for fg in fgs]\n",
    "row_plot([x_gen, y_gen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cc0bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_geb, y_train = [], []\n",
    "for fgs in train_fgs:   \n",
    "    x_train += [fg[-5] for fg in fgs]\n",
    "    y_train += [fg[-4] for fg in fgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee985436",
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = []\n",
    "for img_info in img_manifest:\n",
    "    scales += [fg[-3] for fg in img_info['fg']]\n",
    "print('Statistics of scales from training images:')\n",
    "show_statistics(scales, 'scale')\n",
    "row_plot([scales])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2b1086",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_scales = []\n",
    "for fgs in generated_fgs:\n",
    "    generated_scales += [fg[-3] for fg in fgs]\n",
    "print('Statistics of scales from generated images:')\n",
    "show_statistics(generated_scales, 'scale')\n",
    "row_plot([generated_scales])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd1086e",
   "metadata": {},
   "source": [
    "## Visualize Generated Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f747084",
   "metadata": {},
   "source": [
    "Define image transformation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e811ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tile_image(mat, rows, cols):\n",
    "    '''\n",
    "    Tile mat vertically rows times and horizontally cols times\n",
    "    '''\n",
    "    tiled_image = np.tile(mat, (rows, cols, 1))\n",
    "    return tiled_image\n",
    "\n",
    "def vanilla_rotate(mat, angle):\n",
    "    '''\n",
    "    Rotate mat clockwise angle degrees. This operation keeps sizes and scales, which means \n",
    "    there will be information loss, i.e. corners of original image.\n",
    "    \n",
    "    mat: numpy.ndarray,(h, w, c)\n",
    "        Matrix to rotate.\n",
    "    angle: int\n",
    "        Natural number. It will be moded into [0, 360) \n",
    "    '''\n",
    "    rows,cols = mat.shape[:2]\n",
    "    # cols-1 and rows-1 are the coordinate limits.\n",
    "    M = cv2.getRotationMatrix2D(((cols-1)/2.0,(rows-1)/2.0),angle,1)\n",
    "    rotated = cv2.warpAffine(mat, M, (cols,rows))\n",
    "    return rotated\n",
    "\n",
    "def scale_image(mat, scale):\n",
    "    scaled_mat = cv2.resize(mat,None,fx=scale, fy=scale, interpolation = cv2.INTER_CUBIC)\n",
    "    return scaled_mat\n",
    "\n",
    "def transform_image(mat, scale, angle):\n",
    "    '''\n",
    "    scale and rotate image in one step\n",
    "    '''\n",
    "    scaled_mat = scale_image(mat, scale)\n",
    "    rotated_mat = vanilla_rotate(scaled_mat, angle)\n",
    "    return rotated_mat\n",
    "\n",
    "def overlay_transparent(background, overlay, x, y):\n",
    "    '''\n",
    "    Overlay top left coner of 'overlay' onto background at (x, y).\n",
    "    x, y are expected to be integers.\n",
    "    '''\n",
    "\n",
    "    background_width = background.shape[1]\n",
    "    background_height = background.shape[0]\n",
    "    h, w = overlay.shape[0], overlay.shape[1]\n",
    "\n",
    "    # when overlay is totally to the right or bottom of background\n",
    "    if x >= background_width or y >= background_height:\n",
    "        return background      \n",
    "    # when overlay is totally to the left or top of background\n",
    "    if x + w <= 0 or y + h <= 0:\n",
    "        return background\n",
    "\n",
    "    if x + w > background_width:\n",
    "        w = background_width - x\n",
    "        overlay = overlay[:, :w]   # truncate overlay's width right\n",
    "\n",
    "    if y + h > background_height:\n",
    "        h = background_height - y\n",
    "        overlay = overlay[:h]      # truncate overlay's height bottom\n",
    "\n",
    "    if x < 0:\n",
    "        w = x + w\n",
    "        overlay = overlay[:, -w:] # truncate overlay's width left\n",
    "    \n",
    "    if y < 0:\n",
    "        h = y + h\n",
    "        overlay = overlay[-h:]    # truncate overlay's height top\n",
    "\n",
    "    if overlay.shape[2] < 4:\n",
    "        overlay = np.concatenate(\n",
    "            [\n",
    "                overlay,\n",
    "                np.ones((overlay.shape[0], overlay.shape[1], 1), dtype = overlay.dtype) * 255\n",
    "            ],\n",
    "            axis = 2,\n",
    "        )\n",
    "\n",
    "    overlay_image = overlay[..., :4]\n",
    "    mask = overlay[..., 3:] / 255.0\n",
    "\n",
    "    y = max(y, 0)\n",
    "    x = max(x, 0)\n",
    "    background[y:y+h, x:x+w] = (1.0 - mask) * background[y:y+h, x:x+w] + mask * overlay_image\n",
    "\n",
    "    return background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aef485c",
   "metadata": {},
   "source": [
    "Helper functions for image generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057910d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blank(width, height, rgb_color=(255, 255, 255), opacity=255):\n",
    "    \"\"\"Create new image(numpy array) filled with certain color in RGB\"\"\"\n",
    "    # Create black blank image\n",
    "    image = np.zeros((width, height, 4), np.uint8)\n",
    "\n",
    "    # Since OpenCV uses BGR, convert the color first\n",
    "    color = tuple((*reversed(rgb_color), opacity))\n",
    "    # Fill image with color\n",
    "    image[:] = color\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f85077",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/data.txt') as f:\n",
    "    img_list = [json.loads(line.strip()) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9425a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def draw_image(bg, bg_color, fgs):\n",
    "    fg_list = fgs\n",
    "    cursor = (0, 0)\n",
    "    \n",
    "    if isinstance(bg_color, str):\n",
    "        blank = create_blank(1000, 1000, hex2rgb(bg_color))\n",
    "    else:\n",
    "        blank = create_blank(1000, 1000, bg_color)\n",
    "\n",
    "    bg = cv2.imread(get_bg_path(bg), -1) # -1 flag reads alpha channel\n",
    "    bg = scale_image(bg, 2)\n",
    "    tiled = tile_image(bg, 12, 12)\n",
    "    bg = overlay_transparent(blank, tiled[:1000, :1000], 0, 0)\n",
    "    for fg_info in fgs:\n",
    "        fg_idx = fg_info[0]\n",
    "        fg = cv2.imread(get_fg_path(fg_idx), -1)\n",
    "        fg_x, fg_y = fg_info[2], fg_info[3]\n",
    "        cursor = (fg_y, fg_x)\n",
    "        scale = float(fg_info[4])\n",
    "        angle = int(fg_info[5])\n",
    "        fg = transform_image(fg, scale, angle)\n",
    "        bg = overlay_transparent(bg, fg, cursor[0], cursor[1])\n",
    "    changed_color = cv2.cvtColor(bg, cv2.COLOR_BGRA2RGBA)\n",
    "    return changed_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd1dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "import math\n",
    "\n",
    "def grid_plot(images, n_col, dpi=80, savename=None, savedpi=200):\n",
    "    n_gen = len(images)\n",
    "    n_row = math.ceil(n_gen / n_col)\n",
    "    figure(figsize=(14, 4 * n_row), dpi=dpi)\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(n_row, n_col, i+1, autoscale_on=True)\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "    if savename is not None:\n",
    "        plt.savefig(savename, dpi=savedpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfb1d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "untiled_plots = [draw_image(bg, bg_color, fgs) for bg, bg_color, fgs in generated_images]\n",
    "grid_plot(untiled_plots, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d7e870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @unused\n",
    "def rel2abs(fgs):\n",
    "    '''\n",
    "    Convert foreground images from scaled(relative position, scaled rotation) ones back to drawable format.\n",
    "    '''\n",
    "    absolute_fgs = []\n",
    "    for i, fg in enumerate(fgs):\n",
    "        if i == 0:\n",
    "            x_last, y_last = 0, 0\n",
    "        number, rank, rel_x, rel_y, scale, scaled_rotate, opaque = fg\n",
    "        # constants come from inspection of training data (defined in inspect data section)\n",
    "        x, y = int(x_last + rel_x * SCALE_FACTOR_X), int(y_last + rel_y * SCALE_FACTOR_Y)\n",
    "        x_last, y_last = x, y\n",
    "        rotate = int(scaled_rotate * 360)\n",
    "        absolute_fgs.append((number, rank, x, y, scale, rotate, opaque))\n",
    "    return absolute_fgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25faed1f",
   "metadata": {},
   "source": [
    "Combine all sets of foregrounds, and overlay it on to a background:\n",
    "\n",
    "test snipset:\n",
    "```python\n",
    "fg_combined = combine_fgs(fgs)\n",
    "print(fg_combined.shape)\n",
    "plt.imshow(fg_combined)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f25ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_fgs(fgs):\n",
    "    '''\n",
    "    Combine all foregrounds into a transparent foreground sticker\n",
    "    \n",
    "    Parameters:\n",
    "    fgs (list): a list of (number, rank, x, y, scale, rotate, opaque) tuples.\n",
    "    '''\n",
    "    abs_fgs = np.array(fgs)\n",
    "    min_x, max_x = np.min(abs_fgs[:, 2]), np.max(abs_fgs[:, 2])\n",
    "    min_y, max_y = np.min(abs_fgs[:, 3]), np.max(abs_fgs[:, 3])\n",
    "    # 120 is a buffer for the width and height of the right-/bottom-most image\n",
    "    span_x = int(max_x - min_x) + 120 \n",
    "    span_y = int(max_y - min_y) + 120\n",
    "    # avoid span_x and span_y to be too large\n",
    "    span_x = min(span_x, 2000)\n",
    "    span_x = min(span_y, 2000)\n",
    "    # move the start of foregrounds to topleft\n",
    "    abs_fgs[:, 2] -= min_x\n",
    "    abs_fgs[:, 3] -= min_y\n",
    "    bg = create_blank(span_x, span_y, opacity=0)\n",
    "    for fg in abs_fgs:\n",
    "        number, rank, x, y, scale, rotate, opaque = fg\n",
    "        # convert np.float to int\n",
    "        number, x, y, rotate = int(number), int(x), int(y), int(rotate)\n",
    "        fg = cv2.imread(get_fg_path(number), -1)\n",
    "        cursor = (y, x)\n",
    "        fg = transform_image(fg, scale, rotate)\n",
    "        bg = overlay_transparent(bg, fg, cursor[0], cursor[1])\n",
    "    changed_color = cv2.cvtColor(bg, cv2.COLOR_BGRA2RGBA)\n",
    "    return changed_color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd49fcfc",
   "metadata": {},
   "source": [
    "Test snipset:\n",
    "\n",
    "```python\n",
    "background = create_blank(2000, 2000, opacity=0)\n",
    "overlay = fg_combined\n",
    "combined = tile_overlay(background, overlay, 800, 500)\n",
    "plt.imshow(combined)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeff15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def tile_overlay(background, overlay, x=0, y=0):\n",
    "    '''\n",
    "    Tile overlay multiple times to fully cover background.\n",
    "    \n",
    "    Parameters:\n",
    "    background (mat):\n",
    "    overlay (mat):\n",
    "    x (int): inital transition of overlay's top-left corner on x axis\n",
    "    y (int): inital transition of overlay's top-left corner on y axis\n",
    "    '''\n",
    "    background_width = background.shape[1]\n",
    "    background_height = background.shape[0]\n",
    "    h, w = overlay.shape[0], overlay.shape[1]\n",
    "    \n",
    "    # re-place x (to make sure top is fully filled)\n",
    "    x = x - math.ceil(x / h) * h\n",
    "    # re-place y (to make sure left is fully filled)\n",
    "    y = y - math.ceil(y / w) *  w\n",
    "    \n",
    "    # needed foreground on x axis\n",
    "    num_rows = math.ceil((background_height - x) / h)\n",
    "    # needed foreground on y axis\n",
    "    num_cols = math.ceil((background_width - y) / w)\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            overlay_x = x + i * h\n",
    "            overlay_y = y + j * w\n",
    "            # exchange x and y because overlay_transparent use a different coordinate\n",
    "            background = overlay_transparent(background, overlay, overlay_y, overlay_x)\n",
    "    return background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ede033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_tiled_image(bg, bg_color, fgs):\n",
    "    background_height = 1500\n",
    "    background_width = 1500\n",
    "\n",
    "    if isinstance(bg_color, str):\n",
    "        blank = create_blank(background_width, background_height, hex2rgb(bg_color))\n",
    "    else:\n",
    "        blank = create_blank(background_width, background_height, bg_color)\n",
    "\n",
    "    bg = cv2.imread(get_bg_path(bg), -1) # -1 flag reads alpha channel\n",
    "    bg = scale_image(bg, 2)\n",
    "    tiled = tile_image(bg, 14, 14)\n",
    "    bg = overlay_transparent(blank, tiled[:background_height, :background_width], 0, 0)\n",
    "    combined_fgs = combine_fgs(fgs)\n",
    "    bg = tile_overlay(bg, combined_fgs)\n",
    "    changed_color = cv2.cvtColor(bg, cv2.COLOR_BGRA2RGBA)\n",
    "    return changed_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af427c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiled_plots = [draw_tiled_image(bg, bg_color, fgs) for bg, bg_color, fgs in generated_images]\n",
    "grid_plot(tiled_plots, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea53b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
